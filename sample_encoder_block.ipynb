{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95e91b0-fdca-4424-80cd-5680a4e97381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# =============================================\n",
    "# Previous components (simplified)\n",
    "# =============================================\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class MultiHeadAttentionblock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        return attention_scores @ value, attention_scores\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        query = self.w_q(q).view(q.shape[0], q.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = self.w_k(k).view(k.shape[0], k.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = self.w_v(v).view(v.shape[0], v.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        x, self.attention_scores = MultiHeadAttentionblock.attention(query, key, value, mask, self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        return self.w_o(x)\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "# =============================================\n",
    "# EncoderBlock Implementation\n",
    "# =============================================\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionblock,\n",
    "                 feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        # Two residual connections: one for attention, one for feedforward\n",
    "        self.residual_connections = nn.ModuleList([\n",
    "            ResidualConnection(features, dropout) for _ in range(2)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, src_mask):\n",
    "        # Step 1: Self-Attention with residual\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        # Step 2: FeedForward with residual\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03a7ea2-ce8d-4a3b-9b18-36d1f5569208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENCODER BLOCK DEMO ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# DEMO: Step-by-step execution\n",
    "# =============================================\n",
    "print(\"=== ENCODER BLOCK DEMO ===\\n\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 1 ## number of sentences \n",
    "seq_len = 4 ## number of tokens\n",
    "d_model = 8 ## feature dim\n",
    "h = 2 ##num of heads\n",
    "d_ff = 16 ##hidden dim\n",
    "dropout = 0.1 ##dropout p\n",
    "\n",
    "# Create components\n",
    "attention = MultiHeadAttentionblock(d_model, h, dropout)\n",
    "feedforward = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "encoder_block = EncoderBlock(d_model, attention, feedforward, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bf599d-8b80-4437-bc9a-f8fbb6580034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Input x:\n",
      "  Shape: torch.Size([1, 4, 8])\n",
      "  Sample x[0,0]: tensor([-1.2552, -1.0697, -0.2918,  0.9712,  1.0733, -0.7998,  1.5604, -0.9575])\n",
      "X:  tensor([[[-1.2552, -1.0697, -0.2918,  0.9712,  1.0733, -0.7998,  1.5604,\n",
      "          -0.9575],\n",
      "         [-0.4063, -1.2687, -0.6252, -0.3680, -0.3114, -0.7477, -0.1026,\n",
      "          -1.1438],\n",
      "         [-0.5951,  0.6209, -0.7472,  1.8960, -0.8180, -0.8053,  0.0325,\n",
      "           0.6191],\n",
      "         [ 0.3887, -0.2316,  0.4306, -0.3481, -0.1165,  0.7390,  0.6994,\n",
      "          -0.2416]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input: (batch=2, seq=4, features=8)\n",
    "# Sentence example: \"The cat sat here\" (4 tokens)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(\"Step 1 - Input x:\")\n",
    "print(\"  Shape:\", x.shape)  # (1, 4, 8)\n",
    "print(\"  Sample x[0,0]:\", x[0, 0])\n",
    "print('X: ',x)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae928bb-2b3d-4975-89ba-62fc2b1b467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 - Source mask:\n",
      "  Shape: torch.Size([2, 1, 1, 4])\n",
      "  Mask[0]: tensor([1., 1., 1., 0.])\n",
      "  → Last token (padding) will be ignored\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Source mask: shape (batch, 1, 1, seq_len) for broadcasting\n",
    "# Assume last token is padding (mask it out)\n",
    "src_mask = torch.ones(batch_size, 1, 1, seq_len)\n",
    "src_mask[:, :, :, -1] = 0  # Mask last token\n",
    "print(\"Step 2 - Source mask:\")\n",
    "print(\"  Shape:\", src_mask.shape)  # (2, 1, 1, 4)\n",
    "print(\"  Mask[0]:\", src_mask[0].squeeze())\n",
    "print(\"  → Last token (padding) will be ignored\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ce1e7-464a-42a7-b65f-d1339c377348",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MANUAL STEP-BY-STEP\n",
    "# ============================================\n",
    "print(\"Step 3 - First Residual Connection (Self-Attention)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save original for comparison\n",
    "x_original = x.clone()\n",
    "\n",
    "# Apply first residual (attention)\n",
    "print(\"  3a. LayerNorm(x)\")\n",
    "norm1 = encoder_block.residual_connections[0].norm(x)\n",
    "print(\"    Normalized x[0,0]:\", norm1[0, 0])\n",
    "print()\n",
    "\n",
    "print(\"  3b. MultiHeadAttention(Q=x, K=x, V=x, mask)\")\n",
    "attn_out = encoder_block.self_attention_block(norm1, norm1, norm1, src_mask)\n",
    "print(\"    Attention output shape:\", attn_out.shape)  # (2, 4, 8)\n",
    "print(\"    Attention out[0,0]:\", attn_out[0, 0])\n",
    "print()\n",
    "\n",
    "print(\"  3c. Dropout + Add residual\")\n",
    "x_after_attn = x + encoder_block.residual_connections[0].dropout(attn_out)\n",
    "print(\"    After residual shape:\", x_after_attn.shape)\n",
    "print(\"    x_after_attn[0,0]:\", x_after_attn[0, 0])\n",
    "print()\n",
    "\n",
    "print(\"Step 4 - Second Residual Connection (FeedForward)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"  4a. LayerNorm(x_after_attn)\")\n",
    "norm2 = encoder_block.residual_connections[1].norm(x_after_attn)\n",
    "print(\"    Normalized[0,0]:\", norm2[0, 0])\n",
    "print()\n",
    "\n",
    "print(\"  4b. FeedForward\")\n",
    "ff_out = encoder_block.feed_forward_block(norm2)\n",
    "print(\"    FF output shape:\", ff_out.shape)  # (2, 4, 8)\n",
    "print(\"    FF out[0,0]:\", ff_out[0, 0])\n",
    "print()\n",
    "\n",
    "print(\"  4c. Dropout + Add residual\")\n",
    "x_final_manual = x_after_attn + encoder_block.residual_connections[1].dropout(ff_out)\n",
    "print(\"    Final output shape:\", x_final_manual.shape)\n",
    "print(\"    x_final[0,0]:\", x_final_manual[0, 0])\n",
    "print()\n",
    "\n",
    "# ============================================\n",
    "# END-TO-END\n",
    "# ============================================\n",
    "print(\"Step 5 - END-TO-END: encoder_block(x, src_mask)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "x_direct = x_original.clone()\n",
    "output = encoder_block(x_direct, src_mask)\n",
    "print(\"  Output shape:\", output.shape)  # (2, 4, 8)\n",
    "print(\"  Output[0,0]:\", output[0, 0])\n",
    "print()\n",
    "\n",
    "# ============================================\n",
    "# Visualize information flow\n",
    "# ============================================\n",
    "print(\"Step 6 - INFORMATION FLOW\")\n",
    "print(\"=\"*60)\n",
    "print(\"Token 0 (The)  before encoder:\", x_original[0, 0, :3])\n",
    "print(\"Token 0 (The)  after attention:\", x_after_attn[0, 0, :3])\n",
    "print(\"Token 0 (The)  after FF:\", output[0, 0, :3])\n",
    "print()\n",
    "print(\"✅ Each token now contains:\")\n",
    "print(\"   - Context from ALL other tokens (via self-attention)\")\n",
    "print(\"   - Position-specific transformations (via feedforward)\")\n",
    "print()\n",
    "\n",
    "# ============================================\n",
    "# Check mask effect\n",
    "# ============================================\n",
    "print(\"Step 7 - MASK EFFECT (last token should receive little attention)\")\n",
    "print(\"=\"*60)\n",
    "attn_scores = encoder_block.self_attention_block.attention_scores\n",
    "print(\"Attention scores shape:\", attn_scores.shape)  # (2, h, 4, 4)\n",
    "print(\"Token 0 attention to all tokens (Head 0):\")\n",
    "print(\"  To token 0:\", attn_scores[0, 0, 0, 0].item())\n",
    "print(\"  To token 1:\", attn_scores[0, 0, 0, 1].item())\n",
    "print(\"  To token 2:\", attn_scores[0, 0, 0, 2].item())\n",
    "print(\"  To token 3 (masked):\", attn_scores[0, 0, 0, 3].item())\n",
    "print(\"  → Token 3 (padding) receives near-zero attention ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc6cee-ddad-4b1c-aeca-ed8e872971cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2322147-64c0-471f-9a9c-ece1e1384fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2e0b9-a305-475a-8621-99c8af9c8f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

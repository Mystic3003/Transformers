{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54341ecc-a1a9-49a7-b8e9-0f4448880bbc",
   "metadata": {},
   "source": [
    "The projection layer converts the decoder's output embeddings into vocabulary probabilities to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9167244-62c6-4526-a107-5b6c145861da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE TRANSFORMER: INPUT → ENCODER → DECODER → PROJECTION → OUTPUT\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPLETE TRANSFORMER: INPUT → ENCODER → DECODER → PROJECTION → OUTPUT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a535b92-1b8e-4959-ae08-b3b12d7eadc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY\n",
      "--------------------------------------------------------------------------------\n",
      "Source vocab (English): {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, 'I': 3, 'love': 4, 'cats': 5, 'dogs': 6}\n",
      "Target vocab (French): {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, 'je': 3, 'aime': 4, 'les': 5, 'chats': 6, 'chiens': 7}\n",
      "Source vocab size: 7\n",
      "Target vocab size: 8\n",
      "d_model (embedding size): 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# SETUP: Vocabulary and Parameters\n",
    "# =============================================\n",
    "# English to French translation example\n",
    "src_vocab = {\n",
    "    \"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2,\n",
    "    \"I\": 3, \"love\": 4, \"cats\": 5, \"dogs\": 6\n",
    "}\n",
    "\n",
    "tgt_vocab = {\n",
    "    \"<PAD>\": 0, \"<SOS>\": 1,\"<EOS>\": 2,\n",
    "    \"je\": 3, \"aime\": 4, \"les\": 5, \"chats\": 6, \"chiens\": 7\n",
    "}\n",
    "\n",
    "src_vocab_size = len(src_vocab)  # 7\n",
    "tgt_vocab_size = len(tgt_vocab)  # 8\n",
    "d_model = 8  # Small for demo\n",
    "seq_len = 4\n",
    "\n",
    "print(\"VOCABULARY\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Source vocab (English): {src_vocab}\")\n",
    "print(f\"Target vocab (French): {tgt_vocab}\")\n",
    "print(f\"Source vocab size: {src_vocab_size}\")\n",
    "print(f\"Target vocab size: {tgt_vocab_size}\")\n",
    "print(f\"d_model (embedding size): {d_model}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f83b6ef-4d2c-4d84-b667-a51031d605c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: INPUT SENTENCES\n",
      "--------------------------------------------------------------------------------\n",
      "Source (English): ['I', 'love', 'cats', '<EOS>']\n",
      "Source IDs: [3, 4, 5, 2]\n",
      "\n",
      "Target (French): ['<SOS>', 'je', 'aime', 'les']\n",
      "Target IDs: [1, 3, 4, 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================\n",
    "# INPUT SENTENCES\n",
    "# =============================================\n",
    "print(\"STEP 1: INPUT SENTENCES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Source: \"I love cats\"\n",
    "src_sentence = [\"I\", \"love\", \"cats\", \"<EOS>\"]\n",
    "src_ids = [src_vocab[w] for w in src_sentence]  # [3, 4, 5, 2]\n",
    "\n",
    "# Target: \"je aime les chats\" (during training, we provide this)\n",
    "tgt_sentence = [\"<SOS>\", \"je\", \"aime\", \"les\"]  # First 4 tokens\n",
    "tgt_ids = [tgt_vocab[w] for w in tgt_sentence]  # [1, 3, 4, 5]\n",
    "\n",
    "print(f\"Source (English): {src_sentence}\")\n",
    "print(f\"Source IDs: {src_ids}\")\n",
    "print()\n",
    "print(f\"Target (French): {tgt_sentence}\")\n",
    "print(f\"Target IDs: {tgt_ids}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c2b76fc-f200-43d1-a01d-e88cbafc2d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source batch shape: torch.Size([1, 4])\n",
      "Target batch shape: torch.Size([1, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensors (batch_size=1)\n",
    "src_batch = torch.tensor([src_ids])  # (1, 4 words)\n",
    "tgt_batch = torch.tensor([tgt_ids])  # (1, 4 words)\n",
    "\n",
    "print(f\"Source batch shape: {src_batch.shape}\")\n",
    "print(f\"Target batch shape: {tgt_batch.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b9d6e06-0355-4162-8208-049f40302fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: EMBEDDINGS\n",
      "--------------------------------------------------------------------------------\n",
      "Source embedded shape: torch.Size([1, 4, 8])\n",
      "Target embedded shape: torch.Size([1, 4, 8])\n",
      "\n",
      "Sample embedding for 'I' (src token 0):\n",
      "tensor([ 0.6667,  0.8459, -1.0229,  0.3337,  0.5114,  0.4131,  0.4232,  0.6103],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================\n",
    "# EMBEDDINGS\n",
    "# =============================================\n",
    "print(\"STEP 2: EMBEDDINGS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "src_embedded = src_embedding(src_batch)  # (1, 4, 8)\n",
    "tgt_embedded = tgt_embedding(tgt_batch)  # (1, 4, 8)\n",
    "\n",
    "print(f\"Source embedded shape: {src_embedded.shape}\")\n",
    "print(f\"Target embedded shape: {tgt_embedded.shape}\")\n",
    "print()\n",
    "print(\"Sample embedding for 'I' (src token 0):\")\n",
    "print(src_embedded[0, 0, :])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb1854f8-cd56-48c5-b60b-3dcf253be3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: ENCODER\n",
      "--------------------------------------------------------------------------------\n",
      "Encoder output shape: torch.Size([1, 4, 8])\n",
      "Interpretation: (batch=1, src_seq_len=4, d_model=8)\n",
      "\n",
      "Encoder output for token 0:\n",
      "tensor([ 0.1039,  0.3812, -0.3746, -0.1416,  0.4212,  0.8765, -0.1701, -0.5222],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# ENCODER (simplified)\n",
    "# =============================================\n",
    "print(\"STEP 3: ENCODER\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Simplified encoder (just a linear layer for demo)\n",
    "encoder = nn.Linear(d_model, d_model)\n",
    "encoder_output = encoder(src_embedded)  # (1, 4, 8)\n",
    "\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "print(f\"Interpretation: (batch=1, src_seq_len=4, d_model=8)\")\n",
    "print()\n",
    "print(\"Encoder output for token 0:\")\n",
    "print(encoder_output[0, 0, :])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3ba31f1-965e-4ed5-b326-50e673839870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: DECODER\n",
      "--------------------------------------------------------------------------------\n",
      "Decoder output shape: torch.Size([1, 4, 8])\n",
      "Interpretation: (batch=1, tgt_seq_len=4, d_model=8)\n",
      "\n",
      "Decoder output for token 0:\n",
      "tensor([ 0.0964,  0.4175,  1.0548,  1.2416,  0.9222, -0.0118, -0.7893, -0.4573],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# DECODER (simplified)\n",
    "# =============================================\n",
    "print(\"STEP 4: DECODER\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Simplified decoder (just a linear layer for demo)\n",
    "decoder = nn.Linear(d_model, d_model)\n",
    "decoder_output = decoder(tgt_embedded)  # (1, 4, 8)\n",
    "\n",
    "print(f\"Decoder output shape: {decoder_output.shape}\")\n",
    "print(f\"Interpretation: (batch=1, tgt_seq_len=4, d_model=8)\")\n",
    "print()\n",
    "print(\"Decoder output for token 0:\")\n",
    "print(decoder_output[0, 0, :])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26a775fc-e8e5-495b-be9e-30fe72cced95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: PROJECTION LAYER\n",
      "================================================================================\n",
      "  Created projection: 8 → 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# PROJECTION LAYER (THE KEY PART!)\n",
    "# =============================================\n",
    "print(\"STEP 5: PROJECTION LAYER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "        print(f\"  Created projection: {d_model} → {vocab_size}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model) → (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "\n",
    "projection = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a530859-d93c-4083-88f8-908b58a23244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1597,  0.4743, -0.6621, -0.0593, -0.4071,  0.7157,  0.3807,\n",
      "          -0.0580],\n",
      "         [ 0.4536,  0.2123, -0.1139, -0.1537, -0.1311,  0.2143,  0.1252,\n",
      "          -0.2146],\n",
      "         [ 0.5522,  0.0049,  0.1381,  0.0944,  0.2438,  0.1069,  0.2276,\n",
      "           0.0667],\n",
      "         [ 0.2487,  0.2507, -0.6990, -0.2207, -0.4781,  0.3978,  0.1632,\n",
      "          -0.2684]]], grad_fn=<ViewBackward0>)\n",
      "Logits shape: torch.Size([1, 4, 8])\n",
      "Interpretation: (batch=1, seq_len=4, vocab_size=8)\n",
      "\n",
      "What are logits?\n",
      "  Raw scores for EACH word in the vocabulary\n",
      "  Higher score = model thinks that word is more likely\n",
      "\n",
      "Logits for position 0 (after <SOS>):\n",
      "tensor([ 0.1597,  0.4743, -0.6621, -0.0593, -0.4071,  0.7157,  0.3807, -0.0580],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply projection\n",
    "logits = projection(decoder_output)  # (1, 4, 8) → (1, 4, 8)\n",
    "print(logits)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Interpretation: (batch=1, seq_len=4, vocab_size={tgt_vocab_size})\")\n",
    "print()\n",
    "\n",
    "print(\"What are logits?\")\n",
    "print(\"  Raw scores for EACH word in the vocabulary\")\n",
    "print(\"  Higher score = model thinks that word is more likely\")\n",
    "print()\n",
    "\n",
    "print(\"Logits for position 0 (after <SOS>):\")\n",
    "print(logits[0, 0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "787aba0c-36b9-42ed-910d-3c8e43c28ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpretation:\n",
      "       <PAD>:    0.160\n",
      "       <SOS>:    0.474\n",
      "       <EOS>:   -0.662\n",
      "          je:   -0.059\n",
      "        aime:   -0.407\n",
      "         les:    0.716\n",
      "       chats:    0.381\n",
      "      chiens:   -0.058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Map indices to words\n",
    "reverse_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "print(\"Interpretation:\")\n",
    "for idx, score in enumerate(logits[0, 0]):\n",
    "    word = reverse_tgt_vocab[idx]\n",
    "    print(f\"  {word:>10s}: {score.item():>8.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "685548b6-9b35-4e0d-b5cc-f10b2f5e1cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: SOFTMAX (Convert logits → probabilities)\n",
      "--------------------------------------------------------------------------------\n",
      "Probabilities shape: torch.Size([1, 4, 8])\n",
      "\n",
      "Probabilities for position 0 (predicting next word after <SOS>):\n",
      "tensor([0.1254, 0.1717, 0.0551, 0.1007, 0.0711, 0.2186, 0.1564, 0.1009],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# SOFTMAX: Convert to probabilities\n",
    "# =============================================\n",
    "print(\"STEP 6: SOFTMAX (Convert logits → probabilities)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "probabilities = F.softmax(logits, dim=-1)  # (1, 4, 8)\n",
    "\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Probabilities for position 0 (predicting next word after <SOS>):\")\n",
    "print(probabilities[0, 0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0bb762c-6cc8-4a80-8148-a801bf5f04af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word probabilities:\n",
      "       <PAD>:  12.54%\n",
      "       <SOS>:  17.17%\n",
      "       <EOS>:   5.51%\n",
      "          je:  10.07%\n",
      "        aime:   7.11%\n",
      "         les:  21.86%\n",
      "       chats:  15.64%\n",
      "      chiens:  10.09%\n",
      "\n",
      "Sum of probabilities: 1.000000 (should be 1.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Word probabilities:\")\n",
    "for idx, prob in enumerate(probabilities[0, 0]):\n",
    "    word = reverse_tgt_vocab[idx]\n",
    "    print(f\"  {word:>10s}: {prob.item()*100:>6.2f}%\")\n",
    "print()\n",
    "\n",
    "# Sum should be 1.0\n",
    "print(f\"Sum of probabilities: {probabilities[0, 0].sum().item():.6f} (should be 1.0)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "305794bf-b0cc-427a-a4cc-f600fd30e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: PREDICTION\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted IDs: tensor([[5, 0, 0, 5]])\n",
      "\n",
      "Predicted words for each position:\n",
      "  Position 0: 'les' (ID=5, prob=21.86%)\n",
      "  Position 1: '<PAD>' (ID=0, prob=18.27%)\n",
      "  Position 2: '<PAD>' (ID=0, prob=17.90%)\n",
      "  Position 3: 'les' (ID=5, prob=18.80%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# PREDICTION: Select word with highest probability\n",
    "# =============================================\n",
    "print(\"STEP 7: PREDICTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "predicted_ids = torch.argmax(probabilities, dim=-1)  # (1, 4)\n",
    "\n",
    "print(f\"Predicted IDs: {predicted_ids}\")\n",
    "print()\n",
    "\n",
    "print(\"Predicted words for each position:\")\n",
    "for pos in range(seq_len):\n",
    "    predicted_id = predicted_ids[0, pos].item()\n",
    "    predicted_word = reverse_tgt_vocab[predicted_id]\n",
    "    prob = probabilities[0, pos, predicted_id].item()\n",
    "    print(f\"  Position {pos}: '{predicted_word}' (ID={predicted_id}, prob={prob*100:.2f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddc852-591d-4850-a0ee-dae07d824017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "342ab775-f842-4c95-9e63-30b1646aaef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HOW PROJECTION LAYER WORKS INTERNALLY\n",
      "================================================================================\n",
      "\n",
      "Input to projection layer (one token):\n",
      "  Shape: torch.Size([8])\n",
      "  Values: tensor([-0.0262, -0.4131, -0.4404,  0.1978,  0.4935,  0.0317,  0.0225,  1.0157],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Projection layer is nn.Linear(d_model=8, vocab_size=8)\n",
      "  Weight matrix shape: torch.Size([8, 8])\n",
      "  Bias shape: torch.Size([8])\n",
      "\n",
      "Matrix multiplication:\n",
      "  decoder_output: (batch, seq, d_model) = (1, 4, 8)\n",
      "  weight:         (vocab_size, d_model) = (8, 8)\n",
      "  result:         (batch, seq, vocab_size) = (1, 4, 8)\n",
      "\n",
      "For each position, it computes:\n",
      "  logits[pos] = decoder_output[pos] @ weight.T + bias\n",
      "               └─(8,)─┘              └─(8, 8)─┘   └─(8,)─┘\n",
      "               = (8,) result for each vocab word\n",
      "\n",
      "================================================================================\n",
      "VISUAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "FULL PIPELINE:\n",
      "\n",
      "  Input: 'I love cats'\n",
      "    ↓\n",
      "  Token IDs: [3, 4, 5, 2]\n",
      "    ↓\n",
      "  Embeddings: (1, 4, 8)  ← Each token → 8-dim vector\n",
      "    ↓\n",
      "  ENCODER: (1, 4, 8)  ← Contextualized representations\n",
      "    ↓\n",
      "  Target: '<SOS> je aime les'\n",
      "    ↓\n",
      "  Token IDs: [1, 3, 4, 5]\n",
      "    ↓\n",
      "  Embeddings: (1, 4, 8)\n",
      "    ↓\n",
      "  DECODER: (1, 4, 8)  ← Contextualized with encoder output\n",
      "    ↓\n",
      "  PROJECTION: (1, 4, 8) → (1, 4, 8)\n",
      "              └─d_model─┘   └─vocab─┘\n",
      "    ↓\n",
      "  Logits: Raw scores for each vocab word\n",
      "    ↓\n",
      "  SOFTMAX: Convert to probabilities\n",
      "    ↓\n",
      "  ARGMAX: Select word with highest probability\n",
      "    ↓\n",
      "  Output: Predicted word at each position\n",
      "\n",
      "================================================================================\n",
      "WHY IS PROJECTION LAYER NEEDED?\n",
      "================================================================================\n",
      "\n",
      "Problem:\n",
      "  Decoder outputs vectors of size d_model = 8\n",
      "  But we need to predict one of 8 words!\n",
      "\n",
      "Solution:\n",
      "  Projection layer: Linear(8, 8)\n",
      "  Maps each d_model vector → vocab_size scores\n",
      "  Each score = 'how likely is this word?'\n",
      "\n",
      "Example (position 0):\n",
      "  Decoder output: [0.12, -0.34, 0.56, ...]  (8 numbers)\n",
      "  Projection →    [1.2, 0.3, -0.5, 2.1, ...] (8 numbers)\n",
      "                   ↑    ↑     ↑    ↑\n",
      "                  <PAD> <SOS> <EOS> je  ... (one per vocab word)\n",
      "\n",
      "After softmax:\n",
      "  [0.05, 0.02, 0.01, 0.12, ...]  (probabilities sum to 1)\n",
      "\n",
      "Pick highest: 'je' (position 3) with prob 0.12\n",
      "\n",
      "✅ Projection layer converts abstract embeddings → concrete word predictions!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================\n",
    "# DETAILED BREAKDOWN: How Projection Works\n",
    "# =============================================\n",
    "print(\"=\" * 80)\n",
    "print(\"HOW PROJECTION LAYER WORKS INTERNALLY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Input to projection layer (one token):\")\n",
    "single_token = decoder_output[0, 1, :]  # Token 1\n",
    "print(f\"  Shape: {single_token.shape}\")  # (8,)\n",
    "print(f\"  Values: {single_token}\")\n",
    "print()\n",
    "\n",
    "print(\"Projection layer is nn.Linear(d_model=8, vocab_size=8)\")\n",
    "print(f\"  Weight matrix shape: {projection.proj.weight.shape}\")  # (8, 8)\n",
    "print(f\"  Bias shape: {projection.proj.bias.shape}\")  # (8,)\n",
    "print()\n",
    "\n",
    "print(\"Matrix multiplication:\")\n",
    "print(\"  decoder_output: (batch, seq, d_model) = (1, 4, 8)\")\n",
    "print(\"  weight:         (vocab_size, d_model) = (8, 8)\")\n",
    "print(\"  result:         (batch, seq, vocab_size) = (1, 4, 8)\")\n",
    "print()\n",
    "\n",
    "print(\"For each position, it computes:\")\n",
    "print(\"  logits[pos] = decoder_output[pos] @ weight.T + bias\")\n",
    "print(\"               └─(8,)─┘              └─(8, 8)─┘   └─(8,)─┘\")\n",
    "print(\"               = (8,) result for each vocab word\")\n",
    "print()\n",
    "\n",
    "# =============================================\n",
    "# VISUAL SUMMARY\n",
    "# =============================================\n",
    "print(\"=\" * 80)\n",
    "print(\"VISUAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"FULL PIPELINE:\")\n",
    "print()\n",
    "print(\"  Input: 'I love cats'\")\n",
    "print(\"    ↓\")\n",
    "print(\"  Token IDs: [3, 4, 5, 2]\")\n",
    "print(\"    ↓\")\n",
    "print(\"  Embeddings: (1, 4, 8)  ← Each token → 8-dim vector\")\n",
    "print(\"    ↓\")\n",
    "print(\"  ENCODER: (1, 4, 8)  ← Contextualized representations\")\n",
    "print(\"    ↓\")\n",
    "print(\"  Target: '<SOS> je aime les'\")\n",
    "print(\"    ↓\")\n",
    "print(\"  Token IDs: [1, 3, 4, 5]\")\n",
    "print(\"    ↓\")\n",
    "print(\"  Embeddings: (1, 4, 8)\")\n",
    "print(\"    ↓\")\n",
    "print(\"  DECODER: (1, 4, 8)  ← Contextualized with encoder output\")\n",
    "print(\"    ↓\")\n",
    "print(\"  PROJECTION: (1, 4, 8) → (1, 4, 8)\")\n",
    "print(\"              └─d_model─┘   └─vocab─┘\")\n",
    "print(\"    ↓\")\n",
    "print(\"  Logits: Raw scores for each vocab word\")\n",
    "print(\"    ↓\")\n",
    "print(\"  SOFTMAX: Convert to probabilities\")\n",
    "print(\"    ↓\")\n",
    "print(\"  ARGMAX: Select word with highest probability\")\n",
    "print(\"    ↓\")\n",
    "print(\"  Output: Predicted word at each position\")\n",
    "print()\n",
    "\n",
    "# =============================================\n",
    "# WHY IS PROJECTION NEEDED?\n",
    "# =============================================\n",
    "print(\"=\" * 80)\n",
    "print(\"WHY IS PROJECTION LAYER NEEDED?\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Problem:\")\n",
    "print(f\"  Decoder outputs vectors of size d_model = {d_model}\")\n",
    "print(f\"  But we need to predict one of {tgt_vocab_size} words!\")\n",
    "print()\n",
    "\n",
    "print(\"Solution:\")\n",
    "print(f\"  Projection layer: Linear({d_model}, {tgt_vocab_size})\")\n",
    "print(\"  Maps each d_model vector → vocab_size scores\")\n",
    "print(\"  Each score = 'how likely is this word?'\")\n",
    "print()\n",
    "\n",
    "print(\"Example (position 0):\")\n",
    "print(\"  Decoder output: [0.12, -0.34, 0.56, ...]  (8 numbers)\")\n",
    "print(\"  Projection →    [1.2, 0.3, -0.5, 2.1, ...] (8 numbers)\")\n",
    "print(\"                   ↑    ↑     ↑    ↑\")\n",
    "print(\"                  <PAD> <SOS> <EOS> je  ... (one per vocab word)\")\n",
    "print()\n",
    "\n",
    "print(\"After softmax:\")\n",
    "print(\"  [0.05, 0.02, 0.01, 0.12, ...]  (probabilities sum to 1)\")\n",
    "print()\n",
    "\n",
    "print(\"Pick highest: 'je' (position 3) with prob 0.12\")\n",
    "print()\n",
    "\n",
    "print(\"✅ Projection layer converts abstract embeddings → concrete word predictions!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ce781-bdff-44ea-8ea6-a997943b2e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da6010-4755-4960-a40b-ae69c5a3fde3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

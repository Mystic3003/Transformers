{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743829e9-d91b-4f1a-8d9b-96b0380753d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOKENIZER EXAMPLE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOKENIZER EXAMPLE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0d9445-79a3-4bb0-b8a0-c35c17513a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate dataset\n",
    "def get_all_sentences(ds, lang):\n",
    "    \"\"\"Yields sentences from dataset\"\"\"\n",
    "    sentences = [\n",
    "        \"I love machine learning\",\n",
    "        \"I love deep learning\",\n",
    "        \"transformers are amazing\",\n",
    "        \"attention is all you need\",\n",
    "        \"I love transformers\"\n",
    "    ]\n",
    "    for sentence in sentences:\n",
    "        yield sentence\n",
    "\n",
    "# Config\n",
    "config = {\n",
    "    'tokenizer_file': './tokenizer_{0}.json'\n",
    "}\n",
    "lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb8e11b-4e8c-462a-abd3-f028b410853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tokenizer\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    print(f\"\\nTokenizer path: {tokenizer_path}\")\n",
    "    \n",
    "    if not Path.exists(tokenizer_path):\n",
    "        print(\"Building new tokenizer...\")\n",
    "        \n",
    "        # Create tokenizer\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = WordLevelTrainer(\n",
    "            special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "            min_frequency=2\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        print(\"Training tokenizer on sentences...\")\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        \n",
    "        # Save\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "        print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "    else:\n",
    "        print(\"Loading existing tokenizer...\")\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f095d5-f606-468d-a78d-bb92b0301024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer path: tokenizer_en.json\n",
      "Building new tokenizer...\n",
      "Training tokenizer on sentences...\n",
      "Tokenizer saved to tokenizer_en.json\n",
      "\n",
      "================================================================================\n",
      "TESTING TOKENIZER\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get tokenizer\n",
    "tokenizer = get_or_build_tokenizer(config, None, lang)\n",
    "\n",
    "# Test tokenizer\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING TOKENIZER\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba07711-1181-4760-a130-a02d56febea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size: 8\n",
      "\n",
      "First 15 tokens:\n",
      "            [PAD]: 1\n",
      "             love: 5\n",
      "         learning: 6\n",
      "            [SOS]: 2\n",
      "            [UNK]: 0\n",
      "            [EOS]: 3\n",
      "                I: 4\n",
      "     transformers: 7\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "print(\"\\nFirst 15 tokens:\")\n",
    "for token, id in list(vocab.items())[:15]:\n",
    "    print(f\"  {token:>15s}: {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3c8bb2-05d3-489b-876d-f764ac21f30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Original text: 'I love transformers'\n",
      "Token IDs: [4, 5, 7]\n",
      "Tokens: ['I', 'love', 'transformers']\n"
     ]
    }
   ],
   "source": [
    "# Encode text\n",
    "text = \"I love transformers\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(f\"\\n\\nOriginal text: '{text}'\")\n",
    "print(f\"Token IDs: {encoded.ids}\")\n",
    "print(f\"Tokens: {encoded.tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3efc602-2ed1-45bc-9b52-ab33151b2c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: 'I love transformers'\n"
     ]
    }
   ],
   "source": [
    "# Decode back\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "print(f\"Decoded text: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117bbdce-e2f1-4074-a765-3aa916cea5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Text with unknown word: 'I love python'\n",
      "Token IDs: [4, 5, 0]\n",
      "Tokens: ['I', 'love', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Unknown word\n",
    "text_unk = \"I love python\"  # 'python' not in training data\n",
    "encoded_unk = tokenizer.encode(text_unk)\n",
    "print(f\"\\n\\nText with unknown word: '{text_unk}'\")\n",
    "print(f\"Token IDs: {encoded_unk.ids}\")\n",
    "print(f\"Tokens: {encoded_unk.tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562a606-a6dc-41a6-89dd-29728400672a",
   "metadata": {},
   "source": [
    "### COMPLETE HUGGING FACE TOKENIZERS LIBRARY TUTORIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66738d9f-d8e9-4126-9808-63df3ab1dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel, BPE, WordPiece, Unigram\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel, CharDelimiterSplit\n",
    "from tokenizers.trainers import WordLevelTrainer, BpeTrainer, WordPieceTrainer, UnigramTrainer\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78626445-b4b2-49ed-8841-3bbf2070931e",
   "metadata": {},
   "source": [
    "### STEP1 -GIVE THE DEFINITION OF A TOKEN (what does one token mean -is it a word ,character ?? etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "608b5525-9e2d-461e-b260-b63eb746245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 1: TOKENIZER MODELS (How tokens are created)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PART 1: TOKENIZER MODELS (How tokens are created)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de5d4fed-48d1-47ea-8697-d0ba77238d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1.1 WordLevel (Simplest - Each word = One token)\n",
      "--------------------------------------------------------------------------------\n",
      "Created WordLevel tokenizer\n",
      "  Model type: <class 'tokenizers.models.WordLevel'>\n",
      "  Unknown token: [UNK]\n",
      "\n",
      "Example vocabulary: {'[UNK]': 0, '[PAD]': 1, 'hello': 2, 'world': 3, 'cat': 4, 'dog': 5}\n",
      "Tokenizing 'hello world':\n",
      "  Result: ['hello', 'world'] → IDs [2, 3]\n",
      "Tokenizing 'hello python' (python not in vocab):\n",
      "  Result: ['hello', '[UNK]'] → IDs [2, 0]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. TOKENIZER MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"1.1 WordLevel (Simplest - Each word = One token)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "WordLevel Tokenizer\n",
    "-------------------\n",
    "- Treats each WHOLE WORD as a single token\n",
    "- Unknown words → [UNK]\n",
    "- Vocabulary grows with unique words\n",
    "- Simple but large vocabulary\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.models import WordLevel\n",
    "    model = WordLevel(unk_token=\"[UNK]\")\n",
    "\n",
    "Parameters:\n",
    "    - unk_token (str): Token for unknown words. Default: \"[UNK]\"\n",
    "    - vocab (Dict[str, int], optional): Pre-built vocabulary\n",
    "\n",
    "When to use:\n",
    "    ✓ Small, controlled vocabulary\n",
    "    ✓ Words are meaningful units (not subwords)\n",
    "    ✗ Large datasets (huge vocab)\n",
    "    ✗ Handling rare words (all become [UNK])\n",
    "\"\"\"\n",
    "\n",
    "tokenizer_wl = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "print(\"Created WordLevel tokenizer\")\n",
    "print(f\"  Model type: {type(tokenizer_wl.model)}\")\n",
    "print(f\"  Unknown token: [UNK]\")\n",
    "\n",
    "\n",
    "# Example vocabulary\n",
    "example_vocab_wl = {\n",
    "    \"[UNK]\": 0, \"[PAD]\": 1, \"hello\": 2, \"world\": 3, \"cat\": 4, \"dog\": 5\n",
    "}\n",
    "print(f\"\\nExample vocabulary: {example_vocab_wl}\")\n",
    "print(\"Tokenizing 'hello world':\")\n",
    "print(\"  Result: ['hello', 'world'] → IDs [2, 3]\")\n",
    "print(\"Tokenizing 'hello python' (python not in vocab):\")\n",
    "print(\"  Result: ['hello', '[UNK]'] → IDs [2, 0]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f80b1fe-3ad8-4644-951e-67fbd3cf09e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1.2 BPE (Byte-Pair Encoding - Subword tokenization)\n",
      "--------------------------------------------------------------------------------\n",
      "Created BPE tokenizer\n",
      "  Model type: <class 'tokenizers.models.BPE'>\n",
      "\n",
      "Example (after training):\n",
      "  'unhappiness' → ['un', 'happiness'] or ['un', 'happy', 'ness']\n",
      "  'antidisestablishmentarianism' → subword pieces\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"1.2 BPE (Byte-Pair Encoding - Subword tokenization)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "BPE (Byte-Pair Encoding)\n",
    "------------------------\n",
    "- Learns subword units by merging frequent character pairs\n",
    "- Can represent any word using subwords\n",
    "- Used by GPT-2, RoBERTa\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.models import BPE\n",
    "    model = BPE(unk_token=\"[UNK]\")\n",
    "\n",
    "Parameters:\n",
    "    - unk_token (str, optional): Unknown token\n",
    "    - continuing_subword_prefix (str): Prefix for continuing subwords. Default: \"##\"\n",
    "    - end_of_word_suffix (str): Suffix for end-of-word subwords\n",
    "    - dropout (float, optional): BPE dropout for regularization\n",
    "\n",
    "How it works:\n",
    "    1. Start with characters: h, e, l, l, o\n",
    "    2. Merge frequent pairs: (l, l) → ll\n",
    "    3. Continue merging: (he, ll) → hell, etc.\n",
    "    4. Result: \"hello\" might become [\"hell\", \"o\"] or [\"h\", \"ello\"]\n",
    "\n",
    "When to use:\n",
    "    ✓ Large vocabulary needed\n",
    "    ✓ Handle rare/unknown words\n",
    "    ✓ Open-domain text (internet, books)\n",
    "    ✗ When interpretability needed\n",
    "\"\"\"\n",
    "\n",
    "tokenizer_bpe = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "print(\"Created BPE tokenizer\")\n",
    "print(f\"  Model type: {type(tokenizer_bpe.model)}\")\n",
    "print(\"\\nExample (after training):\")\n",
    "print(\"  'unhappiness' → ['un', 'happiness'] or ['un', 'happy', 'ness']\")\n",
    "print(\"  'antidisestablishmentarianism' → subword pieces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07d50d62-c02b-46e0-bfd3-424d2621f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1.3 WordPiece (Used by BERT)\n",
      "--------------------------------------------------------------------------------\n",
      "Created WordPiece tokenizer\n",
      "  Continuing subword prefix: ##\n",
      "\n",
      "Example (after training):\n",
      "  'playing' → ['play', '##ing']\n",
      "  'unhappiness' → ['un', '##happiness'] or ['un', '##happy', '##ness']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"1.3 WordPiece (Used by BERT)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "WordPiece\n",
    "---------\n",
    "- Similar to BPE but uses likelihood instead of frequency\n",
    "- Used by BERT, DistilBERT\n",
    "- Subword tokenization with ## prefix for continuations\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.models import WordPiece\n",
    "    model = WordPiece(unk_token=\"[UNK]\")\n",
    "\n",
    "Parameters:\n",
    "    - unk_token (str): Unknown token. Default: \"[UNK]\"\n",
    "    - continuing_subword_prefix (str): Prefix for subwords. Default: \"##\"\n",
    "    - max_input_chars_per_word (int): Max chars per word. Default: 100\n",
    "\n",
    "Difference from BPE:\n",
    "    - BPE: Merge most frequent pairs\n",
    "    - WordPiece: Merge pairs that maximize likelihood\n",
    "\n",
    "Example tokenization:\n",
    "    \"playing\" → [\"play\", \"##ing\"]\n",
    "    \"unbelievable\" → [\"un\", \"##be\", \"##liev\", \"##able\"]\n",
    "\n",
    "When to use:\n",
    "    ✓ BERT-style models\n",
    "    ✓ When using pretrained BERT tokenizer\n",
    "\"\"\"\n",
    "\n",
    "tokenizer_wp = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "print(\"Created WordPiece tokenizer\")\n",
    "print(f\"  Continuing subword prefix: ##\")\n",
    "print(\"\\nExample (after training):\")\n",
    "print(\"  'playing' → ['play', '##ing']\")\n",
    "print(\"  'unhappiness' → ['un', '##happiness'] or ['un', '##happy', '##ness']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eff0adc3-0ace-4c14-90bb-6e4fc34613eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1.4 Unigram (Probabilistic)\n",
      "--------------------------------------------------------------------------------\n",
      "Created Unigram tokenizer\n",
      "  Uses probabilistic approach\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"1.4 Unigram (Probabilistic)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "Unigram\n",
    "-------\n",
    "- Probabilistic language model\n",
    "- Each token has a probability\n",
    "- Used by SentencePiece, T5\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.models import Unigram\n",
    "    model = Unigram()\n",
    "\n",
    "How it works:\n",
    "    - Starts with large vocabulary\n",
    "    - Removes tokens to maximize likelihood\n",
    "    - Multiple tokenizations possible (chooses most probable)\n",
    "\n",
    "When to use:\n",
    "    ✓ When probability-based tokenization needed\n",
    "    ✓ SentencePiece-style tokenization\n",
    "\"\"\"\n",
    "\n",
    "tokenizer_uni = Tokenizer(Unigram())\n",
    "print(\"Created Unigram tokenizer\")\n",
    "print(\"  Uses probabilistic approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be59a7-9dca-4ee7-9c66-5489e258db2c",
   "metadata": {},
   "source": [
    "### Step2 - On what Basis to splitting the tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b7dba5d-bbd5-41d3-b2cc-24ba3a843fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "PART 2: PRE-TOKENIZERS (How to split text into words)\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2.1 Whitespace (Split on spaces)\n",
      "--------------------------------------------------------------------------------\n",
      "Set Whitespace pre-tokenizer\n",
      "\n",
      "Example splitting:\n",
      "  Input:  'Hello world! How are you?'\n",
      "  Output: ['Hello', 'world!', 'How', 'are', 'you?']\n",
      "  Note: Punctuation attached to words\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. PRE-TOKENIZERS (Split text before tokenization)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PART 2: PRE-TOKENIZERS (How to split text into words)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"2.1 Whitespace (Split on spaces)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "Whitespace Pre-tokenizer\n",
    "------------------------\n",
    "- Splits on whitespace: spaces, tabs, newlines\n",
    "- Simplest pre-tokenizer\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "Example:\n",
    "    Input:  \"Hello world\\thow are you?\"\n",
    "    Output: [\"Hello\", \"world\", \"how\", \"are\", \"you?\"]\n",
    "    \n",
    "Note: Punctuation stays attached to words!\n",
    "\"\"\"\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer_wl.pre_tokenizer = Whitespace()\n",
    "\n",
    "print(\"Set Whitespace pre-tokenizer\")\n",
    "print(\"\\nExample splitting:\")\n",
    "print(\"  Input:  'Hello world! How are you?'\")\n",
    "print(\"  Output: ['Hello', 'world!', 'How', 'are', 'you?']\")\n",
    "print(\"  Note: Punctuation attached to words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3ad7228-80bb-421d-baa7-d65ab38a323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2.2 ByteLevel (GPT-2 style)\n",
      "--------------------------------------------------------------------------------\n",
      "ByteLevel pre-tokenizer\n",
      "\n",
      "Example splitting:\n",
      "  Input:  'Hello world'\n",
      "  Output: ['Hello', 'Ġworld']  (Ġ represents space)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"2.2 ByteLevel (GPT-2 style)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "ByteLevel Pre-tokenizer\n",
    "-----------------------\n",
    "- Used by GPT-2, RoBERTa\n",
    "- Splits on whitespace but handles bytes\n",
    "- Adds 'Ġ' prefix for space character\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.pre_tokenizers import ByteLevel\n",
    "    tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "Parameters:\n",
    "    - add_prefix_space (bool): Add space before first token. Default: True\n",
    "\n",
    "Example:\n",
    "    Input:  \"Hello world\"\n",
    "    Output: [\"Hello\", \"Ġworld\"]  (Ġ = space marker)\n",
    "\n",
    "When to use:\n",
    "    ✓ GPT-2 style models\n",
    "    ✓ When using byte-level encoding\n",
    "\"\"\"\n",
    "\n",
    "print(\"ByteLevel pre-tokenizer\")\n",
    "print(\"\\nExample splitting:\")\n",
    "print(\"  Input:  'Hello world'\")\n",
    "print(\"  Output: ['Hello', 'Ġworld']  (Ġ represents space)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2514296c-a0f3-476d-8c39-4d4b15eaea40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2.3 CharDelimiterSplit (Custom delimiter)\n",
      "--------------------------------------------------------------------------------\n",
      "CharDelimiterSplit(delimiter=',')\n",
      "\n",
      "Example splitting:\n",
      "  Input:  'apple,banana,cherry'\n",
      "  Output: ['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"2.3 CharDelimiterSplit (Custom delimiter)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "CharDelimiterSplit\n",
    "------------------\n",
    "- Split on custom character delimiter\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.pre_tokenizers import CharDelimiterSplit\n",
    "    tokenizer.pre_tokenizer = CharDelimiterSplit(delimiter=',')\n",
    "\n",
    "Parameters:\n",
    "    - delimiter (str): Character to split on\n",
    "\n",
    "Example:\n",
    "    delimiter = ','\n",
    "    Input:  \"apple,banana,cherry\"\n",
    "    Output: [\"apple\", \"banana\", \"cherry\"]\n",
    "\"\"\"\n",
    "\n",
    "print(\"CharDelimiterSplit(delimiter=',')\")\n",
    "print(\"\\nExample splitting:\")\n",
    "print(\"  Input:  'apple,banana,cherry'\")\n",
    "print(\"  Output: ['apple', 'banana', 'cherry']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8096d6-711c-41a5-9629-abf08e4d64fa",
   "metadata": {},
   "source": [
    "### NORMALIZERS - Pre processing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9498b9f-8e97-4058-b76d-6d429396bc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "PART 3: NORMALIZERS (Text cleaning/standardization)\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3.1 Lowercase (Convert to lowercase)\n",
      "--------------------------------------------------------------------------------\n",
      "Lowercase normalizer\n",
      "\n",
      "Example:\n",
      "  Input:  'Hello WORLD'\n",
      "  Output: 'hello world'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. NORMALIZERS (Clean/standardize text)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PART 3: NORMALIZERS (Text cleaning/standardization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"3.1 Lowercase (Convert to lowercase)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "Lowercase Normalizer\n",
    "--------------------\n",
    "- Converts all text to lowercase\n",
    "- Reduces vocabulary size\n",
    "- Loses case information\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.normalizers import Lowercase\n",
    "    tokenizer.normalizer = Lowercase()\n",
    "\n",
    "Example:\n",
    "    Input:  \"Hello WORLD\"\n",
    "    Output: \"hello world\"\n",
    "\n",
    "When to use:\n",
    "    ✓ Case doesn't matter\n",
    "    ✓ Reduce vocabulary\n",
    "    ✗ Case is meaningful (names, acronyms)\n",
    "\"\"\"\n",
    "\n",
    "from tokenizers.normalizers import Lowercase\n",
    "print(\"Lowercase normalizer\")\n",
    "print(\"\\nExample:\")\n",
    "print(\"  Input:  'Hello WORLD'\")\n",
    "print(\"  Output: 'hello world'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01c1d1a5-2668-4cfc-87ef-940b4443c281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3.2 NFD (Unicode Normalization)\n",
      "--------------------------------------------------------------------------------\n",
      "NFD normalizer\n",
      "\n",
      "Example:\n",
      "  Input:  'café' (é as single char)\n",
      "  Output: 'café' (e + combining accent)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"3.2 NFD (Unicode Normalization)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "NFD (Unicode Normalization Form D)\n",
    "-----------------------------------\n",
    "- Decomposes unicode characters\n",
    "- Example: é → e + ́ (e + combining accent)\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.normalizers import NFD\n",
    "    tokenizer.normalizer = NFD()\n",
    "\n",
    "Example:\n",
    "    Input:  \"café\"  (é is single character U+00E9)\n",
    "    Output: \"café\"  (e + ́ is two characters U+0065 + U+0301)\n",
    "\n",
    "When to use:\n",
    "    ✓ Handling accented characters\n",
    "    ✓ Before StripAccents\n",
    "\"\"\"\n",
    "\n",
    "print(\"NFD normalizer\")\n",
    "print(\"\\nExample:\")\n",
    "print(\"  Input:  'café' (é as single char)\")\n",
    "print(\"  Output: 'café' (e + combining accent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cc39b59-d59e-47f9-9ec6-df8bd5a935f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3.3 StripAccents (Remove accents)\n",
      "--------------------------------------------------------------------------------\n",
      "StripAccents normalizer\n",
      "\n",
      "Example:\n",
      "  Input:  'café naïve résumé'\n",
      "  Output: 'cafe naive resume'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"3.3 StripAccents (Remove accents)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "StripAccents\n",
    "------------\n",
    "- Removes diacritical marks/accents\n",
    "- Usually used after NFD\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.normalizers import StripAccents\n",
    "    tokenizer.normalizer = StripAccents()\n",
    "\n",
    "Example:\n",
    "    Input:  \"café naïve\"\n",
    "    Output: \"cafe naive\"\n",
    "\n",
    "When to use:\n",
    "    ✓ Ignore accents\n",
    "    ✓ Treat café = cafe\n",
    "\"\"\"\n",
    "\n",
    "print(\"StripAccents normalizer\")\n",
    "print(\"\\nExample:\")\n",
    "print(\"  Input:  'café naïve résumé'\")\n",
    "print(\"  Output: 'cafe naive resume'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2633de38-7df2-4d57-b2ac-3216c8faa0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3.4 Sequence (Combine multiple normalizers)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sequence normalizer (NFD → Lowercase → StripAccents)\n",
      "\n",
      "Example pipeline:\n",
      "  Input:  'CAFÉ Naïve'\n",
      "  → NFD:  'CAFÉ Naïve' (decompose)\n",
      "  → Lowercase: 'café naïve'\n",
      "  → StripAccents: 'cafe naive'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"3.4 Sequence (Combine multiple normalizers)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "Sequence\n",
    "--------\n",
    "- Chains multiple normalizers\n",
    "- Applies in order\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.normalizers import Sequence, NFD, Lowercase, StripAccents\n",
    "    tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "Example pipeline:\n",
    "    Input:  \"CAFÉ\"\n",
    "    → NFD:  \"CAFÉ\" (decompose é)\n",
    "    → Lowercase: \"café\"\n",
    "    → StripAccents: \"cafe\"\n",
    "\"\"\"\n",
    "print()\n",
    "from tokenizers.normalizers import Sequence, NFD, StripAccents\n",
    "normalizer_seq = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "print(\"Sequence normalizer (NFD → Lowercase → StripAccents)\")\n",
    "print(\"\\nExample pipeline:\")\n",
    "print(\"  Input:  'CAFÉ Naïve'\")\n",
    "print(\"  → NFD:  'CAFÉ Naïve' (decompose)\")\n",
    "print(\"  → Lowercase: 'café naïve'\")\n",
    "print(\"  → StripAccents: 'cafe naive'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29effa6-cc67-460a-8944-80d47656a11d",
   "metadata": {},
   "source": [
    "### Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3df4ca3d-1fda-468b-8fcd-d15c581fa71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "PART 4: TRAINERS (Building vocabulary)\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4.1 WordLevelTrainer\n",
      "--------------------------------------------------------------------------------\n",
      "WordLevelTrainer created\n",
      "  Special tokens: [UNK], [PAD], [SOS], [EOS]\n",
      "  min_frequency: 2 (ignore words appearing once)\n",
      "  vocab_size: 10000 (max vocabulary size)\n",
      "\n",
      "Training process:\n",
      "  1. Count: {'cat': 5, 'dog': 3, 'python': 1, ...}\n",
      "  2. Filter: Keep cat (5≥2), dog (3≥2), drop python (1<2)\n",
      "  3. Build vocab: {[UNK]:0, [PAD]:1, [SOS]:2, [EOS]:3, cat:4, dog:5, ...}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. TRAINERS (How to build vocabulary)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PART 4: TRAINERS (Building vocabulary)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4.1 WordLevelTrainer\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "WordLevelTrainer\n",
    "----------------\n",
    "- Trains WordLevel tokenizer\n",
    "- Builds vocabulary from whole words\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.trainers import WordLevelTrainer\n",
    "    trainer = WordLevelTrainer(\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "        min_frequency=2,\n",
    "        vocab_size=30000\n",
    "    )\n",
    "\n",
    "Parameters:\n",
    "    - special_tokens (List[str]): Special tokens to add. Default: []\n",
    "    - min_frequency (int): Minimum word frequency. Default: 0\n",
    "    - vocab_size (int, optional): Maximum vocabulary size\n",
    "    - show_progress (bool): Show progress bar. Default: True\n",
    "\n",
    "Training process:\n",
    "    1. Scan all sentences\n",
    "    2. Count word frequencies\n",
    "    3. Keep words with frequency ≥ min_frequency\n",
    "    4. Assign unique ID to each word\n",
    "\"\"\"\n",
    "\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "trainer_wl = WordLevelTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "    min_frequency=2,\n",
    "    vocab_size=10000\n",
    ")\n",
    "\n",
    "print(\"WordLevelTrainer created\")\n",
    "print(\"  Special tokens: [UNK], [PAD], [SOS], [EOS]\")\n",
    "print(\"  min_frequency: 2 (ignore words appearing once)\")\n",
    "print(\"  vocab_size: 10000 (max vocabulary size)\")\n",
    "print(\"\\nTraining process:\")\n",
    "print(\"  1. Count: {'cat': 5, 'dog': 3, 'python': 1, ...}\")\n",
    "print(\"  2. Filter: Keep cat (5≥2), dog (3≥2), drop python (1<2)\")\n",
    "print(\"  3. Build vocab: {[UNK]:0, [PAD]:1, [SOS]:2, [EOS]:3, cat:4, dog:5, ...}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9970c177-4d8e-4193-bb36-949de4a0d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4.2 BpeTrainer\n",
      "--------------------------------------------------------------------------------\n",
      "BpeTrainer created\n",
      "  vocab_size: 30000\n",
      "  min_frequency: 2\n",
      "\n",
      "Training process:\n",
      "  1. Start: {h, e, l, o, ...}\n",
      "  2. Most frequent pair: (l, o) → merge to 'lo'\n",
      "  3. Continue: {h, e, lo, ...} → (h, e) → 'he'\n",
      "  4. Final vocab: {a, b, c, ..., he, lo, hello, ...}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4.2 BpeTrainer\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "BpeTrainer\n",
    "----------\n",
    "- Trains BPE tokenizer\n",
    "- Learns subword merges\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.trainers import BpeTrainer\n",
    "    trainer = BpeTrainer(\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\"],\n",
    "        vocab_size=30000,\n",
    "        min_frequency=2\n",
    "    )\n",
    "\n",
    "Parameters:\n",
    "    - special_tokens (List[str]): Special tokens\n",
    "    - vocab_size (int): Target vocabulary size. Default: 30000\n",
    "    - min_frequency (int): Min frequency. Default: 0\n",
    "    - show_progress (bool): Show progress. Default: True\n",
    "    - continuing_subword_prefix (str): Subword prefix. Default: \"##\"\n",
    "\n",
    "Training process:\n",
    "    1. Start with characters\n",
    "    2. Count pair frequencies\n",
    "    3. Merge most frequent pair\n",
    "    4. Repeat until vocab_size reached\n",
    "\"\"\"\n",
    "\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer_bpe = BpeTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2\n",
    ")\n",
    "\n",
    "print(\"BpeTrainer created\")\n",
    "print(\"  vocab_size: 30000\")\n",
    "print(\"  min_frequency: 2\")\n",
    "print(\"\\nTraining process:\")\n",
    "print(\"  1. Start: {h, e, l, o, ...}\")\n",
    "print(\"  2. Most frequent pair: (l, o) → merge to 'lo'\")\n",
    "print(\"  3. Continue: {h, e, lo, ...} → (h, e) → 'he'\")\n",
    "print(\"  4. Final vocab: {a, b, c, ..., he, lo, hello, ...}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75055702-0157-459b-85f3-d4131d98e155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4.3 WordPieceTrainer\n",
      "--------------------------------------------------------------------------------\n",
      "WordPieceTrainer created\n",
      "  Special tokens: BERT-style [CLS], [SEP], [MASK]\n",
      "  Uses ## prefix for subwords\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4.3 WordPieceTrainer\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "WordPieceTrainer\n",
    "----------------\n",
    "- Trains WordPiece tokenizer\n",
    "- BERT-style subword tokenization\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.trainers import WordPieceTrainer\n",
    "    trainer = WordPieceTrainer(\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "        vocab_size=30000\n",
    "    )\n",
    "\n",
    "Parameters:\n",
    "    - special_tokens (List[str]): Special tokens\n",
    "    - vocab_size (int): Vocabulary size. Default: 30000\n",
    "    - min_frequency (int): Min frequency. Default: 0\n",
    "    - continuing_subword_prefix (str): Prefix. Default: \"##\"\n",
    "\"\"\"\n",
    "\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer_wp = WordPieceTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    vocab_size=30000\n",
    ")\n",
    "\n",
    "print(\"WordPieceTrainer created\")\n",
    "print(\"  Special tokens: BERT-style [CLS], [SEP], [MASK]\")\n",
    "print(\"  Uses ## prefix for subwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bf60974-efe3-4a4a-bb93-06a83580b9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4.4 UnigramTrainer\n",
      "--------------------------------------------------------------------------------\n",
      "UnigramTrainer created\n",
      "  vocab_size: 8000\n",
      "  Probabilistic tokenization\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4.4 UnigramTrainer\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "\"\"\"\n",
    "UnigramTrainer\n",
    "--------------\n",
    "- Trains Unigram tokenizer\n",
    "- Probabilistic approach\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.trainers import UnigramTrainer\n",
    "    trainer = UnigramTrainer(\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\"],\n",
    "        vocab_size=8000\n",
    "    )\n",
    "\n",
    "Parameters:\n",
    "    - special_tokens (List[str]): Special tokens\n",
    "    - vocab_size (int): Vocabulary size. Default: 8000\n",
    "\"\"\"\n",
    "\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "trainer_uni = UnigramTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "    vocab_size=8000\n",
    ")\n",
    "\n",
    "print(\"UnigramTrainer created\")\n",
    "print(\"  vocab_size: 8000\")\n",
    "print(\"  Probabilistic tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20a351-3d94-4986-8686-a924f7ee2fcd",
   "metadata": {},
   "source": [
    "### POST PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fde08f41-9fa1-4c7a-81f2-f9c5e204b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "PART 5: POST-PROCESSORS (Add special tokens to sequences)\n",
      "================================================================================\n",
      "\n",
      "TemplateProcessing\n",
      "--------------------------------------------------------------------------------\n",
      "Adds special tokens around sequences\n",
      "\n",
      "BERT-style example:\n",
      "  Template: '[CLS] $A [SEP]'\n",
      "  Input: 'Hello world'\n",
      "  Output: '[CLS] Hello world [SEP]'\n",
      "\n",
      "Sequence pair (for classification):\n",
      "  Template: '[CLS] $A [SEP] $B [SEP]'\n",
      "  Input: ('This is sentence 1', 'This is sentence 2')\n",
      "  Output: '[CLS] This is sentence 1 [SEP] This is sentence 2 [SEP]'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. POST-PROCESSORS (Add special tokens around sequences)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PART 5: POST-PROCESSORS (Add special tokens to sequences)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\"\"\"\n",
    "TemplateProcessing\n",
    "------------------\n",
    "- Adds special tokens around tokenized sequences\n",
    "- Used for BERT, GPT, etc.\n",
    "\n",
    "Syntax:\n",
    "    from tokenizers.processors import TemplateProcessing\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B [SEP]\",\n",
    "        special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)]\n",
    "    )\n",
    "\n",
    "Parameters:\n",
    "    - single (str): Template for single sequence\n",
    "        $A = first sequence\n",
    "    - pair (str): Template for sequence pair\n",
    "        $A = first sequence, $B = second sequence\n",
    "    - special_tokens (List[Tuple[str, int]]): Special tokens and their IDs\n",
    "\n",
    "Example:\n",
    "    Single sequence:\n",
    "        Input: \"Hello world\"\n",
    "        Template: \"[CLS] $A [SEP]\"\n",
    "        Output: \"[CLS] Hello world [SEP]\"\n",
    "    \n",
    "    Sequence pair:\n",
    "        Input: (\"Hello\", \"world\")\n",
    "        Template: \"[CLS] $A [SEP] $B [SEP]\"\n",
    "        Output: \"[CLS] Hello [SEP] world [SEP]\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nTemplateProcessing\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Adds special tokens around sequences\")\n",
    "print(\"\\nBERT-style example:\")\n",
    "print(\"  Template: '[CLS] $A [SEP]'\")\n",
    "print(\"  Input: 'Hello world'\")\n",
    "print(\"  Output: '[CLS] Hello world [SEP]'\")\n",
    "print(\"\\nSequence pair (for classification):\")\n",
    "print(\"  Template: '[CLS] $A [SEP] $B [SEP]'\")\n",
    "print(\"  Input: ('This is sentence 1', 'This is sentence 2')\")\n",
    "print(\"  Output: '[CLS] This is sentence 1 [SEP] This is sentence 2 [SEP]'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b06f6c64-24a6-4559-bcbc-867d3cd6c756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "PART 6: COMPLETE WORKING EXAMPLE \n",
      "================================================================================\n",
      "\n",
      "Training data:\n",
      "  1. I love machine learning and deep learning\n",
      "  2. Transformers are amazing for NLP tasks\n",
      "  3. I love natural language processing\n",
      "  4. Deep learning models are powerful\n",
      "  5. Machine learning is everywhere\n",
      "  6. I love transformers and attention mechanisms\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Building WordLevel tokenizer...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Training tokenizer...\n",
      "✓ Tokenizer trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. COMPLETE WORKING EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PART 6: COMPLETE WORKING EXAMPLE \")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sample training data\n",
    "training_data = [\n",
    "    \"I love machine learning and deep learning\",\n",
    "    \"Transformers are amazing for NLP tasks\",\n",
    "    \"I love natural language processing\",\n",
    "    \"Deep learning models are powerful\",\n",
    "    \"Machine learning is everywhere\",\n",
    "    \"I love transformers and attention mechanisms\"\n",
    "]\n",
    "\n",
    "print(\"\\nTraining data:\")\n",
    "for i, text in enumerate(training_data, 1):\n",
    "    print(f\"  {i}. {text}\")\n",
    "\n",
    "# Create and train tokenizer\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Building WordLevel tokenizer...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "tokenizer_object = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer_object.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer_object.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_object = WordLevelTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "    min_frequency=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining tokenizer...\")\n",
    "tokenizer_object.train_from_iterator(training_data, trainer=trainer_object)\n",
    "\n",
    "# Add post-processor\n",
    "tokenizer_object.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    special_tokens=[(\"[SOS]\", 2), (\"[EOS]\", 3)]\n",
    ")\n",
    "\n",
    "print(\"✓ Tokenizer trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7176dbf5-0ad1-49a7-98b5-6e615d04910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size: 25\n",
      "\n",
      "Vocabulary:\n",
      "   0: [UNK]\n",
      "   1: [PAD]\n",
      "   2: [SOS]\n",
      "   3: [EOS]\n",
      "   4: learning\n",
      "   5: i\n",
      "   6: love\n",
      "   7: and\n",
      "   8: are\n",
      "   9: deep\n",
      "  10: machine\n",
      "  11: transformers\n",
      "  12: amazing\n",
      "  13: attention\n",
      "  14: everywhere\n",
      "  15: for\n",
      "  16: is\n",
      "  17: language\n",
      "  18: mechanisms\n",
      "  19: models\n",
      "  20: natural\n",
      "  21: nlp\n",
      "  22: powerful\n",
      "  23: processing\n",
      "  24: tasks\n"
     ]
    }
   ],
   "source": [
    "# Show vocabulary\n",
    "vocab = tokenizer_object.get_vocab()\n",
    "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "print(\"\\nVocabulary:\")\n",
    "for token, idx in sorted(vocab.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {idx:2d}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8befb075-075d-4629-98c9-ebb710113f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Testing tokenization\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: 'I love transformers'\n",
      "  Tokens: ['[SOS]', 'i', 'love', 'transformers', '[EOS]']\n",
      "  IDs: [2, 5, 6, 11, 3]\n",
      "  Decoded: 'i love transformers'\n",
      "\n",
      "Text: 'Deep learning is amazing'\n",
      "  Tokens: ['[SOS]', 'deep', 'learning', 'is', 'amazing', '[EOS]']\n",
      "  IDs: [2, 9, 4, 16, 12, 3]\n",
      "  Decoded: 'deep learning is amazing'\n",
      "\n",
      "Text: 'Python programming'\n",
      "  Tokens: ['[SOS]', '[UNK]', '[UNK]', '[EOS]']\n",
      "  IDs: [2, 0, 0, 3]\n",
      "  Decoded: ''\n"
     ]
    }
   ],
   "source": [
    "# Test tokenization\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Testing tokenization\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "test_texts = [\n",
    "    \"I love transformers\",\n",
    "    \"Deep learning is amazing\",\n",
    "    \"Python programming\"  # 'python' and 'programming' not in training data\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    encoded = tokenizer_object.encode(text)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"  Tokens: {encoded.tokens}\")\n",
    "    print(f\"  IDs: {encoded.ids}\")\n",
    "    decoded = tokenizer_object.decode(encoded.ids)\n",
    "    print(f\"  Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a837970-b314-4378-a9ad-492f9a283ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Saving and loading tokenizer\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Saved to: ./example_tokenizer.json\n",
      "✓ Loaded from: ./example_tokenizer.json\n",
      "\n",
      "Verification:\n",
      "  Original: [2, 5, 6, 10, 4, 3]\n",
      "  Loaded:   [2, 5, 6, 10, 4, 3]\n",
      "  Match: True ✓\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save and load\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Saving and loading tokenizer\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "save_path = \"./example_tokenizer.json\"\n",
    "tokenizer_object.save(save_path)\n",
    "print(f\"✓ Saved to: {save_path}\")\n",
    "\n",
    "loaded_tokenizer = Tokenizer.from_file(save_path)\n",
    "print(f\"✓ Loaded from: {save_path}\")\n",
    "\n",
    "# Verify loaded tokenizer works\n",
    "test_text = \"I love machine learning\"\n",
    "original_encoded = tokenizer_object.encode(test_text)\n",
    "loaded_encoded = loaded_tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  Original: {original_encoded.ids}\")\n",
    "print(f\"  Loaded:   {loaded_encoded.ids}\")\n",
    "print(f\"  Match: {original_encoded.ids == loaded_encoded.ids} ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10613d66-d724-4dca-bf56-36a875c0e43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "PART 7: COMMON TOKENIZER METHODS\n",
      "================================================================================\n",
      "\n",
      "Tokenizer Object Methods:\n",
      "------------------------\n",
      "\n",
      "1. tokenizer.encode(text)\n",
      "   - Encodes text to token IDs\n",
      "   - Returns: Encoding object\n",
      "   - Example: tokenizer.encode(\"Hello world\")\n",
      "\n",
      "2. tokenizer.encode_batch(texts)\n",
      "   - Encodes multiple texts\n",
      "   - Returns: List[Encoding]\n",
      "   - Example: tokenizer.encode_batch([\"Hello\", \"World\"])\n",
      "\n",
      "3. tokenizer.decode(ids)\n",
      "   - Decodes token IDs back to text\n",
      "   - Returns: str\n",
      "   - Example: tokenizer.decode([1, 2, 3])\n",
      "\n",
      "4. tokenizer.decode_batch(sequences)\n",
      "   - Decodes multiple sequences\n",
      "   - Returns: List[str]\n",
      "\n",
      "5. tokenizer.get_vocab()\n",
      "   - Returns vocabulary dictionary\n",
      "   - Returns: Dict[str, int]\n",
      "   - Example: {\"hello\": 0, \"world\": 1}\n",
      "\n",
      "6. tokenizer.get_vocab_size()\n",
      "   - Returns vocabulary size\n",
      "   - Returns: int\n",
      "\n",
      "7. tokenizer.token_to_id(token)\n",
      "   - Get ID of specific token\n",
      "   - Returns: int or None\n",
      "\n",
      "8. tokenizer.id_to_token(id)\n",
      "   - Get token from ID\n",
      "   - Returns: str or None\n",
      "\n",
      "9. tokenizer.save(path)\n",
      "   - Save tokenizer to file\n",
      "   - Example: tokenizer.save(\"tokenizer.json\")\n",
      "\n",
      "10. Tokenizer.from_file(path)\n",
      "    - Load tokenizer from file\n",
      "    - Example: Tokenizer.from_file(\"tokenizer.json\")\n",
      "\n",
      "11. tokenizer.train_from_iterator(iterator, trainer)\n",
      "    - Train from text iterator\n",
      "    - Example: tokenizer.train_from_iterator(texts, trainer)\n",
      "\n",
      "12. tokenizer.enable_padding(pad_id, pad_token)\n",
      "    - Enable padding\n",
      "    - Example: tokenizer.enable_padding(pad_id=1, pad_token=\"[PAD]\")\n",
      "\n",
      "13. tokenizer.enable_truncation(max_length)\n",
      "    - Enable truncation\n",
      "    - Example: tokenizer.enable_truncation(max_length=512)\n",
      "\n",
      "Encoding Object Properties:\n",
      "---------------------------\n",
      "encoding.ids          - List of token IDs\n",
      "encoding.tokens       - List of token strings\n",
      "encoding.offsets      - Character offsets\n",
      "encoding.attention_mask - Attention mask (for padding)\n",
      "encoding.type_ids     - Token type IDs (for BERT)\n",
      "\n",
      "================================================================================\n",
      "TUTORIAL COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Key takeaways:\n",
      "  1. Models: WordLevel, BPE, WordPiece, Unigram\n",
      "  2. Pre-tokenizers: Split text (Whitespace, ByteLevel, etc.)\n",
      "  3. Normalizers: Clean text (Lowercase, StripAccents, etc.)\n",
      "  4. Trainers: Build vocabulary (min_frequency, vocab_size)\n",
      "  5. Post-processors: Add special tokens ([CLS], [SEP], etc.)\n",
      "  6. Save/Load: .save() and .from_file()\n",
      "\n",
      "You now have a complete understanding of Hugging Face tokenizers!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 7. COMMON METHODS REFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PART 7: COMMON TOKENIZER METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Tokenizer Object Methods:\n",
    "------------------------\n",
    "\n",
    "1. tokenizer.encode(text)\n",
    "   - Encodes text to token IDs\n",
    "   - Returns: Encoding object\n",
    "   - Example: tokenizer.encode(\"Hello world\")\n",
    "\n",
    "2. tokenizer.encode_batch(texts)\n",
    "   - Encodes multiple texts\n",
    "   - Returns: List[Encoding]\n",
    "   - Example: tokenizer.encode_batch([\"Hello\", \"World\"])\n",
    "\n",
    "3. tokenizer.decode(ids)\n",
    "   - Decodes token IDs back to text\n",
    "   - Returns: str\n",
    "   - Example: tokenizer.decode([1, 2, 3])\n",
    "\n",
    "4. tokenizer.decode_batch(sequences)\n",
    "   - Decodes multiple sequences\n",
    "   - Returns: List[str]\n",
    "\n",
    "5. tokenizer.get_vocab()\n",
    "   - Returns vocabulary dictionary\n",
    "   - Returns: Dict[str, int]\n",
    "   - Example: {\"hello\": 0, \"world\": 1}\n",
    "\n",
    "6. tokenizer.get_vocab_size()\n",
    "   - Returns vocabulary size\n",
    "   - Returns: int\n",
    "\n",
    "7. tokenizer.token_to_id(token)\n",
    "   - Get ID of specific token\n",
    "   - Returns: int or None\n",
    "\n",
    "8. tokenizer.id_to_token(id)\n",
    "   - Get token from ID\n",
    "   - Returns: str or None\n",
    "\n",
    "9. tokenizer.save(path)\n",
    "   - Save tokenizer to file\n",
    "   - Example: tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "10. Tokenizer.from_file(path)\n",
    "    - Load tokenizer from file\n",
    "    - Example: Tokenizer.from_file(\"tokenizer.json\")\n",
    "\n",
    "11. tokenizer.train_from_iterator(iterator, trainer)\n",
    "    - Train from text iterator\n",
    "    - Example: tokenizer.train_from_iterator(texts, trainer)\n",
    "\n",
    "12. tokenizer.enable_padding(pad_id, pad_token)\n",
    "    - Enable padding\n",
    "    - Example: tokenizer.enable_padding(pad_id=1, pad_token=\"[PAD]\")\n",
    "\n",
    "13. tokenizer.enable_truncation(max_length)\n",
    "    - Enable truncation\n",
    "    - Example: tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "Encoding Object Properties:\n",
    "---------------------------\n",
    "encoding.ids          - List of token IDs\n",
    "encoding.tokens       - List of token strings\n",
    "encoding.offsets      - Character offsets\n",
    "encoding.attention_mask - Attention mask (for padding)\n",
    "encoding.type_ids     - Token type IDs (for BERT)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TUTORIAL COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"  1. Models: WordLevel, BPE, WordPiece, Unigram\")\n",
    "print(\"  2. Pre-tokenizers: Split text (Whitespace, ByteLevel, etc.)\")\n",
    "print(\"  3. Normalizers: Clean text (Lowercase, StripAccents, etc.)\")\n",
    "print(\"  4. Trainers: Build vocabulary (min_frequency, vocab_size)\")\n",
    "print(\"  5. Post-processors: Add special tokens ([CLS], [SEP], etc.)\")\n",
    "print(\"  6. Save/Load: .save() and .from_file()\")\n",
    "print(\"\\nYou now have a complete understanding of Hugging Face tokenizers!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4ac82-0d9a-468a-9e77-3fb042b538ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70979422-954e-4682-b020-f37f657dcc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f956e1b-d8c7-4636-9d12-d8b4563e824e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

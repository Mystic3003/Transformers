{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d6be00-3340-424e-99b3-850a6740fb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 1: UNDERSTANDING PyTorch Dataset Class\n",
      "================================================================================\n",
      "\n",
      "PyTorch Dataset requires 3 methods:\n",
      "  1. __init__()  ‚Üí Initialize\n",
      "  2. __len__()   ‚Üí Return dataset size\n",
      "  3. __getitem__(idx) ‚Üí Return sample at index idx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BILINGUAL DATASET FOR TRANSFORMER \n",
    "======================================================\n",
    "This tutorial explains how to prepare data for machine translation training.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 1: UNDERSTANDING PyTorch Dataset Class\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\"\"\"\n",
    "PyTorch Dataset Base Class\n",
    "---------------------------\n",
    "All custom datasets inherit from torch.utils.data.Dataset and must implement:\n",
    "\n",
    "1. __init__(self, ...)\n",
    "   - Initialize dataset\n",
    "   - Store data, tokenizers, parameters\n",
    "\n",
    "2. __len__(self)\n",
    "   - Return number of samples in dataset\n",
    "   - Used by DataLoader to know dataset size\n",
    "\n",
    "3. __getitem__(self, idx)\n",
    "   - Return one sample at index idx\n",
    "   - Called by DataLoader during training\n",
    "   - Must return: input data + labels\n",
    "\n",
    "Reference: https://pytorch.org/docs/stable/data.html\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "PyTorch Dataset requires 3 methods:\n",
    "  1. __init__()  ‚Üí Initialize\n",
    "  2. __len__()   ‚Üí Return dataset size\n",
    "  3. __getitem__(idx) ‚Üí Return sample at index idx\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51cbf12c-03ab-4e7a-8cf8-b636bdccbaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2: CREATE SAMPLE TRANSLATION DATASET\n",
      "================================================================================\n",
      "Sample dataset:\n",
      "  0. EN: 'I love cats'\n",
      "     FR: 'J'aime les chats'\n",
      "  1. EN: 'I love dogs'\n",
      "     FR: 'J'aime les chiens'\n",
      "  2. EN: 'Hello world'\n",
      "     FR: 'Bonjour le monde'\n",
      "  3. EN: 'Machine learning is amazing'\n",
      "     FR: 'L'apprentissage automatique est incroyable'\n",
      "  4. EN: 'Transformers are powerful'\n",
      "     FR: 'Les transformateurs sont puissants'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 2: CREATE SAMPLE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: CREATE SAMPLE TRANSLATION DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sample English-French translation pairs\n",
    "sample_data = [\n",
    "    {\"translation\": {\"en\": \"I love cats\", \"fr\": \"J'aime les chats\"}},\n",
    "    {\"translation\": {\"en\": \"I love dogs\", \"fr\": \"J'aime les chiens\"}},\n",
    "    {\"translation\": {\"en\": \"Hello world\", \"fr\": \"Bonjour le monde\"}},\n",
    "    {\"translation\": {\"en\": \"Machine learning is amazing\", \"fr\": \"L'apprentissage automatique est incroyable\"}},\n",
    "    {\"translation\": {\"en\": \"Transformers are powerful\", \"fr\": \"Les transformateurs sont puissants\"}},\n",
    "]\n",
    "\n",
    "print(\"Sample dataset:\")\n",
    "for i, pair in enumerate(sample_data):\n",
    "    en = pair['translation']['en']\n",
    "    fr = pair['translation']['fr']\n",
    "    print(f\"  {i}. EN: '{en}'\")\n",
    "    print(f\"     FR: '{fr}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbf37a6-4509-4565-b605-fc7d2e354226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3: CREATE TOKENIZERS\n",
      "================================================================================\n",
      "\n",
      "Creating English tokenizer...\n",
      "‚úì English tokenizer trained\n",
      "  Vocabulary size: 17\n",
      "\n",
      "Creating French tokenizer...\n",
      "‚úì French tokenizer trained\n",
      "  Vocabulary size: 22\n",
      "\n",
      "English vocabulary:\n",
      "   0: [UNK]\n",
      "   1: [PAD]\n",
      "   2: [SOS]\n",
      "   3: [EOS]\n",
      "   4: I\n",
      "   5: love\n",
      "   6: Hello\n",
      "   7: Machine\n",
      "   8: Transformers\n",
      "   9: amazing\n",
      "  10: are\n",
      "  11: cats\n",
      "  12: dogs\n",
      "  13: is\n",
      "  14: learning\n",
      "\n",
      "French vocabulary:\n",
      "   0: [UNK]\n",
      "   1: [PAD]\n",
      "   2: [SOS]\n",
      "   3: [EOS]\n",
      "   4: '\n",
      "   5: J\n",
      "   6: aime\n",
      "   7: les\n",
      "   8: Bonjour\n",
      "   9: L\n",
      "  10: Les\n",
      "  11: apprentissage\n",
      "  12: automatique\n",
      "  13: chats\n",
      "  14: chiens\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3: CREATE TOKENIZERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: CREATE TOKENIZERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    \"\"\"Extract all sentences for a language\"\"\"\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang] ## return one sentence at a time\n",
    "\n",
    "# Create English tokenizer\n",
    "print(\"\\nCreating English tokenizer...\")\n",
    "tokenizer_en = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "trainer_en = WordLevelTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "    min_frequency=1  # Keep all words (small dataset)\n",
    ")\n",
    "tokenizer_en.train_from_iterator(get_all_sentences(sample_data, \"en\"), trainer=trainer_en)\n",
    "\n",
    "print(\"‚úì English tokenizer trained\")\n",
    "print(f\"  Vocabulary size: {tokenizer_en.get_vocab_size()}\")\n",
    "\n",
    "# Create French tokenizer\n",
    "print(\"\\nCreating French tokenizer...\")\n",
    "tokenizer_fr = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer_fr.pre_tokenizer = Whitespace()\n",
    "trainer_fr = WordLevelTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "    min_frequency=1\n",
    ")\n",
    "tokenizer_fr.train_from_iterator(get_all_sentences(sample_data, \"fr\"), trainer=trainer_fr)\n",
    "\n",
    "print(\"‚úì French tokenizer trained\")\n",
    "print(f\"  Vocabulary size: {tokenizer_fr.get_vocab_size()}\")\n",
    "\n",
    "# Show vocabularies\n",
    "print(\"\\nEnglish vocabulary:\")\n",
    "en_vocab = tokenizer_en.get_vocab()\n",
    "for token, idx in sorted(en_vocab.items(), key=lambda x: x[1])[:15]:\n",
    "    print(f\"  {idx:2d}: {token}\")\n",
    "\n",
    "print(\"\\nFrench vocabulary:\")\n",
    "fr_vocab = tokenizer_fr.get_vocab()\n",
    "for token, idx in sorted(fr_vocab.items(), key=lambda x: x[1])[:15]:\n",
    "    print(f\"  {idx:2d}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "258cc8aa-6ad6-4567-a8b3-8c18c3ff391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 4: SPECIAL TOKENS EXPLAINED\n",
      "================================================================================\n",
      "\n",
      "Special Tokens in Machine Translation:\n",
      "---------------------------------------\n",
      "\n",
      "[UNK] (Unknown)   - ID: 0\n",
      "  - Replaces unknown/rare words not in vocabulary\n",
      "  - Example: \"xylophone\" ‚Üí [UNK] if not trained on it\n",
      "\n",
      "[PAD] (Padding)   - ID: 1\n",
      "  - Fills shorter sentences to match seq_len\n",
      "  - Masked out in attention (ignored)\n",
      "  - Example: \"Hello\" + [PAD][PAD][PAD] ‚Üí length 4\n",
      "\n",
      "[SOS] (Start)     - ID: 2\n",
      "  - Marks beginning of sequence\n",
      "  - Tells model \"sentence starts here\"\n",
      "  - Added to encoder input & decoder input\n",
      "\n",
      "[EOS] (End)       - ID: 3\n",
      "  - Marks end of sequence\n",
      "  - Tells model \"sentence ends here\"\n",
      "  - Added to encoder input & label (not decoder input!)\n",
      "\n",
      "Token IDs:\n",
      "  [SOS]: 2\n",
      "  [EOS]: 3\n",
      "  [PAD]: 1\n",
      "  [UNK]: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 4: UNDERSTANDING SPECIAL TOKENS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4: SPECIAL TOKENS EXPLAINED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Special Tokens in Machine Translation:\n",
    "---------------------------------------\n",
    "\n",
    "[UNK] (Unknown)   - ID: 0\n",
    "  - Replaces unknown/rare words not in vocabulary\n",
    "  - Example: \"xylophone\" ‚Üí [UNK] if not trained on it\n",
    "\n",
    "[PAD] (Padding)   - ID: 1\n",
    "  - Fills shorter sentences to match seq_len\n",
    "  - Masked out in attention (ignored)\n",
    "  - Example: \"Hello\" + [PAD][PAD][PAD] ‚Üí length 4\n",
    "\n",
    "[SOS] (Start)     - ID: 2\n",
    "  - Marks beginning of sequence\n",
    "  - Tells model \"sentence starts here\"\n",
    "  - Added to encoder input & decoder input\n",
    "\n",
    "[EOS] (End)       - ID: 3\n",
    "  - Marks end of sequence\n",
    "  - Tells model \"sentence ends here\"\n",
    "  - Added to encoder input & label (not decoder input!)\n",
    "\"\"\")\n",
    "\n",
    "sos_token_id = tokenizer_fr.token_to_id(\"[SOS]\")\n",
    "eos_token_id = tokenizer_fr.token_to_id(\"[EOS]\")\n",
    "pad_token_id = tokenizer_fr.token_to_id(\"[PAD]\")\n",
    "unk_token_id = tokenizer_fr.token_to_id(\"[UNK]\")\n",
    "\n",
    "print(f\"Token IDs:\")\n",
    "print(f\"  [SOS]: {sos_token_id}\")\n",
    "print(f\"  [EOS]: {eos_token_id}\")\n",
    "print(f\"  [PAD]: {pad_token_id}\")\n",
    "print(f\"  [UNK]: {unk_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba351591-3326-449f-85d7-125636f224af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 5: CAUSAL MASK (Prevents looking ahead)\n",
      "================================================================================\n",
      "\n",
      "Causal Mask Purpose:\n",
      "-------------------\n",
      "During training, decoder sees full target sentence but should only use\n",
      "past tokens to predict next token (autoregressive generation).\n",
      "\n",
      "Example: Predicting \"J'aime les chats\"\n",
      "  Position 0 (J'):     Can see: nothing (only [SOS])\n",
      "  Position 1 (aime):   Can see: J'\n",
      "  Position 2 (les):    Can see: J', aime\n",
      "  Position 3 (chats):  Can see: J', aime, les\n",
      "\n",
      "\n",
      "Causal mask for size=3:\n",
      "tensor([[1, 0, 0],\n",
      "        [1, 1, 0],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "  1 = Can attend\n",
      "  0 = Cannot attend (future tokens)\n",
      "\n",
      "Causal mask for size=5:\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "  1 = Can attend\n",
      "  0 = Cannot attend (future tokens)\n",
      "\n",
      "Visual explanation (size=4):\n",
      "\n",
      "       Token 0  Token 1  Token 2  Token 3\n",
      "Token 0    1       0        0        0     ‚Üê Can only see itself\n",
      "Token 1    1       1        0        0     ‚Üê Can see 0, 1\n",
      "Token 2    1       1        1        0     ‚Üê Can see 0, 1, 2\n",
      "Token 3    1       1        1        1     ‚Üê Can see all (0, 1, 2, 3)\n",
      "\n",
      "Lower triangular = can attend to past\n",
      "Upper triangular = BLOCKED (cannot see future)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 5: CAUSAL MASK EXPLAINED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: CAUSAL MASK (Prevents looking ahead)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def causal_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a causal (upper triangular) mask for decoder self-attention.\n",
    "    Prevents positions from attending to future positions.\n",
    "    \n",
    "    Parameters:\n",
    "        size (int): Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Mask of shape (1, size, size)\n",
    "                     True = can attend, False = cannot attend\n",
    "    \"\"\"\n",
    "    # torch.triu creates upper triangular matrix\n",
    "    # diagonal=1 means start from 1st diagonal (exclude main diagonal)\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    # Invert: 0 becomes True (can attend), 1 becomes False (cannot)\n",
    "    return mask == 0\n",
    "\n",
    "print(\"\"\"\n",
    "Causal Mask Purpose:\n",
    "-------------------\n",
    "During training, decoder sees full target sentence but should only use\n",
    "past tokens to predict next token (autoregressive generation).\n",
    "\n",
    "Example: Predicting \"J'aime les chats\"\n",
    "  Position 0 (J'):     Can see: nothing (only [SOS])\n",
    "  Position 1 (aime):   Can see: J'\n",
    "  Position 2 (les):    Can see: J', aime\n",
    "  Position 3 (chats):  Can see: J', aime, les\n",
    "\"\"\")\n",
    "\n",
    "# Example causal masks\n",
    "for size in [3, 5]:\n",
    "    mask = causal_mask(size)\n",
    "    print(f\"\\nCausal mask for size={size}:\") ## size is the number of tokens in each sentence .\n",
    "    print(mask.squeeze(0).int())\n",
    "    print(\"  1 = Can attend\")\n",
    "    print(\"  0 = Cannot attend (future tokens)\")\n",
    "\n",
    "print(\"\\nVisual explanation (size=4):\")\n",
    "print(\"\"\"\n",
    "       Token 0  Token 1  Token 2  Token 3\n",
    "Token 0    1       0        0        0     ‚Üê Can only see itself\n",
    "Token 1    1       1        0        0     ‚Üê Can see 0, 1\n",
    "Token 2    1       1        1        0     ‚Üê Can see 0, 1, 2\n",
    "Token 3    1       1        1        1     ‚Üê Can see all (0, 1, 2, 3)\n",
    "\n",
    "Lower triangular = can attend to past\n",
    "Upper triangular = BLOCKED (cannot see future)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14a75984-065c-4ce4-8925-29dab5b76513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 6: BILINGUAL DATASET CLASS (Line-by-line)\n",
      "================================================================================\n",
      "‚úì BilingualDataset class defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 6: BILINGUAL DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 6: BILINGUAL DATASET CLASS (Line-by-line)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for machine translation training.\n",
    "    Prepares encoder input, decoder input, and labels with proper padding and masks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Parameters:\n",
    "            ds: Raw dataset (list of dicts with 'translation' key)\n",
    "            tokenizer_src: Tokenizer for source language\n",
    "            tokenizer_tgt: Tokenizer for target language\n",
    "            src_lang: Source language code (e.g., \"en\")\n",
    "            tgt_lang: Target language code (e.g., \"fr\")\n",
    "            seq_len: Maximum sequence length (all tensors padded to this)\n",
    "        \"\"\"\n",
    "        super().__init__()  # Call parent Dataset.__init__\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Store parameters\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        \n",
    "        # Create special token tensors (pre-computed for efficiency)\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return dataset size\"\"\"\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get one training sample.\n",
    "        \n",
    "        Returns dict with:\n",
    "            - encoder_input: Source sentence with [SOS] + tokens + [EOS] + padding\n",
    "            - decoder_input: Target sentence with [SOS] + tokens + padding\n",
    "            - label: Target sentence with tokens + [EOS] + padding (shifted by 1)\n",
    "            - encoder_mask: Padding mask for encoder , so the padded tokens are not considered for attention .\n",
    "            - decoder_mask: Combined padding + causal mask for decoder\n",
    "            - src_text: Original source text (for logging)\n",
    "            - tgt_text: Original target text (for logging)\n",
    "        \"\"\"\n",
    "        # Get source-target pair\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "        \n",
    "        # Tokenize (convert text ‚Üí token IDs)\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "        \n",
    "        # Calculate padding needed\n",
    "        # Encoder: [SOS] + tokens + [EOS] + padding = seq_len\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
    "        \n",
    "        # Decoder input: [SOS] + tokens + padding = seq_len (no [EOS]!)\n",
    "        # Label: tokens + [EOS] + padding = seq_len (no [SOS]!)\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "        \n",
    "        # Check if sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(f\"Sentence too long! src: {len(enc_input_tokens)}, tgt: {len(dec_input_tokens)}, max: {self.seq_len}\")\n",
    "        \n",
    "        # Build encoder input: [SOS] + tokens + [EOS] + [PAD]...\n",
    "        encoder_input = torch.cat([\n",
    "            self.sos_token,  # Start token\n",
    "            torch.tensor(enc_input_tokens, dtype=torch.int64),  # Actual tokens\n",
    "            self.eos_token,  # End token\n",
    "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),  # Padding\n",
    "        ], dim=0)\n",
    "        \n",
    "        # Build decoder input: [SOS] + tokens + [PAD]...\n",
    "        # (No [EOS] - model learns to predict it!)\n",
    "        decoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "        ], dim=0)\n",
    "        \n",
    "        # Build label: tokens + [EOS] + [PAD]...\n",
    "        # (No [SOS] - shifted by 1 position)\n",
    "        label = torch.cat([\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "        ], dim=0)\n",
    "        \n",
    "        # Verify all tensors have correct length\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "        \n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),  # (1, seq_len, seq_len)\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "print(\"‚úì BilingualDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d13ad9dd-5f12-4705-8f0d-c85b8c39876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 7: CREATE DATASET INSTANCE\n",
      "================================================================================\n",
      "‚úì Dataset created\n",
      "  Dataset size: 5\n",
      "  Sequence length: 20\n",
      "input data [{'translation': {'en': 'I love cats', 'fr': \"J'aime les chats\"}}, {'translation': {'en': 'I love dogs', 'fr': \"J'aime les chiens\"}}, {'translation': {'en': 'Hello world', 'fr': 'Bonjour le monde'}}, {'translation': {'en': 'Machine learning is amazing', 'fr': \"L'apprentissage automatique est incroyable\"}}, {'translation': {'en': 'Transformers are powerful', 'fr': 'Les transformateurs sont puissants'}}] \n",
      "\n",
      "input tokenizer english and french <tokenizers.Tokenizer object at 0xa53d19400> <tokenizers.Tokenizer object at 0xa53d18a00> \n",
      "\n",
      "input seq length 20\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 7: CREATE DATASET INSTANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 7: CREATE DATASET INSTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "SEQ_LEN = 20  # Maximum sequence length\n",
    "\n",
    "dataset = BilingualDataset(\n",
    "    ds=sample_data,\n",
    "    tokenizer_src=tokenizer_en,\n",
    "    tokenizer_tgt=tokenizer_fr,\n",
    "    src_lang=\"en\",\n",
    "    tgt_lang=\"fr\",\n",
    "    seq_len=SEQ_LEN\n",
    ")\n",
    "\n",
    "print(f\"‚úì Dataset created\")\n",
    "print(f\"  Dataset size: {len(dataset)}\")\n",
    "print(f\"  Sequence length: {SEQ_LEN}\")\n",
    "\n",
    "print('input data',sample_data, '\\n')\n",
    "print('input tokenizer english and french',tokenizer_en,tokenizer_fr,'\\n')\n",
    "print('input seq length', SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "068f5671-0c35-4f9f-8fc6-613c385eb155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 8: EXAMINE SAMPLE #0 IN DETAIL\n",
      "================================================================================\n",
      "\n",
      "Original texts:\n",
      "  Source (EN): 'I love cats'\n",
      "  Target (FR): 'J'aime les chats'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ENCODER INPUT (source sentence)\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: torch.Size([20])\n",
      "Tensor: tensor([ 2,  4,  5, 11,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1])\n",
      "DECODER INPUT (target sentence)\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: torch.Size([20])\n",
      "Tensor: tensor([ 2,  5,  4,  6,  7, 13,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 8: EXAMINE ONE SAMPLE (DETAILED)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 8: EXAMINE SAMPLE #0 IN DETAIL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample = dataset[0]\n",
    "\n",
    "print(f\"\\nOriginal texts:\")\n",
    "print(f\"  Source (EN): '{sample['src_text']}'\")\n",
    "print(f\"  Target (FR): '{sample['tgt_text']}'\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"ENCODER INPUT (source sentence)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['encoder_input'].shape}\")\n",
    "print(f\"Tensor: {sample['encoder_input']}\")\n",
    "\n",
    "\n",
    "print(\"DECODER INPUT (target sentence)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['decoder_input'].shape}\")\n",
    "print(f\"Tensor: {sample['decoder_input']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1320962-055b-4ae9-ad82-086f352290f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token breakdown:\n",
      "  Position  0: ID   2 = '[SOS]' ‚Üê Start token\n",
      "  Position  1: ID   4 = 'I'\n",
      "  Position  2: ID   5 = 'love'\n",
      "  Position  3: ID  11 = 'cats'\n",
      "  Position  4: ID   3 = '[EOS]' ‚Üê End token\n",
      "  Position  5: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position  6: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position  7: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position  8: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position  9: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 10: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 11: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 12: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 13: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 14: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 15: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 16: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 17: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 18: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 19: ID   1 = '[PAD]' ‚Üê Padding\n"
     ]
    }
   ],
   "source": [
    "# Decode to show tokens\n",
    "encoder_tokens = []\n",
    "for token_id in sample['encoder_input']:\n",
    "    token = tokenizer_en.id_to_token(int(token_id))\n",
    "    encoder_tokens.append(token)\n",
    "\n",
    "print(f\"\\nToken breakdown:\")\n",
    "for i, (token_id, token) in enumerate(zip(sample['encoder_input'], encoder_tokens)):\n",
    "    marker = \"\"\n",
    "    if token == \"[SOS]\":\n",
    "        marker = \" ‚Üê Start token\"\n",
    "    elif token == \"[EOS]\":\n",
    "        marker = \" ‚Üê End token\"\n",
    "    elif token == \"[PAD]\":\n",
    "        marker = \" ‚Üê Padding\"\n",
    "    print(f\"  Position {i:2d}: ID {token_id:3d} = '{token}'{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2da0dc28-b73d-4fc6-b0e2-3bbcf5421b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DECODER INPUT (target sentence for teacher forcing)\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: torch.Size([20])\n",
      "Tensor: tensor([ 2,  5,  4,  6,  7, 13,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1])\n",
      "\n",
      "Token breakdown:\n",
      "  Position  0: ID   2 = '[SOS]' ‚Üê Start token\n",
      "  Position  1: ID   5 = 'J'\n",
      "  Position  2: ID   4 = '''\n",
      "  Position  3: ID   6 = 'aime'\n",
      "  Position  4: ID   7 = 'les'\n",
      "  Position  5: ID  13 = 'chats'\n",
      "  Position  6: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position  7: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position  8: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position  9: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 10: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 11: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 12: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 13: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 14: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 15: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 16: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 17: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 18: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 19: ID   1 = '[PAD]' ‚Üê Padding\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"DECODER INPUT (target sentence for teacher forcing)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['decoder_input'].shape}\")\n",
    "print(f\"Tensor: {sample['decoder_input']}\")\n",
    "\n",
    "decoder_tokens = []\n",
    "for token_id in sample['decoder_input']:\n",
    "    token = tokenizer_fr.id_to_token(int(token_id))\n",
    "    decoder_tokens.append(token)\n",
    "\n",
    "print(f\"\\nToken breakdown:\")\n",
    "for i, (token_id, token) in enumerate(zip(sample['decoder_input'], decoder_tokens)):\n",
    "    marker = \"\"\n",
    "    if token == \"[SOS]\":\n",
    "        marker = \" ‚Üê Start token\"\n",
    "    elif token == \"[PAD]\":\n",
    "        marker = \" ‚Üê Padding\"\n",
    "    print(f\"  Position {i:2d}: ID {token_id:3d} = '{token}'{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "486d5a00-2041-47a7-b8b2-5bafdbcc224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "LABEL (what decoder should predict)\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: torch.Size([20])\n",
      "Tensor: tensor([ 5,  4,  6,  7, 13,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1])\n",
      "\n",
      "Token breakdown:\n",
      "  Position  0: ID   5 = 'J'\n",
      "  Position  1: ID   4 = '''\n",
      "  Position  2: ID   6 = 'aime'\n",
      "  Position  3: ID   7 = 'les'\n",
      "  Position  4: ID  13 = 'chats'\n",
      "  Position  5: ID   3 = '[EOS]' ‚Üê End token\n",
      "  Position  6: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position  7: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position  8: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position  9: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 10: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 11: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 12: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 13: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 14: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 15: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 16: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 17: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 18: ID   1 = '[PAD]' ‚Üê Padding\n",
      "  Position 19: ID   1 = '[PAD]' ‚Üê Padding\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"LABEL (what decoder should predict)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['label'].shape}\")\n",
    "print(f\"Tensor: {sample['label']}\")\n",
    "\n",
    "label_tokens = []\n",
    "for token_id in sample['label']:\n",
    "    token = tokenizer_fr.id_to_token(int(token_id))\n",
    "    label_tokens.append(token)\n",
    "\n",
    "print(f\"\\nToken breakdown:\")\n",
    "for i, (token_id, token) in enumerate(zip(sample['label'], label_tokens)):\n",
    "    marker = \"\"\n",
    "    if token == \"[EOS]\":\n",
    "        marker = \" ‚Üê End token\"\n",
    "    elif token == \"[PAD]\":\n",
    "        marker = \" ‚Üê Padding\"\n",
    "    print(f\"  Position {i:2d}: ID {token_id:3d} = '{token}'{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbd114ab-efaf-433e-b2b0-3c2217b17a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DECODER INPUT vs LABEL (Notice the shift!)\n",
      "--------------------------------------------------------------------------------\n",
      "Position | Decoder Input  | Label         | Explanation\n",
      "---------|----------------|---------------|---------------------------\n",
      "       0 | [SOS]          | J             | Decoder starts with [SOS], predicts first real token\n",
      "       1 | J              | '             | Decoder sees 'J', predicts next '''\n",
      "       2 | '              | aime          | Decoder sees ''', predicts next 'aime'\n",
      "       3 | aime           | les           | Decoder sees 'aime', predicts next 'les'\n",
      "       4 | les            | chats         | Decoder sees 'les', predicts next 'chats'\n",
      "       5 | chats          | [EOS]         | Decoder sees last token, predicts [EOS]\n",
      "       6 | [PAD]          | [PAD]         | Both padding (ignored in loss)\n",
      "       7 | [PAD]          | [PAD]         | Both padding (ignored in loss)\n",
      "\n",
      "‚úÖ This shifting is KEY for autoregressive training!\n",
      "   Model learns: given tokens 0...i, predict token i+1\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"DECODER INPUT vs LABEL (Notice the shift!)\")\n",
    "print(\"-\"*80)\n",
    "print(\"Position | Decoder Input  | Label         | Explanation\")\n",
    "print(\"---------|----------------|---------------|---------------------------\")\n",
    "for i in range(min(8, SEQ_LEN)):\n",
    "    dec_token = decoder_tokens[i]\n",
    "    label_token = label_tokens[i]\n",
    "    \n",
    "    if i == 0:\n",
    "        explanation = \"Decoder starts with [SOS], predicts first real token\"\n",
    "    elif label_token == \"[EOS]\":\n",
    "        explanation = \"Decoder sees last token, predicts [EOS]\"\n",
    "    elif dec_token == \"[PAD]\":\n",
    "        explanation = \"Both padding (ignored in loss)\"\n",
    "    else:\n",
    "        explanation = f\"Decoder sees '{dec_token}', predicts next '{label_token}'\"\n",
    "    \n",
    "    print(f\"{i:8d} | {dec_token:14s} | {label_token:13s} | {explanation}\")\n",
    "\n",
    "print(\"\\n‚úÖ This shifting is KEY for autoregressive training!\")\n",
    "print(\"   Model learns: given tokens 0...i, predict token i+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "531bc56b-2a07-423a-94fb-99e2fc45d802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 9: MASKS EXPLAINED\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ENCODER MASK (Padding mask)\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: torch.Size([1, 1, 20])\n",
      "Purpose: Prevent attention to [PAD] tokens in source\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 9: MASKS EXPLAINED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 9: MASKS EXPLAINED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ENCODER MASK (Padding mask)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['encoder_mask'].shape}\")\n",
    "print(f\"Purpose: Prevent attention to [PAD] tokens in source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96b7609d-c09c-4f2b-9d52-88fee63c22d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mask values (first 20 positions):\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "\n",
      "1 = Real token (attend)\n",
      "0 = Padding (ignore)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMask values (first 20 positions):\")\n",
    "mask_values = sample['encoder_mask'].squeeze()[:20]\n",
    "print(mask_values)\n",
    "print(\"\\n1 = Real token (attend)\")\n",
    "print(\"0 = Padding (ignore)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2acd117-e4c1-4106-9952-8999a7936ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[SOS]',\n",
       " 'I',\n",
       " 'love',\n",
       " 'cats',\n",
       " '[EOS]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d38c8337-7d12-4f4b-bbbf-d2d25daa8102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualization:\n",
      "  Position  0: '[SOS]     ' ‚Üí Mask=1 ‚úì Attend\n",
      "  Position  1: 'I         ' ‚Üí Mask=1 ‚úì Attend\n",
      "  Position  2: 'love      ' ‚Üí Mask=1 ‚úì Attend\n",
      "  Position  3: 'cats      ' ‚Üí Mask=1 ‚úì Attend\n",
      "  Position  4: '[EOS]     ' ‚Üí Mask=1 ‚úì Attend\n",
      "  Position  5: '[PAD]     ' ‚Üí Mask=0 ‚úó Ignore\n",
      "  Position  6: '[PAD]     ' ‚Üí Mask=0 ‚úó Ignore\n",
      "  Position  7: '[PAD]     ' ‚Üí Mask=0 ‚úó Ignore\n",
      "  Position  8: '[PAD]     ' ‚Üí Mask=0 ‚úó Ignore\n",
      "  Position  9: '[PAD]     ' ‚Üí Mask=0 ‚úó Ignore\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVisualization:\")\n",
    "for i in range(min(10, SEQ_LEN)):\n",
    "    token = encoder_tokens[i]\n",
    "    mask_val = sample['encoder_mask'].squeeze()[i].item()\n",
    "    status = \"‚úì Attend\" if mask_val == 1 else \"‚úó Ignore\"\n",
    "    print(f\"  Position {i:2d}: '{token:10s}' ‚Üí Mask={mask_val} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12d5d19e-a778-4da4-a876-10cf38447a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DECODER MASK (Padding + Causal)\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: torch.Size([1, 20, 20])\n",
      "Purpose: Prevent attention to [PAD] AND future tokens\n",
      "\n",
      "Full decoder mask (first 8x8):\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]], dtype=torch.int32)\n",
      "\n",
      "Interpretation (rows=queries, cols=keys):\n",
      "  Each row shows what that position can attend to\n",
      "  1 = Can attend, 0 = Cannot attend\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DECODER MASK (Padding + Causal)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['decoder_mask'].shape}\")\n",
    "print(f\"Purpose: Prevent attention to [PAD] AND future tokens\")\n",
    "\n",
    "print(f\"\\nFull decoder mask (first 8x8):\")\n",
    "decoder_mask_subset = sample['decoder_mask'].squeeze()[:8, :8]\n",
    "print(decoder_mask_subset.int())\n",
    "\n",
    "print(\"\\nInterpretation (rows=queries, cols=keys):\")\n",
    "print(\"  Each row shows what that position can attend to\")\n",
    "print(\"  1 = Can attend, 0 = Cannot attend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a7d64b0-9e42-42c2-af16-2fb3e798631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 10: ITERATE THROUGH ALL SAMPLES\n",
      "================================================================================\n",
      "\n",
      "Sample 0:\n",
      "  Source: 'I love cats'\n",
      "  Target: 'J'aime les chats'\n",
      "  Encoder input shape: torch.Size([20])\n",
      "  Decoder input shape: torch.Size([20])\n",
      "  Label shape: torch.Size([20])\n",
      "\n",
      "Sample 1:\n",
      "  Source: 'I love dogs'\n",
      "  Target: 'J'aime les chiens'\n",
      "  Encoder input shape: torch.Size([20])\n",
      "  Decoder input shape: torch.Size([20])\n",
      "  Label shape: torch.Size([20])\n",
      "\n",
      "Sample 2:\n",
      "  Source: 'Hello world'\n",
      "  Target: 'Bonjour le monde'\n",
      "  Encoder input shape: torch.Size([20])\n",
      "  Decoder input shape: torch.Size([20])\n",
      "  Label shape: torch.Size([20])\n",
      "\n",
      "Sample 3:\n",
      "  Source: 'Machine learning is amazing'\n",
      "  Target: 'L'apprentissage automatique est incroyable'\n",
      "  Encoder input shape: torch.Size([20])\n",
      "  Decoder input shape: torch.Size([20])\n",
      "  Label shape: torch.Size([20])\n",
      "\n",
      "Sample 4:\n",
      "  Source: 'Transformers are powerful'\n",
      "  Target: 'Les transformateurs sont puissants'\n",
      "  Encoder input shape: torch.Size([20])\n",
      "  Decoder input shape: torch.Size([20])\n",
      "  Label shape: torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 10: ITERATE THROUGH DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 10: ITERATE THROUGH ALL SAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    sample = dataset[idx]\n",
    "    print(f\"\\nSample {idx}:\")\n",
    "    print(f\"  Source: '{sample['src_text']}'\")\n",
    "    print(f\"  Target: '{sample['tgt_text']}'\")\n",
    "    print(f\"  Encoder input shape: {sample['encoder_input'].shape}\")\n",
    "    print(f\"  Decoder input shape: {sample['decoder_input'].shape}\")\n",
    "    print(f\"  Label shape: {sample['label'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4c6b654-a76f-449a-83bf-4951862d6073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY: WHAT HAPPENS IN BilingualDataset\n",
      "================================================================================\n",
      "\n",
      "1. INPUT:\n",
      "   - Source text: \"I love cats\"\n",
      "   - Target text: \"J'aime les chats\"\n",
      "\n",
      "2. TOKENIZATION:\n",
      "   - Source tokens: [I, love, cats]\n",
      "   - Target tokens: [J'aime, les, chats]\n",
      "\n",
      "3. ADD SPECIAL TOKENS + PADDING:\n",
      "   \n",
      "   Encoder Input:   [SOS] I love cats [EOS] [PAD] [PAD] ...\n",
      "                     ‚Üë                  ‚Üë     ‚Üë\n",
      "                   Start              End   Padding\n",
      "   \n",
      "   Decoder Input:   [SOS] J'aime les chats [PAD] [PAD] ...\n",
      "                     ‚Üë                      ‚Üë\n",
      "                   Start              No [EOS]!\n",
      "   \n",
      "   Label:           J'aime les chats [EOS] [PAD] [PAD] ...\n",
      "                    ‚Üë                 ‚Üë\n",
      "                  No [SOS]!         End\n",
      "\n",
      "4. CREATE MASKS:\n",
      "   - Encoder mask: Hide padding\n",
      "   - Decoder mask: Hide padding + future tokens (causal)\n",
      "\n",
      "5. OUTPUT:\n",
      "   Dict with encoder_input, decoder_input, label, masks, original texts\n",
      "\n",
      "WHY THIS STRUCTURE?\n",
      "-------------------\n",
      "‚úì Encoder input: Full source sentence with boundaries\n",
      "‚úì Decoder input: Starts with [SOS], model predicts next tokens\n",
      "‚úì Label: Shifted by 1, includes [EOS] as final prediction\n",
      "‚úì This enables teacher forcing during training!\n",
      "\n",
      "Teacher Forcing:\n",
      "  At position i, decoder sees correct tokens 0..i-1\n",
      "  Predicts token i (from label)\n",
      "  Even if previous predictions were wrong, uses ground truth\n",
      "\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TUTORIAL COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "You now understand:\n",
      "  ‚úì PyTorch Dataset class\n",
      "  ‚úì Tokenization for translation\n",
      "  ‚úì Special tokens ([SOS], [EOS], [PAD])\n",
      "  ‚úì Padding and sequence alignment\n",
      "  ‚úì Teacher forcing setup (decoder_input vs label)\n",
      "  ‚úì Encoder mask (padding only)\n",
      "  ‚úì Decoder mask (padding + causal)\n",
      "  ‚úì Why causal mask prevents looking ahead\n",
      "\n",
      "Ready for training! üöÄ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 11: SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: WHAT HAPPENS IN BilingualDataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. INPUT:\n",
    "   - Source text: \"I love cats\"\n",
    "   - Target text: \"J'aime les chats\"\n",
    "\n",
    "2. TOKENIZATION:\n",
    "   - Source tokens: [I, love, cats]\n",
    "   - Target tokens: [J'aime, les, chats]\n",
    "\n",
    "3. ADD SPECIAL TOKENS + PADDING:\n",
    "   \n",
    "   Encoder Input:   [SOS] I love cats [EOS] [PAD] [PAD] ...\n",
    "                     ‚Üë                  ‚Üë     ‚Üë\n",
    "                   Start              End   Padding\n",
    "   \n",
    "   Decoder Input:   [SOS] J'aime les chats [PAD] [PAD] ...\n",
    "                     ‚Üë                      ‚Üë\n",
    "                   Start              No [EOS]!\n",
    "   \n",
    "   Label:           J'aime les chats [EOS] [PAD] [PAD] ...\n",
    "                    ‚Üë                 ‚Üë\n",
    "                  No [SOS]!         End\n",
    "\n",
    "4. CREATE MASKS:\n",
    "   - Encoder mask: Hide padding\n",
    "   - Decoder mask: Hide padding + future tokens (causal)\n",
    "\n",
    "5. OUTPUT:\n",
    "   Dict with encoder_input, decoder_input, label, masks, original texts\n",
    "\n",
    "WHY THIS STRUCTURE?\n",
    "-------------------\n",
    "‚úì Encoder input: Full source sentence with boundaries\n",
    "‚úì Decoder input: Starts with [SOS], model predicts next tokens\n",
    "‚úì Label: Shifted by 1, includes [EOS] as final prediction\n",
    "‚úì This enables teacher forcing during training!\n",
    "\n",
    "Teacher Forcing:\n",
    "  At position i, decoder sees correct tokens 0..i-1\n",
    "  Predicts token i (from label)\n",
    "  Even if previous predictions were wrong, uses ground truth\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TUTORIAL COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYou now understand:\")\n",
    "print(\"  ‚úì PyTorch Dataset class\")\n",
    "print(\"  ‚úì Tokenization for translation\")\n",
    "print(\"  ‚úì Special tokens ([SOS], [EOS], [PAD])\")\n",
    "print(\"  ‚úì Padding and sequence alignment\")\n",
    "print(\"  ‚úì Teacher forcing setup (decoder_input vs label)\")\n",
    "print(\"  ‚úì Encoder mask (padding only)\")\n",
    "print(\"  ‚úì Decoder mask (padding + causal)\")\n",
    "print(\"  ‚úì Why causal mask prevents looking ahead\")\n",
    "print(\"\\nReady for training! üöÄ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e3deb-a41b-4ede-a626-35ed96342b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d6be00-3340-424e-99b3-850a6740fb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 1: UNDERSTANDING PyTorch Dataset Class\n",
      "================================================================================\n",
      "\n",
      "PyTorch Dataset requires 3 methods:\n",
      "  1. __init__()  ‚Üí Initialize\n",
      "  2. __len__()   ‚Üí Return dataset size\n",
      "  3. __getitem__(idx) ‚Üí Return sample at index idx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BILINGUAL DATASET FOR TRANSFORMER \n",
    "======================================================\n",
    "This tutorial explains how to prepare data for machine translation training.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 1: UNDERSTANDING PyTorch Dataset Class\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\"\"\"\n",
    "PyTorch Dataset Base Class\n",
    "---------------------------\n",
    "All custom datasets inherit from torch.utils.data.Dataset and must implement:\n",
    "\n",
    "1. __init__(self, ...)\n",
    "   - Initialize dataset\n",
    "   - Store data, tokenizers, parameters\n",
    "\n",
    "2. __len__(self)\n",
    "   - Return number of samples in dataset\n",
    "   - Used by DataLoader to know dataset size\n",
    "\n",
    "3. __getitem__(self, idx)\n",
    "   - Return one sample at index idx\n",
    "   - Called by DataLoader during training\n",
    "   - Must return: input data + labels\n",
    "\n",
    "Reference: https://pytorch.org/docs/stable/data.html\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "PyTorch Dataset requires 3 methods:\n",
    "  1. __init__()  ‚Üí Initialize\n",
    "  2. __len__()   ‚Üí Return dataset size\n",
    "  3. __getitem__(idx) ‚Üí Return sample at index idx\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51cbf12c-03ab-4e7a-8cf8-b636bdccbaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 2: CREATE SAMPLE TRANSLATION DATASET\n",
      "================================================================================\n",
      "Sample dataset:\n",
      "  0. EN: 'I love cats'\n",
      "     FR: 'J'aime les chats'\n",
      "  1. EN: 'I love dogs'\n",
      "     FR: 'J'aime les chiens'\n",
      "  2. EN: 'Hello world'\n",
      "     FR: 'Bonjour le monde'\n",
      "  3. EN: 'Machine learning is amazing'\n",
      "     FR: 'L'apprentissage automatique est incroyable'\n",
      "  4. EN: 'Transformers are powerful'\n",
      "     FR: 'Les transformateurs sont puissants'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 2: CREATE SAMPLE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: CREATE SAMPLE TRANSLATION DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sample English-French translation pairs\n",
    "sample_data = [\n",
    "    {\"translation\": {\"en\": \"I love cats\", \"fr\": \"J'aime les chats\"}},\n",
    "    {\"translation\": {\"en\": \"I love dogs\", \"fr\": \"J'aime les chiens\"}},\n",
    "    {\"translation\": {\"en\": \"Hello world\", \"fr\": \"Bonjour le monde\"}},\n",
    "    {\"translation\": {\"en\": \"Machine learning is amazing\", \"fr\": \"L'apprentissage automatique est incroyable\"}},\n",
    "    {\"translation\": {\"en\": \"Transformers are powerful\", \"fr\": \"Les transformateurs sont puissants\"}},\n",
    "]\n",
    "\n",
    "print(\"Sample dataset:\")\n",
    "for i, pair in enumerate(sample_data):\n",
    "    en = pair['translation']['en']\n",
    "    fr = pair['translation']['fr']\n",
    "    print(f\"  {i}. EN: '{en}'\")\n",
    "    print(f\"     FR: '{fr}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbf37a6-4509-4565-b605-fc7d2e354226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 3: CREATE TOKENIZERS\n",
      "================================================================================\n",
      "\n",
      "Creating English tokenizer...\n",
      "‚úì English tokenizer trained\n",
      "  Vocabulary size: 17\n",
      "\n",
      "Creating French tokenizer...\n",
      "‚úì French tokenizer trained\n",
      "  Vocabulary size: 22\n",
      "\n",
      "English vocabulary:\n",
      "   0: [UNK]\n",
      "   1: [PAD]\n",
      "   2: [SOS]\n",
      "   3: [EOS]\n",
      "   4: I\n",
      "   5: love\n",
      "   6: Hello\n",
      "   7: Machine\n",
      "   8: Transformers\n",
      "   9: amazing\n",
      "  10: are\n",
      "  11: cats\n",
      "  12: dogs\n",
      "  13: is\n",
      "  14: learning\n",
      "\n",
      "French vocabulary:\n",
      "   0: [UNK]\n",
      "   1: [PAD]\n",
      "   2: [SOS]\n",
      "   3: [EOS]\n",
      "   4: '\n",
      "   5: J\n",
      "   6: aime\n",
      "   7: les\n",
      "   8: Bonjour\n",
      "   9: L\n",
      "  10: Les\n",
      "  11: apprentissage\n",
      "  12: automatique\n",
      "  13: chats\n",
      "  14: chiens\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3: CREATE TOKENIZERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: CREATE TOKENIZERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    \"\"\"Extract all sentences for a language\"\"\"\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang] ## return one sentence at a time\n",
    "\n",
    "# Create English tokenizer\n",
    "print(\"\\nCreating English tokenizer...\")\n",
    "tokenizer_en = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "trainer_en = WordLevelTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "    min_frequency=1  # Keep all words (small dataset)\n",
    ")\n",
    "tokenizer_en.train_from_iterator(get_all_sentences(sample_data, \"en\"), trainer=trainer_en)\n",
    "\n",
    "print(\"‚úì English tokenizer trained\")\n",
    "print(f\"  Vocabulary size: {tokenizer_en.get_vocab_size()}\")\n",
    "\n",
    "# Create French tokenizer\n",
    "print(\"\\nCreating French tokenizer...\")\n",
    "tokenizer_fr = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer_fr.pre_tokenizer = Whitespace()\n",
    "trainer_fr = WordLevelTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "    min_frequency=1\n",
    ")\n",
    "tokenizer_fr.train_from_iterator(get_all_sentences(sample_data, \"fr\"), trainer=trainer_fr)\n",
    "\n",
    "print(\"‚úì French tokenizer trained\")\n",
    "print(f\"  Vocabulary size: {tokenizer_fr.get_vocab_size()}\")\n",
    "\n",
    "# Show vocabularies\n",
    "print(\"\\nEnglish vocabulary:\")\n",
    "en_vocab = tokenizer_en.get_vocab()\n",
    "for token, idx in sorted(en_vocab.items(), key=lambda x: x[1])[:15]:\n",
    "    print(f\"  {idx:2d}: {token}\")\n",
    "\n",
    "print(\"\\nFrench vocabulary:\")\n",
    "fr_vocab = tokenizer_fr.get_vocab()\n",
    "for token, idx in sorted(fr_vocab.items(), key=lambda x: x[1])[:15]:\n",
    "    print(f\"  {idx:2d}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "258cc8aa-6ad6-4567-a8b3-8c18c3ff391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 4: SPECIAL TOKENS EXPLAINED\n",
      "================================================================================\n",
      "\n",
      "Special Tokens in Machine Translation:\n",
      "---------------------------------------\n",
      "\n",
      "[UNK] (Unknown)   - ID: 0\n",
      "  - Replaces unknown/rare words not in vocabulary\n",
      "  - Example: \"xylophone\" ‚Üí [UNK] if not trained on it\n",
      "\n",
      "[PAD] (Padding)   - ID: 1\n",
      "  - Fills shorter sentences to match seq_len\n",
      "  - Masked out in attention (ignored)\n",
      "  - Example: \"Hello\" + [PAD][PAD][PAD] ‚Üí length 4\n",
      "\n",
      "[SOS] (Start)     - ID: 2\n",
      "  - Marks beginning of sequence\n",
      "  - Tells model \"sentence starts here\"\n",
      "  - Added to encoder input & decoder input\n",
      "\n",
      "[EOS] (End)       - ID: 3\n",
      "  - Marks end of sequence\n",
      "  - Tells model \"sentence ends here\"\n",
      "  - Added to encoder input & label (not decoder input!)\n",
      "\n",
      "Token IDs:\n",
      "  [SOS]: 2\n",
      "  [EOS]: 3\n",
      "  [PAD]: 1\n",
      "  [UNK]: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 4: UNDERSTANDING SPECIAL TOKENS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4: SPECIAL TOKENS EXPLAINED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Special Tokens in Machine Translation:\n",
    "---------------------------------------\n",
    "\n",
    "[UNK] (Unknown)   - ID: 0\n",
    "  - Replaces unknown/rare words not in vocabulary\n",
    "  - Example: \"xylophone\" ‚Üí [UNK] if not trained on it\n",
    "\n",
    "[PAD] (Padding)   - ID: 1\n",
    "  - Fills shorter sentences to match seq_len\n",
    "  - Masked out in attention (ignored)\n",
    "  - Example: \"Hello\" + [PAD][PAD][PAD] ‚Üí length 4\n",
    "\n",
    "[SOS] (Start)     - ID: 2\n",
    "  - Marks beginning of sequence\n",
    "  - Tells model \"sentence starts here\"\n",
    "  - Added to encoder input & decoder input\n",
    "\n",
    "[EOS] (End)       - ID: 3\n",
    "  - Marks end of sequence\n",
    "  - Tells model \"sentence ends here\"\n",
    "  - Added to encoder input & label (not decoder input!)\n",
    "\"\"\")\n",
    "\n",
    "sos_token_id = tokenizer_fr.token_to_id(\"[SOS]\")\n",
    "eos_token_id = tokenizer_fr.token_to_id(\"[EOS]\")\n",
    "pad_token_id = tokenizer_fr.token_to_id(\"[PAD]\")\n",
    "unk_token_id = tokenizer_fr.token_to_id(\"[UNK]\")\n",
    "\n",
    "print(f\"Token IDs:\")\n",
    "print(f\"  [SOS]: {sos_token_id}\")\n",
    "print(f\"  [EOS]: {eos_token_id}\")\n",
    "print(f\"  [PAD]: {pad_token_id}\")\n",
    "print(f\"  [UNK]: {unk_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba351591-3326-449f-85d7-125636f224af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 5: CAUSAL MASK (Prevents looking ahead)\n",
      "================================================================================\n",
      "\n",
      "Causal Mask Purpose:\n",
      "-------------------\n",
      "During training, decoder sees full target sentence but should only use\n",
      "past tokens to predict next token (autoregressive generation).\n",
      "\n",
      "Example: Predicting \"J'aime les chats\"\n",
      "  Position 0 (J'):     Can see: nothing (only [SOS])\n",
      "  Position 1 (aime):   Can see: J'\n",
      "  Position 2 (les):    Can see: J', aime\n",
      "  Position 3 (chats):  Can see: J', aime, les\n",
      "\n",
      "\n",
      "Causal mask for size=3:\n",
      "tensor([[1, 0, 0],\n",
      "        [1, 1, 0],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "  1 = Can attend\n",
      "  0 = Cannot attend (future tokens)\n",
      "\n",
      "Causal mask for size=5:\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "  1 = Can attend\n",
      "  0 = Cannot attend (future tokens)\n",
      "\n",
      "Visual explanation (size=4):\n",
      "\n",
      "       Token 0  Token 1  Token 2  Token 3\n",
      "Token 0    1       0        0        0     ‚Üê Can only see itself\n",
      "Token 1    1       1        0        0     ‚Üê Can see 0, 1\n",
      "Token 2    1       1        1        0     ‚Üê Can see 0, 1, 2\n",
      "Token 3    1       1        1        1     ‚Üê Can see all (0, 1, 2, 3)\n",
      "\n",
      "Lower triangular = can attend to past\n",
      "Upper triangular = BLOCKED (cannot see future)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 602, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/sheenachanda/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/6r/l83_f5fs7ws27nz_4txcl69r0000gp/T/ipykernel_17586/1566311503.py\", line 42, in <module>\n",
      "    mask = causal_mask(size)\n",
      "  File \"/var/folders/6r/l83_f5fs7ws27nz_4txcl69r0000gp/T/ipykernel_17586/1566311503.py\", line 23, in causal_mask\n",
      "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
      "/var/folders/6r/l83_f5fs7ws27nz_4txcl69r0000gp/T/ipykernel_17586/1566311503.py:23: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 5: CAUSAL MASK EXPLAINED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: CAUSAL MASK (Prevents looking ahead)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def causal_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a causal (upper triangular) mask for decoder self-attention.\n",
    "    Prevents positions from attending to future positions.\n",
    "    \n",
    "    Parameters:\n",
    "        size (int): Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Mask of shape (1, size, size)\n",
    "                     True = can attend, False = cannot attend\n",
    "    \"\"\"\n",
    "    # torch.triu creates upper triangular matrix\n",
    "    # diagonal=1 means start from 1st diagonal (exclude main diagonal)\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    # Invert: 0 becomes True (can attend), 1 becomes False (cannot)\n",
    "    return mask == 0\n",
    "\n",
    "print(\"\"\"\n",
    "Causal Mask Purpose:\n",
    "-------------------\n",
    "During training, decoder sees full target sentence but should only use\n",
    "past tokens to predict next token (autoregressive generation).\n",
    "\n",
    "Example: Predicting \"J'aime les chats\"\n",
    "  Position 0 (J'):     Can see: nothing (only [SOS])\n",
    "  Position 1 (aime):   Can see: J'\n",
    "  Position 2 (les):    Can see: J', aime\n",
    "  Position 3 (chats):  Can see: J', aime, les\n",
    "\"\"\")\n",
    "\n",
    "# Example causal masks\n",
    "for size in [3, 5]:\n",
    "    mask = causal_mask(size)\n",
    "    print(f\"\\nCausal mask for size={size}:\")\n",
    "    print(mask.squeeze(0).int())\n",
    "    print(\"  1 = Can attend\")\n",
    "    print(\"  0 = Cannot attend (future tokens)\")\n",
    "\n",
    "print(\"\\nVisual explanation (size=4):\")\n",
    "print(\"\"\"\n",
    "       Token 0  Token 1  Token 2  Token 3\n",
    "Token 0    1       0        0        0     ‚Üê Can only see itself\n",
    "Token 1    1       1        0        0     ‚Üê Can see 0, 1\n",
    "Token 2    1       1        1        0     ‚Üê Can see 0, 1, 2\n",
    "Token 3    1       1        1        1     ‚Üê Can see all (0, 1, 2, 3)\n",
    "\n",
    "Lower triangular = can attend to past\n",
    "Upper triangular = BLOCKED (cannot see future)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a75984-065c-4ce4-8925-29dab5b76513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 6: BILINGUAL DATASET CLASS (Line-by-line)\n",
      "================================================================================\n",
      "‚úì BilingualDataset class defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 6: BILINGUAL DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 6: BILINGUAL DATASET CLASS (Line-by-line)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for machine translation training.\n",
    "    Prepares encoder input, decoder input, and labels with proper padding and masks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Parameters:\n",
    "            ds: Raw dataset (list of dicts with 'translation' key)\n",
    "            tokenizer_src: Tokenizer for source language\n",
    "            tokenizer_tgt: Tokenizer for target language\n",
    "            src_lang: Source language code (e.g., \"en\")\n",
    "            tgt_lang: Target language code (e.g., \"fr\")\n",
    "            seq_len: Maximum sequence length (all tensors padded to this)\n",
    "        \"\"\"\n",
    "        super().__init__()  # Call parent Dataset.__init__\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Store parameters\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        \n",
    "        # Create special token tensors (pre-computed for efficiency)\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return dataset size\"\"\"\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get one training sample.\n",
    "        \n",
    "        Returns dict with:\n",
    "            - encoder_input: Source sentence with [SOS] + tokens + [EOS] + padding\n",
    "            - decoder_input: Target sentence with [SOS] + tokens + padding\n",
    "            - label: Target sentence with tokens + [EOS] + padding (shifted by 1)\n",
    "            - encoder_mask: Padding mask for encoder\n",
    "            - decoder_mask: Combined padding + causal mask for decoder\n",
    "            - src_text: Original source text (for logging)\n",
    "            - tgt_text: Original target text (for logging)\n",
    "        \"\"\"\n",
    "        # Get source-target pair\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "        \n",
    "        # Tokenize (convert text ‚Üí token IDs)\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "        \n",
    "        # Calculate padding needed\n",
    "        # Encoder: [SOS] + tokens + [EOS] + padding = seq_len\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
    "        \n",
    "        # Decoder input: [SOS] + tokens + padding = seq_len (no [EOS]!)\n",
    "        # Label: tokens + [EOS] + padding = seq_len (no [SOS]!)\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "        \n",
    "        # Check if sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(f\"Sentence too long! src: {len(enc_input_tokens)}, tgt: {len(dec_input_tokens)}, max: {self.seq_len}\")\n",
    "        \n",
    "        # Build encoder input: [SOS] + tokens + [EOS] + [PAD]...\n",
    "        encoder_input = torch.cat([\n",
    "            self.sos_token,  # Start token\n",
    "            torch.tensor(enc_input_tokens, dtype=torch.int64),  # Actual tokens\n",
    "            self.eos_token,  # End token\n",
    "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),  # Padding\n",
    "        ], dim=0)\n",
    "        \n",
    "        # Build decoder input: [SOS] + tokens + [PAD]...\n",
    "        # (No [EOS] - model learns to predict it!)\n",
    "        decoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "        ], dim=0)\n",
    "        \n",
    "        # Build label: tokens + [EOS] + [PAD]...\n",
    "        # (No [SOS] - shifted by 1 position)\n",
    "        label = torch.cat([\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "        ], dim=0)\n",
    "        \n",
    "        # Verify all tensors have correct length\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "        \n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),  # (1, seq_len, seq_len)\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "print(\"‚úì BilingualDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13ad9dd-5f12-4705-8f0d-c85b8c39876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 7: CREATE DATASET INSTANCE\n",
      "================================================================================\n",
      "‚úì Dataset created\n",
      "  Dataset size: 5\n",
      "  Sequence length: 20\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 7: CREATE DATASET INSTANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 7: CREATE DATASET INSTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "SEQ_LEN = 20  # Maximum sequence length\n",
    "\n",
    "dataset = BilingualDataset(\n",
    "    ds=sample_data,\n",
    "    tokenizer_src=tokenizer_en,\n",
    "    tokenizer_tgt=tokenizer_fr,\n",
    "    src_lang=\"en\",\n",
    "    tgt_lang=\"fr\",\n",
    "    seq_len=SEQ_LEN\n",
    ")\n",
    "\n",
    "print(f\"‚úì Dataset created\")\n",
    "print(f\"  Dataset size: {len(dataset)}\")\n",
    "print(f\"  Sequence length: {SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "068f5671-0c35-4f9f-8fc6-613c385eb155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 8: EXAMINE SAMPLE #0 IN DETAIL\n",
      "================================================================================\n",
      "\n",
      "Original texts:\n",
      "  Source (EN): 'I love cats'\n",
      "  Target (FR): 'J'aime les chats'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ENCODER INPUT (source sentence)\n",
      "--------------------------------------------------------------------------------\n",
      "Shape: torch.Size([20])\n",
      "Tensor: tensor([ 2,  4,  5, 11,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 8: EXAMINE ONE SAMPLE (DETAILED)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 8: EXAMINE SAMPLE #0 IN DETAIL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample = dataset[0]\n",
    "\n",
    "print(f\"\\nOriginal texts:\")\n",
    "print(f\"  Source (EN): '{sample['src_text']}'\")\n",
    "print(f\"  Target (FR): '{sample['tgt_text']}'\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"ENCODER INPUT (source sentence)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['encoder_input'].shape}\")\n",
    "print(f\"Tensor: {sample['encoder_input']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1320962-055b-4ae9-ad82-086f352290f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode to show tokens\n",
    "encoder_tokens = []\n",
    "for token_id in sample['encoder_input']:\n",
    "    token = tokenizer_en.id_to_token(int(token_id))\n",
    "    encoder_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0dc28-b73d-4fc6-b0e2-3bbcf5421b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d5a00-2041-47a7-b8b2-5bafdbcc224b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6b654-a76f-449a-83bf-4951862d6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"\\nToken breakdown:\")\n",
    "for i, (token_id, token) in enumerate(zip(sample['encoder_input'], encoder_tokens)):\n",
    "    marker = \"\"\n",
    "    if token == \"[SOS]\":\n",
    "        marker = \" ‚Üê Start token\"\n",
    "    elif token == \"[EOS]\":\n",
    "        marker = \" ‚Üê End token\"\n",
    "    elif token == \"[PAD]\":\n",
    "        marker = \" ‚Üê Padding\"\n",
    "    print(f\"  Position {i:2d}: ID {token_id:3d} = '{token}'{marker}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"DECODER INPUT (target sentence for teacher forcing)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['decoder_input'].shape}\")\n",
    "print(f\"Tensor: {sample['decoder_input']}\")\n",
    "\n",
    "decoder_tokens = []\n",
    "for token_id in sample['decoder_input']:\n",
    "    token = tokenizer_fr.id_to_token(int(token_id))\n",
    "    decoder_tokens.append(token)\n",
    "\n",
    "print(f\"\\nToken breakdown:\")\n",
    "for i, (token_id, token) in enumerate(zip(sample['decoder_input'], decoder_tokens)):\n",
    "    marker = \"\"\n",
    "    if token == \"[SOS]\":\n",
    "        marker = \" ‚Üê Start token\"\n",
    "    elif token == \"[PAD]\":\n",
    "        marker = \" ‚Üê Padding\"\n",
    "    print(f\"  Position {i:2d}: ID {token_id:3d} = '{token}'{marker}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"LABEL (what decoder should predict)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['label'].shape}\")\n",
    "print(f\"Tensor: {sample['label']}\")\n",
    "\n",
    "label_tokens = []\n",
    "for token_id in sample['label']:\n",
    "    token = tokenizer_fr.id_to_token(int(token_id))\n",
    "    label_tokens.append(token)\n",
    "\n",
    "print(f\"\\nToken breakdown:\")\n",
    "for i, (token_id, token) in enumerate(zip(sample['label'], label_tokens)):\n",
    "    marker = \"\"\n",
    "    if token == \"[EOS]\":\n",
    "        marker = \" ‚Üê End token\"\n",
    "    elif token == \"[PAD]\":\n",
    "        marker = \" ‚Üê Padding\"\n",
    "    print(f\"  Position {i:2d}: ID {token_id:3d} = '{token}'{marker}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"DECODER INPUT vs LABEL (Notice the shift!)\")\n",
    "print(\"-\"*80)\n",
    "print(\"Position | Decoder Input  | Label         | Explanation\")\n",
    "print(\"---------|----------------|---------------|---------------------------\")\n",
    "for i in range(min(8, SEQ_LEN)):\n",
    "    dec_token = decoder_tokens[i]\n",
    "    label_token = label_tokens[i]\n",
    "    \n",
    "    if i == 0:\n",
    "        explanation = \"Decoder starts with [SOS], predicts first real token\"\n",
    "    elif label_token == \"[EOS]\":\n",
    "        explanation = \"Decoder sees last token, predicts [EOS]\"\n",
    "    elif dec_token == \"[PAD]\":\n",
    "        explanation = \"Both padding (ignored in loss)\"\n",
    "    else:\n",
    "        explanation = f\"Decoder sees '{dec_token}', predicts next '{label_token}'\"\n",
    "    \n",
    "    print(f\"{i:8d} | {dec_token:14s} | {label_token:13s} | {explanation}\")\n",
    "\n",
    "print(\"\\n‚úÖ This shifting is KEY for autoregressive training!\")\n",
    "print(\"   Model learns: given tokens 0...i, predict token i+1\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 9: MASKS EXPLAINED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 9: MASKS EXPLAINED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ENCODER MASK (Padding mask)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['encoder_mask'].shape}\")\n",
    "print(f\"Purpose: Prevent attention to [PAD] tokens in source\")\n",
    "\n",
    "print(f\"\\nMask values (first 20 positions):\")\n",
    "mask_values = sample['encoder_mask'].squeeze()[:20]\n",
    "print(mask_values)\n",
    "print(\"\\n1 = Real token (attend)\")\n",
    "print(\"0 = Padding (ignore)\")\n",
    "\n",
    "print(\"\\nVisualization:\")\n",
    "for i in range(min(10, SEQ_LEN)):\n",
    "    token = encoder_tokens[i]\n",
    "    mask_val = sample['encoder_mask'].squeeze()[i].item()\n",
    "    status = \"‚úì Attend\" if mask_val == 1 else \"‚úó Ignore\"\n",
    "    print(f\"  Position {i:2d}: '{token:10s}' ‚Üí Mask={mask_val} {status}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DECODER MASK (Padding + Causal)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Shape: {sample['decoder_mask'].shape}\")\n",
    "print(f\"Purpose: Prevent attention to [PAD] AND future tokens\")\n",
    "\n",
    "print(f\"\\nFull decoder mask (first 8x8):\")\n",
    "decoder_mask_subset = sample['decoder_mask'].squeeze()[:8, :8]\n",
    "print(decoder_mask_subset.int())\n",
    "\n",
    "print(\"\\nInterpretation (rows=queries, cols=keys):\")\n",
    "print(\"  Each row shows what that position can attend to\")\n",
    "print(\"  1 = Can attend, 0 = Cannot attend\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 10: ITERATE THROUGH DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 10: ITERATE THROUGH ALL SAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    sample = dataset[idx]\n",
    "    print(f\"\\nSample {idx}:\")\n",
    "    print(f\"  Source: '{sample['src_text']}'\")\n",
    "    print(f\"  Target: '{sample['tgt_text']}'\")\n",
    "    print(f\"  Encoder input shape: {sample['encoder_input'].shape}\")\n",
    "    print(f\"  Decoder input shape: {sample['decoder_input'].shape}\")\n",
    "    print(f\"  Label shape: {sample['label'].shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 11: SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: WHAT HAPPENS IN BilingualDataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. INPUT:\n",
    "   - Source text: \"I love cats\"\n",
    "   - Target text: \"J'aime les chats\"\n",
    "\n",
    "2. TOKENIZATION:\n",
    "   - Source tokens: [I, love, cats]\n",
    "   - Target tokens: [J'aime, les, chats]\n",
    "\n",
    "3. ADD SPECIAL TOKENS + PADDING:\n",
    "   \n",
    "   Encoder Input:   [SOS] I love cats [EOS] [PAD] [PAD] ...\n",
    "                     ‚Üë                  ‚Üë     ‚Üë\n",
    "                   Start              End   Padding\n",
    "   \n",
    "   Decoder Input:   [SOS] J'aime les chats [PAD] [PAD] ...\n",
    "                     ‚Üë                      ‚Üë\n",
    "                   Start              No [EOS]!\n",
    "   \n",
    "   Label:           J'aime les chats [EOS] [PAD] [PAD] ...\n",
    "                    ‚Üë                 ‚Üë\n",
    "                  No [SOS]!         End\n",
    "\n",
    "4. CREATE MASKS:\n",
    "   - Encoder mask: Hide padding\n",
    "   - Decoder mask: Hide padding + future tokens (causal)\n",
    "\n",
    "5. OUTPUT:\n",
    "   Dict with encoder_input, decoder_input, label, masks, original texts\n",
    "\n",
    "WHY THIS STRUCTURE?\n",
    "-------------------\n",
    "‚úì Encoder input: Full source sentence with boundaries\n",
    "‚úì Decoder input: Starts with [SOS], model predicts next tokens\n",
    "‚úì Label: Shifted by 1, includes [EOS] as final prediction\n",
    "‚úì This enables teacher forcing during training!\n",
    "\n",
    "Teacher Forcing:\n",
    "  At position i, decoder sees correct tokens 0..i-1\n",
    "  Predicts token i (from label)\n",
    "  Even if previous predictions were wrong, uses ground truth\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TUTORIAL COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYou now understand:\")\n",
    "print(\"  ‚úì PyTorch Dataset class\")\n",
    "print(\"  ‚úì Tokenization for translation\")\n",
    "print(\"  ‚úì Special tokens ([SOS], [EOS], [PAD])\")\n",
    "print(\"  ‚úì Padding and sequence alignment\")\n",
    "print(\"  ‚úì Teacher forcing setup (decoder_input vs label)\")\n",
    "print(\"  ‚úì Encoder mask (padding only)\")\n",
    "print(\"  ‚úì Decoder mask (padding + causal)\")\n",
    "print(\"  ‚úì Why causal mask prevents looking ahead\")\n",
    "print(\"\\nReady for training! üöÄ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

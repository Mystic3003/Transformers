{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f644f69-cf59-48ce-80b1-500aaf18057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 1: Understanding Inheritance & Wrappers\n",
      "================================================================================\n",
      "\n",
      "INHERITANCE BASICS:\n",
      "-------------------\n",
      "\n",
      "class Parent:\n",
      "    def method(self):\n",
      "        return \"Parent method\"\n",
      "\n",
      "class Child(Parent):  # Child inherits from Parent\n",
      "    def method(self):  # Override parent method\n",
      "        return \"Child method\"\n",
      "\n",
      "WRAPPER CONCEPT:\n",
      "----------------\n",
      "- Extend existing class functionality\n",
      "- Keep original behavior\n",
      "- Add custom features\n",
      "- Maintain compatibility\n",
      "\n",
      "PyTorch Common Wrappers:\n",
      "------------------------\n",
      "1. Dataset â†’ Custom Dataset (your BilingualDataset)\n",
      "2. nn.Module â†’ Custom Models\n",
      "3. Optimizer â†’ Custom Optimizers\n",
      "4. Loss Functions â†’ Custom Losses\n",
      "5. Transforms â†’ Custom Transforms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PYTORCH WRAPPER CLASSES - COMPLETE TUTORIAL\n",
    "============================================\n",
    "Learn to write clean, reusable wrappers with examples\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 1: Understanding Inheritance & Wrappers\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "INHERITANCE BASICS:\n",
    "-------------------\n",
    "\n",
    "class Parent:\n",
    "    def method(self):\n",
    "        return \"Parent method\"\n",
    "\n",
    "class Child(Parent):  # Child inherits from Parent\n",
    "    def method(self):  # Override parent method\n",
    "        return \"Child method\"\n",
    "\n",
    "WRAPPER CONCEPT:\n",
    "----------------\n",
    "- Extend existing class functionality\n",
    "- Keep original behavior\n",
    "- Add custom features\n",
    "- Maintain compatibility\n",
    "\n",
    "PyTorch Common Wrappers:\n",
    "------------------------\n",
    "1. Dataset â†’ Custom Dataset (your BilingualDataset)\n",
    "2. nn.Module â†’ Custom Models\n",
    "3. Optimizer â†’ Custom Optimizers\n",
    "4. Loss Functions â†’ Custom Losses\n",
    "5. Transforms â†’ Custom Transforms\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb9f93d-1c70-484a-9593-5d3c662edfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE 1: Simple Dataset Wrapper\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 1: Simple Wrapper - Understanding the Pattern\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 1: Simple Dataset Wrapper\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Basic wrapper around Dataset class\n",
    "    \n",
    "    Pattern:\n",
    "        1. Inherit from base class\n",
    "        2. Initialize with super().__init__()\n",
    "        3. Implement required methods\n",
    "        4. Add custom functionality\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        Step 1: Initialize parent class\n",
    "        Step 2: Store your custom data\n",
    "        \"\"\"\n",
    "        super().__init__()  # Call parent __init__\n",
    "        \n",
    "        # Custom initialization\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        print(f\"âœ“ SimpleDataset initialized with {len(data)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Required method: Return dataset size\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Required method: Return one sample\n",
    "        Add your custom processing here\n",
    "        \"\"\"\n",
    "        # Your custom logic\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        # Convert to tensors (custom behavior)\n",
    "        return {\n",
    "            'data': torch.tensor(x, dtype=torch.float32),\n",
    "            'label': torch.tensor(y, dtype=torch.long),\n",
    "            'idx': idx  # Extra field (custom addition)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64d63802-bc43-44b8-a5a7-78a78eca896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SimpleDataset initialized with 3 samples\n",
      "Dataset length: 3\n",
      "Sample 0: {'data': tensor([1., 2., 3.]), 'label': tensor(0), 'idx': 0}\n",
      "Sample 1: {'data': tensor([4., 5., 6.]), 'label': tensor(1), 'idx': 1}\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "labels = [0, 1, 2]\n",
    "\n",
    "dataset = SimpleDataset(data, labels)\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "print(f\"Sample 0: {dataset[0]}\")\n",
    "print(f\"Sample 1: {dataset[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b0bcf7c-ec77-4d23-9fc2-4c60ebb7544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE 2: Dataset with Transform Wrapper\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 2: Dataset with Transforms (Adding Features)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 2: Dataset with Transform Wrapper\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class TransformDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper that adds transform functionality\n",
    "    \n",
    "    New Feature: Apply transformations to data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Add transform parameter - new feature!\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform  # NEW: Optional transform\n",
    "        print(f\"âœ“ TransformDataset initialized\")\n",
    "        print(f\"  - Samples: {len(data)}\")\n",
    "        print(f\"  - Transform: {transform is not None}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        # NEW: Apply transform if provided\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return {'data': x, 'label': y}\n",
    "\n",
    "# Custom transform function\n",
    "def normalize_transform(x):\n",
    "    \"\"\"Normalize to 0-1 range\"\"\"\n",
    "    return (x - x.min()) / (x.max() - x.min() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43b107ea-bbd1-49a1-a669-a9eff2487913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ TransformDataset initialized\n",
      "  - Samples: 3\n",
      "  - Transform: False\n",
      "âœ“ TransformDataset initialized\n",
      "  - Samples: 3\n",
      "  - Transform: True\n",
      "\n",
      "Without transform:\n",
      "  Sample 0: tensor([1., 2., 3.])\n",
      "\n",
      "With transform:\n",
      "  Sample 0: tensor([0.0000, 0.5000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Test with transform\n",
    "dataset_no_transform = TransformDataset(data, labels)\n",
    "dataset_with_transform = TransformDataset(data, labels, transform=normalize_transform)\n",
    "\n",
    "print(\"\\nWithout transform:\")\n",
    "print(f\"  Sample 0: {dataset_no_transform[0]['data']}\")\n",
    "\n",
    "print(\"\\nWith transform:\")\n",
    "print(f\"  Sample 0: {dataset_with_transform[0]['data']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19c0c10a-b996-4784-a004-fabf5d14bd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE 3: Caching Dataset Wrapper\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 3: Caching Dataset Wrapper (Performance Optimization)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 3: Caching Dataset Wrapper\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class CachedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper that caches loaded samples\n",
    "    \n",
    "    Feature: Loads data once, stores in memory\n",
    "    Use case: When data loading is expensive\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, labels, enable_cache=True):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.enable_cache = enable_cache\n",
    "        \n",
    "        # NEW: Cache storage\n",
    "        self.cache = {} if enable_cache else None\n",
    "        print(f\"âœ“ CachedDataset initialized\")\n",
    "        print(f\"  - Caching: {enable_cache}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Check cache first\n",
    "        if self.enable_cache and idx in self.cache:\n",
    "            print(f\"  âš¡ Cache hit for idx {idx}\")\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        # Load data (expensive operation simulated)\n",
    "        print(f\"  ðŸ’¾ Loading idx {idx} from disk\")\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        sample = {'data': x, 'label': y}\n",
    "        \n",
    "        # Store in cache\n",
    "        if self.enable_cache:\n",
    "            self.cache[idx] = sample\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f3fde02-607d-450e-95a8-42fd4f5949c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CachedDataset initialized\n",
      "  - Caching: True\n",
      "\n",
      "First access:\n",
      "  ðŸ’¾ Loading idx 0 from disk\n",
      "\n",
      "Second access (should be cached):\n",
      "  âš¡ Cache hit for idx 0\n"
     ]
    }
   ],
   "source": [
    "# Test caching\n",
    "dataset = CachedDataset(data, labels, enable_cache=True)\n",
    "\n",
    "print(\"\\nFirst access:\")\n",
    "_ = dataset[0]\n",
    "\n",
    "print(\"\\nSecond access (should be cached):\")\n",
    "_ = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce7e7b23-8020-48cd-bbb5-848df087cc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE 4: BilingualDataset ( Code Explained)\n",
      "================================================================================\n",
      "\n",
      "class BilingualDataset(Dataset):\n",
      "    '''\n",
      "    Wrapper for translation data\n",
      "    \n",
      "    What it wraps: torch.utils.data.Dataset\n",
      "    What it adds:\n",
      "        - Tokenization\n",
      "        - Padding to fixed length\n",
      "        - Mask creation\n",
      "        - Teacher forcing setup\n",
      "    '''\n",
      "    \n",
      "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
      "        # Step 1: Call parent init\n",
      "        super().__init__()\n",
      "        \n",
      "        # Step 2: Store configuration\n",
      "        self.ds = ds\n",
      "        self.tokenizer_src = tokenizer_src\n",
      "        self.tokenizer_tgt = tokenizer_tgt\n",
      "        self.src_lang = src_lang\n",
      "        self.tgt_lang = tgt_lang\n",
      "        self.seq_len = seq_len\n",
      "        \n",
      "        # Step 3: Pre-compute reusable values\n",
      "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")])\n",
      "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")])\n",
      "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")])\n",
      "    \n",
      "    def __len__(self):\n",
      "        # Return size of wrapped dataset\n",
      "        return len(self.ds)\n",
      "    \n",
      "    def __getitem__(self, idx):\n",
      "        # Step 1: Get raw data\n",
      "        pair = self.ds[idx]\n",
      "        src_text = pair['translation'][self.src_lang]\n",
      "        tgt_text = pair['translation'][self.tgt_lang]\n",
      "        \n",
      "        # Step 2: Apply custom processing\n",
      "        # - Tokenization\n",
      "        # - Padding\n",
      "        # - Mask creation\n",
      "        \n",
      "        # Step 3: Return processed sample\n",
      "        return {\n",
      "            \"encoder_input\": encoder_input,\n",
      "            \"decoder_input\": decoder_input,\n",
      "            \"label\": label,\n",
      "            # ... etc\n",
      "        }\n",
      "\n",
      "KEY WRAPPER FEATURES:\n",
      "---------------------\n",
      "âœ“ Wraps raw dataset (ds)\n",
      "âœ“ Adds tokenization logic\n",
      "âœ“ Adds padding logic\n",
      "âœ“ Adds mask creation\n",
      "âœ“ Returns training-ready tensors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 4: BilingualDataset - Our Real Example\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 4: BilingualDataset ( Code Explained)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "class BilingualDataset(Dataset):\n",
    "    '''\n",
    "    Wrapper for translation data\n",
    "    \n",
    "    What it wraps: torch.utils.data.Dataset\n",
    "    What it adds:\n",
    "        - Tokenization\n",
    "        - Padding to fixed length\n",
    "        - Mask creation\n",
    "        - Teacher forcing setup\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        # Step 1: Call parent init\n",
    "        super().__init__()\n",
    "        \n",
    "        # Step 2: Store configuration\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Step 3: Pre-compute reusable values\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")])\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")])\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")])\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return size of wrapped dataset\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Step 1: Get raw data\n",
    "        pair = self.ds[idx]\n",
    "        src_text = pair['translation'][self.src_lang]\n",
    "        tgt_text = pair['translation'][self.tgt_lang]\n",
    "        \n",
    "        # Step 2: Apply custom processing\n",
    "        # - Tokenization\n",
    "        # - Padding\n",
    "        # - Mask creation\n",
    "        \n",
    "        # Step 3: Return processed sample\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"label\": label,\n",
    "            # ... etc\n",
    "        }\n",
    "\n",
    "KEY WRAPPER FEATURES:\n",
    "---------------------\n",
    "âœ“ Wraps raw dataset (ds)\n",
    "âœ“ Adds tokenization logic\n",
    "âœ“ Adds padding logic\n",
    "âœ“ Adds mask creation\n",
    "âœ“ Returns training-ready tensors\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f44fb-731f-46a8-b85b-6f59990d1301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61a7e457-b96a-4c0c-b564-0229c0ee54b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE 5: Custom Model Wrapper\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 5: Model Wrapper (nn.Module)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 5: Custom Model Wrapper\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleMLPWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around nn.Module\n",
    "    \n",
    "    Pattern: Same as Dataset\n",
    "        1. Inherit from nn.Module\n",
    "        2. Call super().__init__()\n",
    "        3. Define layers in __init__\n",
    "        4. Implement forward()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize custom model\n",
    "        \"\"\"\n",
    "        super().__init__()  # REQUIRED: Call parent init\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        print(f\"âœ“ SimpleMLPWrapper initialized\")\n",
    "        print(f\"  - Input: {input_size}\")\n",
    "        print(f\"  - Hidden: {hidden_size}\")\n",
    "        print(f\"  - Output: {output_size}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define forward pass (REQUIRED)\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "121b466a-e0e7-42c2-835e-2532a8c4dfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SimpleMLPWrapper initialized\n",
      "  - Input: 10\n",
      "  - Hidden: 20\n",
      "  - Output: 2\n",
      "\n",
      "Input shape: torch.Size([5, 10])\n",
      "Output shape: torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Test model wrapper\n",
    "model = SimpleMLPWrapper(input_size=10, hidden_size=20, output_size=2)\n",
    "x = torch.randn(5, 10)  # Batch of 5 samples , each having 10 features \n",
    "output = model(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "827a2a48-7a3f-477a-8808-ab87041d0755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE 6: Advanced Model Wrapper\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 6: Advanced Model Wrapper with Custom Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 6: Advanced Model Wrapper\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom block that can be reused\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Residual connection\n",
    "        return x + self.relu(self.fc(x))\n",
    "\n",
    "class AdvancedModelWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced wrapper with multiple features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_blocks=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, 128)\n",
    "        \n",
    "        # Create multiple residual blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualBlock(128) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.output_proj = nn.Linear(128, output_dim)\n",
    "        \n",
    "        print(f\"âœ“ AdvancedModelWrapper initialized\")\n",
    "        print(f\"  - Residual blocks: {num_blocks}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Pass through residual blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.output_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "558a0d62-1f16-4d26-8b57-6b0a3109ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AdvancedModelWrapper initialized\n",
      "  - Residual blocks: 3\n",
      "\n",
      "Input shape: torch.Size([5, 10])\n",
      "Output shape: torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "model = AdvancedModelWrapper(input_dim=10, output_dim=2, num_blocks=3)\n",
    "x = torch.randn(5, 10)\n",
    "output = model(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34cef373-6856-4320-b205-45dffe479113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE 7: Custom Loss Function Wrapper\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 7: Loss Function Wrapper\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 7: Custom Loss Function Wrapper\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class WeightedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around standard loss with custom weighting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_loss, weight_factor=1.0):\n",
    "        super().__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.weight_factor = weight_factor\n",
    "        print(f\"âœ“ WeightedLoss initialized\")\n",
    "        print(f\"  - Base loss: {base_loss.__class__.__name__}\")\n",
    "        print(f\"  - Weight factor: {weight_factor}\")\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Calculate base loss\n",
    "        loss = self.base_loss(pred, target)\n",
    "        \n",
    "        # Apply custom weighting\n",
    "        weighted_loss = loss * self.weight_factor\n",
    "        \n",
    "        return weighted_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c47638c-1f47-46fe-a368-bcb174a326a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ WeightedLoss initialized\n",
      "  - Base loss: MSELoss\n",
      "  - Weight factor: 2.0\n",
      "\n",
      "Base loss: 2.1309\n",
      "Custom loss: 4.2618\n",
      "Ratio: 2.0x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test custom loss\n",
    "base_criterion = nn.MSELoss()\n",
    "custom_criterion = WeightedLoss(base_criterion, weight_factor=2.0)\n",
    "\n",
    "pred = torch.randn(5, 10)\n",
    "target = torch.randn(5, 10)\n",
    "\n",
    "base_loss = base_criterion(pred, target)\n",
    "custom_loss = custom_criterion(pred, target)\n",
    "\n",
    "print(f\"\\nBase loss: {base_loss.item():.4f}\")\n",
    "print(f\"Custom loss: {custom_loss.item():.4f}\")\n",
    "print(f\"Ratio: {custom_loss.item() / base_loss.item():.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66ed8fc4-a7b5-4b15-950b-810bdc8d427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE 8: Optimizer Wrapper (Advanced)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE 8: Optimizer Wrapper (Advanced)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 8: Optimizer Wrapper (Advanced)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class WarmupOptimizer:\n",
    "    \"\"\"\n",
    "    Wrapper around optimizer with learning rate warmup\n",
    "    \n",
    "    NOT inheriting from nn.Module (optimizers don't inherit from it)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Store initial learning rate\n",
    "        self.base_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"âœ“ WarmupOptimizer initialized\")\n",
    "        print(f\"  - Warmup steps: {warmup_steps}\")\n",
    "        print(f\"  - Base LR: {self.base_lr}\")\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Custom step with warmup\"\"\"\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Calculate warmup learning rate\n",
    "        if self.step_count < self.warmup_steps:\n",
    "            lr_scale = self.step_count / self.warmup_steps\n",
    "            lr = self.base_lr * lr_scale\n",
    "            \n",
    "            # Update optimizer learning rate\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            print(f\"  Step {self.step_count}: LR = {lr:.6f} (warmup)\")\n",
    "        else:\n",
    "            print(f\"  Step {self.step_count}: LR = {self.base_lr:.6f} (normal)\")\n",
    "        \n",
    "        # Call wrapped optimizer step\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Delegate to wrapped optimizer\"\"\"\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca002fee-5196-4e83-a64b-945f0187377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ WarmupOptimizer initialized\n",
      "  - Warmup steps: 5\n",
      "  - Base LR: 0.001\n",
      "\n",
      "Simulating training steps:\n",
      "  Step 1: LR = 0.000200 (warmup)\n",
      "  Step 2: LR = 0.000400 (warmup)\n",
      "  Step 3: LR = 0.000600 (warmup)\n",
      "  Step 4: LR = 0.000800 (warmup)\n",
      "  Step 5: LR = 0.001000 (normal)\n",
      "  Step 6: LR = 0.001000 (normal)\n",
      "  Step 7: LR = 0.001000 (normal)\n",
      "  Step 8: LR = 0.001000 (normal)\n"
     ]
    }
   ],
   "source": [
    "# Test warmup optimizer\n",
    "dummy_model = nn.Linear(10, 2)\n",
    "base_optimizer = torch.optim.Adam(dummy_model.parameters(), lr=0.001)\n",
    "wrapped_optimizer = WarmupOptimizer(base_optimizer, warmup_steps=5)\n",
    "\n",
    "print(\"\\nSimulating training steps:\")\n",
    "for i in range(8):\n",
    "    wrapped_optimizer.zero_grad()\n",
    "    # Simulate backward pass\n",
    "    wrapped_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "468e85af-e9bd-433f-b96a-d3d1802b5721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WRAPPER CLASS PATTERN SUMMARY\n",
      "================================================================================\n",
      "\n",
      "UNIVERSAL WRAPPER PATTERN:\n",
      "--------------------------\n",
      "\n",
      "class CustomWrapper(BaseClass):\n",
      "    '''\n",
      "    Step 0: Choose base class to wrap\n",
      "    - Dataset\n",
      "    - nn.Module\n",
      "    - Loss function\n",
      "    - etc.\n",
      "    '''\n",
      "    \n",
      "    def __init__(self, *args, **kwargs):\n",
      "        '''\n",
      "        Step 1: Always call super().__init__()\n",
      "        '''\n",
      "        super().__init__()\n",
      "        \n",
      "        '''\n",
      "        Step 2: Store configuration\n",
      "        '''\n",
      "        self.config = kwargs\n",
      "        \n",
      "        '''\n",
      "        Step 3: Initialize custom components\n",
      "        '''\n",
      "        self.custom_component = ...\n",
      "    \n",
      "    def required_method(self, *args):\n",
      "        '''\n",
      "        Step 4: Implement required methods\n",
      "        - Dataset: __len__, __getitem__\n",
      "        - nn.Module: forward\n",
      "        '''\n",
      "        # Your custom logic\n",
      "        pass\n",
      "    \n",
      "    def optional_custom_method(self):\n",
      "        '''\n",
      "        Step 5: Add optional custom methods\n",
      "        '''\n",
      "        pass\n",
      "\n",
      "CHECKLIST:\n",
      "----------\n",
      "âœ“ Inherit from base class\n",
      "âœ“ Call super().__init__()\n",
      "âœ“ Implement required methods\n",
      "âœ“ Add custom features\n",
      "âœ“ Document your additions\n",
      "âœ“ Test your wrapper\n",
      "\n",
      "COMMON BASES TO WRAP:\n",
      "---------------------\n",
      "1. torch.utils.data.Dataset\n",
      "   - Required: __init__, __len__, __getitem__\n",
      "\n",
      "2. torch.nn.Module\n",
      "   - Required: __init__, forward\n",
      "\n",
      "3. torch.optim.Optimizer\n",
      "   - Wrap existing optimizer\n",
      "   - Add custom step logic\n",
      "\n",
      "4. Functions\n",
      "   - Create wrapper functions\n",
      "   - Add logging, timing, validation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PATTERN SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WRAPPER CLASS PATTERN SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "UNIVERSAL WRAPPER PATTERN:\n",
    "--------------------------\n",
    "\n",
    "class CustomWrapper(BaseClass):\n",
    "    '''\n",
    "    Step 0: Choose base class to wrap\n",
    "    - Dataset\n",
    "    - nn.Module\n",
    "    - Loss function\n",
    "    - etc.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        '''\n",
    "        Step 1: Always call super().__init__()\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        Step 2: Store configuration\n",
    "        '''\n",
    "        self.config = kwargs\n",
    "        \n",
    "        '''\n",
    "        Step 3: Initialize custom components\n",
    "        '''\n",
    "        self.custom_component = ...\n",
    "    \n",
    "    def required_method(self, *args):\n",
    "        '''\n",
    "        Step 4: Implement required methods\n",
    "        - Dataset: __len__, __getitem__\n",
    "        - nn.Module: forward\n",
    "        '''\n",
    "        # Your custom logic\n",
    "        pass\n",
    "    \n",
    "    def optional_custom_method(self):\n",
    "        '''\n",
    "        Step 5: Add optional custom methods\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "CHECKLIST:\n",
    "----------\n",
    "âœ“ Inherit from base class\n",
    "âœ“ Call super().__init__()\n",
    "âœ“ Implement required methods\n",
    "âœ“ Add custom features\n",
    "âœ“ Document your additions\n",
    "âœ“ Test your wrapper\n",
    "\n",
    "COMMON BASES TO WRAP:\n",
    "---------------------\n",
    "1. torch.utils.data.Dataset\n",
    "   - Required: __init__, __len__, __getitem__\n",
    "\n",
    "2. torch.nn.Module\n",
    "   - Required: __init__, forward\n",
    "\n",
    "3. torch.optim.Optimizer\n",
    "   - Wrap existing optimizer\n",
    "   - Add custom step logic\n",
    "\n",
    "4. Functions\n",
    "   - Create wrapper functions\n",
    "   - Add logging, timing, validation\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94728996-1dc3-4d7d-b2cc-f852c36175ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRACTICAL: Complete Custom Dataset Template\n",
      "================================================================================\n",
      "âœ“ MyCustomDataset initialized\n",
      "  - Samples: 100\n",
      "  - Max length: 512\n",
      "  - Caching: True\n",
      "\n",
      "Dataset length: 100\n",
      "Sample 0: {'data': tensor([-1.0317,  1.2901,  1.1302, -0.6877, -0.0383, -0.4107, -1.0900,  0.2785,\n",
      "        -0.6659,  1.9071]), 'label': 0, 'idx': 0}\n",
      "\n",
      "================================================================================\n",
      "âœ… WRAPPER CLASSES TUTORIAL COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "KEY TAKEAWAYS:\n",
      "--------------\n",
      "1. Wrappers extend existing classes\n",
      "2. Always call super().__init__()\n",
      "3. Implement required methods\n",
      "4. Add custom features incrementally\n",
      "5. Keep code modular and reusable\n",
      "\n",
      "PRACTICE EXERCISE:\n",
      "------------------\n",
      "Write a wrapper for:\n",
      "- Dataset that applies random augmentation\n",
      "- Model that adds dropout layers\n",
      "- Loss that combines multiple losses\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PRACTICAL EXAMPLE: Complete Custom Dataset\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRACTICAL: Complete Custom Dataset Template\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Template for any custom dataset\n",
    "    Copy this and modify for your needs!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path,\n",
    "                 transform=None,\n",
    "                 max_length=512,\n",
    "                 cache=True):\n",
    "        \"\"\"\n",
    "        Initialize dataset\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to data\n",
    "            transform: Optional transform\n",
    "            max_length: Maximum sequence length\n",
    "            cache: Whether to cache samples\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store config\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        self.cache_enabled = cache\n",
    "        \n",
    "        # Load metadata (not full data!)\n",
    "        self.metadata = self._load_metadata()\n",
    "        \n",
    "        # Initialize cache\n",
    "        self.cache = {} if cache else None\n",
    "        \n",
    "        print(f\"âœ“ MyCustomDataset initialized\")\n",
    "        print(f\"  - Samples: {len(self.metadata)}\")\n",
    "        print(f\"  - Max length: {max_length}\")\n",
    "        print(f\"  - Caching: {cache}\")\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Helper: Load file paths or indices\"\"\"\n",
    "        # In real code: load file list, database entries, etc.\n",
    "        return list(range(100))  # Dummy metadata\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return dataset size\"\"\"\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Load and return sample\"\"\"\n",
    "        # Check cache\n",
    "        if self.cache_enabled and idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        # Load data\n",
    "        data = self._load_sample(idx)\n",
    "        \n",
    "        # Apply transform\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        # Cache if enabled\n",
    "        if self.cache_enabled:\n",
    "            self.cache[idx] = data\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _load_sample(self, idx):\n",
    "        \"\"\"Helper: Load one sample\"\"\"\n",
    "        # In real code: load from disk, database, etc.\n",
    "        return {\n",
    "            'data': torch.randn(10),\n",
    "            'label': idx % 2,\n",
    "            'idx': idx\n",
    "        }\n",
    "\n",
    "# Test template\n",
    "dataset = MyCustomDataset(\n",
    "    data_path=\"./data\",\n",
    "    transform=None,\n",
    "    max_length=512,\n",
    "    cache=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset length: {len(dataset)}\")\n",
    "print(f\"Sample 0: {dataset[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… WRAPPER CLASSES TUTORIAL COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "KEY TAKEAWAYS:\n",
    "--------------\n",
    "1. Wrappers extend existing classes\n",
    "2. Always call super().__init__()\n",
    "3. Implement required methods\n",
    "4. Add custom features incrementally\n",
    "5. Keep code modular and reusable\n",
    "\n",
    "PRACTICE EXERCISE:\n",
    "------------------\n",
    "Write a wrapper for:\n",
    "- Dataset that applies random augmentation\n",
    "- Model that adds dropout layers\n",
    "- Loss that combines multiple losses\n",
    "\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbda36a-8e6a-4a24-81c1-fddb8b57fe1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

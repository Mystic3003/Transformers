{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9fc20a-dd27-4a83-bccc-ede08802426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPLETE TRANSFORMER ATTENTION WALKTHROUGH\n",
      "======================================================================\n",
      "\n",
      "STEP 0: Input Sentences\n",
      "----------------------------------------------------------------------\n",
      "Token IDs:\n",
      "tensor([[1, 2, 3, 0],\n",
      "        [4, 5, 6, 0]])\n",
      "Shape: torch.Size([2, 4])  # (batch_size=2, seq_len=4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPLETE TRANSFORMER ATTENTION WALKTHROUGH\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# =============================================\n",
    "# SETUP: Two sentences with padding\n",
    "# =============================================\n",
    "print(\"STEP 0: Input Sentences\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "sentences = [\n",
    "    \"The cat sat\",     # 3 words\n",
    "    \"I am here\"        # 3 words (but we'll add padding to show masking)\n",
    "]\n",
    "\n",
    "# Token IDs (simplified vocabulary)\n",
    "# Vocab: {PAD:0, The:1, cat:2, sat:3, I:4, am:5, here:6}\n",
    "token_ids = [\n",
    "    [1, 2, 3, 0],      # \"The cat sat PAD\"\n",
    "    [4, 5, 6, 0]       # \"I am here PAD\"\n",
    "]\n",
    "\n",
    "batch = torch.tensor(token_ids)\n",
    "print(f\"Token IDs:\\n{batch}\")\n",
    "print(f\"Shape: {batch.shape}  # (batch_size=2 - num of sentences, seq_len=4-num of tokens)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfd2da5-2a24-42cf-ac62-e491f3a65006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Convert token IDs → embeddings\n",
      "----------------------------------------------------------------------\n",
      "Embeddings shape: torch.Size([2, 4, 8])  # (batch=2, seq_len=4, d_model=8)\n",
      "\n",
      "Sample embedding for 'The' (token 1):\n",
      "tensor([ 1.3679,  0.3618, -0.5893,  0.3309,  0.1397,  0.6632, -0.5643,  1.5214],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "tensor([[[ 1.3679,  0.3618, -0.5893,  0.3309,  0.1397,  0.6632, -0.5643,\n",
      "           1.5214],\n",
      "         [-1.9882, -0.4915,  0.9148,  0.8288, -0.8523, -0.0460,  0.4177,\n",
      "           1.3448],\n",
      "         [ 0.1492,  0.4637, -1.4802, -1.3158, -2.2878, -2.1838,  0.1744,\n",
      "          -0.8128],\n",
      "         [ 0.5497,  0.4373, -0.8928, -1.0195, -0.0478,  0.2787, -1.3684,\n",
      "           0.4537]],\n",
      "\n",
      "        [[ 0.1047,  1.2801, -1.2533,  0.3623,  1.3270,  0.0127, -1.4751,\n",
      "           1.0377],\n",
      "         [ 0.7780,  0.9732, -1.5413, -1.1941,  1.8389, -0.7806, -0.1243,\n",
      "          -0.6337],\n",
      "         [ 1.1894,  0.3994, -1.6332,  0.5182, -1.0384, -0.0923, -1.5324,\n",
      "           1.1770],\n",
      "         [ 0.5497,  0.4373, -0.8928, -1.0195, -0.0478,  0.2787, -1.3684,\n",
      "           0.4537]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# STEP 1: Embeddings\n",
    "# =============================================\n",
    "print(\"STEP 1: Convert token IDs → embeddings\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "vocab_size = 7\n",
    "d_model = 8  # Small for visualization (normally 512)\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "x = embedding(batch)\n",
    "print(f\"Embeddings shape: {x.shape}  # (batch=2, seq_len=4, d_model=8)\")\n",
    "print(f\"\\nSample embedding for 'The' (token 1):\")\n",
    "print(x[0, 0, :])  # First sentence, first token\n",
    "print()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15a0dd8-54df-46a2-9cd4-8988ceadb163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Create padding mask\n",
      "----------------------------------------------------------------------\n",
      "Padding mask (1=real, 0=padding):\n",
      "tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 1, 0]], dtype=torch.int32)\n",
      "Shape: torch.Size([2, 4])\n",
      "\n",
      "Reshaped for attention: torch.Size([2, 1, 1, 4])\n",
      "Why? Will broadcast to (batch, heads, seq_len, seq_len)\n",
      "\n",
      "tensor([[[[1, 1, 1, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# STEP 2: Create padding mask\n",
    "# =============================================\n",
    "print(\"STEP 2: Create padding mask\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "padding_mask = (batch != PAD_TOKEN).int()  # (2, 4)\n",
    "print(f\"Padding mask (1=real, 0=padding):\\n{padding_mask}\")\n",
    "print(f\"Shape: {padding_mask.shape}\")\n",
    "print()\n",
    "\n",
    "# Reshape for attention broadcasting: (batch, 1, 1, seq_len)\n",
    "src_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "print(f\"Reshaped for attention: {src_mask.shape}\")\n",
    "print(f\"Why? Will broadcast to (batch, heads, seq_len, seq_len)\")\n",
    "print()\n",
    "print(src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ff04fdb-4444-413d-88fe-97aa14e65073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Multi-Head Attention Parameters\n",
      "----------------------------------------------------------------------\n",
      "d_model (total embedding size): 8\n",
      "h (number of heads): 2\n",
      "d_k (dimension per head): 4\n",
      "\n",
      "W_q weight shape: torch.Size([8, 8])  # (d_model, d_model)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# STEP 3: Multi-Head Attention Setup\n",
    "# =============================================\n",
    "print(\"STEP 3: Multi-Head Attention Parameters\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "h = 2          # Number of heads\n",
    "d_k = d_model // h  # Dimension per head = 8/2 = 4\n",
    "\n",
    "print(f\"d_model (total embedding size): {d_model}\")\n",
    "print(f\"h (number of heads): {h}\")\n",
    "print(f\"d_k (dimension per head): {d_k}\")\n",
    "print()\n",
    "\n",
    "# Weight matrices\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)  # (8, 8)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "print(f\"W_q weight shape: {W_q.weight.shape}  # (d_model, d_model)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f11a0e5-ce53-4a47-9f4b-bbfa84ea2c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: Project to Q, K, V\n",
      "----------------------------------------------------------------------\n",
      "Input x shape: torch.Size([2, 4, 8])  # (2, 4, 8)\n",
      "Q shape after W_q: torch.Size([2, 4, 8])  # (2, 4, 8)\n",
      "K shape after W_k: torch.Size([2, 4, 8])  # (2, 4, 8)\n",
      "V shape after W_v: torch.Size([2, 4, 8])  # (2, 4, 8)\n",
      "\n",
      "Q for first token of first sentence:\n",
      "tensor([-0.5166, -0.8399,  1.1240, -0.6333, -0.5183,  0.4259, -0.2674, -0.4571],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "tensor([[[-0.5166, -0.8399,  1.1240, -0.6333, -0.5183,  0.4259, -0.2674,\n",
      "          -0.4571],\n",
      "         [-0.1915,  0.1562, -0.8790, -0.3929,  0.4915, -0.3057, -0.1240,\n",
      "          -0.6254],\n",
      "         [-0.3095,  0.3828, -1.2461,  0.8325, -0.7844, -0.6981,  1.9459,\n",
      "           1.6369],\n",
      "         [ 0.0453, -0.5539,  0.4856,  0.1726, -0.9883, -0.3127,  0.2092,\n",
      "          -0.3390]],\n",
      "\n",
      "        [[ 0.4746,  0.0251,  1.4981, -0.5967, -0.8238, -0.2069, -0.5127,\n",
      "          -0.7154],\n",
      "         [ 1.0588,  0.4383,  0.7039,  0.1993, -0.4947, -0.2872, -0.0034,\n",
      "           0.2500],\n",
      "         [-0.9692, -0.5840,  1.0330, -0.2840, -1.2656,  0.3486,  0.6001,\n",
      "           0.1785],\n",
      "         [ 0.0453, -0.5539,  0.4856,  0.1726, -0.9883, -0.3127,  0.2092,\n",
      "          -0.3390]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# STEP 4: Linear projections Q, K, V\n",
    "# =============================================\n",
    "print(\"STEP 4: Project to Q, K, V\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "Q = W_q(x)\n",
    "K = W_k(x)\n",
    "V = W_v(x)\n",
    "\n",
    "print(f\"Input x shape: {x.shape}  # (2, 4, 8)\")\n",
    "print(f\"Q shape after W_q: {Q.shape}  # (2, 4, 8)\")\n",
    "print(f\"K shape after W_k: {K.shape}  # (2, 4, 8)\")\n",
    "print(f\"V shape after W_v: {V.shape}  # (2, 4, 8)\")\n",
    "print()\n",
    "print(\"Q for first token of first sentence:\")\n",
    "print(Q[0, 0, :])\n",
    "print()\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1f42e7b-2202-45e3-abf9-a7b8be4d3ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: Reshape to split heads\n",
      "----------------------------------------------------------------------\n",
      "Original Q shape: torch.Size([2, 4, 8])  # (batch=2, seq=4, d_model=8)\n",
      "\n",
      "After .view(2, 4, 2, 4): torch.Size([2, 4, 2, 4])\n",
      "Meaning: (batch, seq_len, heads, dim_per_head)\n",
      "\n",
      "Q_reshaped[0, 0] (first token, both heads):\n",
      "tensor([[-0.5166, -0.8399,  1.1240, -0.6333],\n",
      "        [-0.5183,  0.4259, -0.2674, -0.4571]], grad_fn=<SelectBackward0>)\n",
      "  Head 0: tensor([-0.5166, -0.8399,  1.1240, -0.6333], grad_fn=<SelectBackward0>)\n",
      "  Head 1: tensor([-0.5183,  0.4259, -0.2674, -0.4571], grad_fn=<SelectBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================\n",
    "# STEP 5: Reshape for multiple heads\n",
    "# =============================================\n",
    "print(\"STEP 5: Reshape to split heads\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "batch_size, seq_len, _ = x.shape\n",
    "\n",
    "print(f\"Original Q shape: {Q.shape}  # (batch=2, seq=4, d_model=8)\")\n",
    "print()\n",
    "\n",
    "# Step 5a: Reshape to (batch, seq, h, d_k)\n",
    "Q_reshaped = Q.view(batch_size, seq_len, h, d_k)\n",
    "print(f\"After .view(2, 4, 2, 4): {Q_reshaped.shape}\")\n",
    "print(f\"Meaning: (batch, seq_len, heads, dim_per_head)\")\n",
    "print()\n",
    "\n",
    "print(\"Q_reshaped[0, 0] (first token, both heads):\")\n",
    "print(Q_reshaped[0, 0])\n",
    "print(f\"  Head 0: {Q_reshaped[0, 0, 0]}\")\n",
    "print(f\"  Head 1: {Q_reshaped[0, 0, 1]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "174b1964-aa81-4e8e-a55a-5483aad0bb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After .transpose(1, 2): torch.Size([2, 2, 4, 4])\n",
      "Meaning: (batch, heads, seq_len, dim_per_head)\n",
      "\n",
      "WHY transpose? For parallel processing:\n",
      "  Now each head can process ALL tokens independently\n",
      "  Head 0 data: Q_heads[:, 0, :, :]\n",
      "  Head 1 data: Q_heads[:, 1, :, :]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5b: Transpose to (batch, h, seq, d_k)\n",
    "Q_heads = Q_reshaped.transpose(1, 2)\n",
    "K_heads = K.view(batch_size, seq_len, h, d_k).transpose(1, 2)\n",
    "V_heads = V.view(batch_size, seq_len, h, d_k).transpose(1, 2)\n",
    "\n",
    "print(f\"After .transpose(1, 2): {Q_heads.shape}\")\n",
    "print(f\"Meaning: (batch, heads, seq_len, dim_per_head)\")\n",
    "print()\n",
    "\n",
    "print(\"WHY transpose? For parallel processing:\")\n",
    "print(\"  Now each head can process ALL tokens independently\")\n",
    "print(\"  Head 0 data: Q_heads[:, 0, :, :]\")\n",
    "print(\"  Head 1 data: Q_heads[:, 1, :, :]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f11ac0d5-07c9-4cd3-853c-4aba37c40748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: Scaled Dot-Product Attention\n",
      "----------------------------------------------------------------------\n",
      "6a. Compute attention scores: Q @ K^T\n",
      "\n",
      "Q_heads shape: torch.Size([2, 2, 4, 4])  # (2, 2, 4, 4)\n",
      "K_heads shape: torch.Size([2, 2, 4, 4])  # (2, 2, 4, 4)\n",
      "\n",
      "K^T shape (transpose last 2 dims): torch.Size([2, 2, 4, 4])  # (2, 2, 4, 4)\n",
      "\n",
      "Q @ K^T shape: torch.Size([2, 2, 4, 4])  # (2, 2, 4, 4)\n",
      "Meaning: (batch, heads, queries, keys)\n",
      "\n",
      "Attention scores for Sentence 0, Head 0:\n",
      "tensor([[ 0.1673, -1.2938,  1.0706,  0.1693],\n",
      "        [-0.6364,  0.3039,  0.0047,  0.0273],\n",
      "        [ 0.1687,  0.5906, -0.8376,  0.1491],\n",
      "        [ 0.2219, -0.3857,  0.2408, -0.0291]], grad_fn=<SelectBackward0>)\n",
      "Each row = how much that query attends to each key\n",
      "\n",
      "tensor([[[[ 0.1673, -1.2938,  1.0706,  0.1693],\n",
      "          [-0.6364,  0.3039,  0.0047,  0.0273],\n",
      "          [ 0.1687,  0.5906, -0.8376,  0.1491],\n",
      "          [ 0.2219, -0.3857,  0.2408, -0.0291]],\n",
      "\n",
      "         [[-0.3048, -0.8697, -0.3871,  0.1783],\n",
      "          [ 0.0196,  0.3884, -0.2620, -0.0572],\n",
      "          [ 0.9272,  1.5876, -0.2701, -0.5478],\n",
      "          [-0.3786,  0.2217, -1.1211, -0.4464]]],\n",
      "\n",
      "\n",
      "        [[[-1.0191,  0.1590, -0.1711, -0.1605],\n",
      "          [-0.2539, -0.4628, -0.2236, -0.3664],\n",
      "          [-0.3891,  0.6773,  0.8599,  0.3967],\n",
      "          [-0.5331, -0.1396,  0.2173, -0.0291]],\n",
      "\n",
      "         [[ 0.0415,  0.4616, -1.4024, -0.4224],\n",
      "          [ 0.1946, -0.0751, -0.2490, -0.3907],\n",
      "          [ 0.7241,  0.1483,  0.2293,  0.0129],\n",
      "          [ 0.2374, -0.2028, -0.7081, -0.4464]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# STEP 6: Scaled Dot-Product Attention\n",
    "# =============================================\n",
    "print(\"STEP 6: Scaled Dot-Product Attention\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Step 6a: Q @ K^T\n",
    "print(\"6a. Compute attention scores: Q @ K^T\")\n",
    "print()\n",
    "\n",
    "print(f\"Q_heads shape: {Q_heads.shape}  # (2, 2, 4, 4)\")\n",
    "print(f\"K_heads shape: {K_heads.shape}  # (2, 2, 4, 4)\")\n",
    "print()\n",
    "\n",
    "K_T = K_heads.transpose(-2, -1)\n",
    "print(f\"K^T shape (transpose last 2 dims): {K_T.shape}  # (2, 2, 4, 4)\")\n",
    "print()\n",
    "\n",
    "attention_scores = Q_heads @ K_T\n",
    "print(f\"Q @ K^T shape: {attention_scores.shape}  # (2, 2, 4, 4)\")\n",
    "print(f\"Meaning: (batch, heads, queries, keys)\")\n",
    "print()\n",
    "\n",
    "print(\"Attention scores for Sentence 0, Head 0:\")\n",
    "print(attention_scores[0, 0])\n",
    "print(\"Each row = how much that query attends to each key\")\n",
    "print()\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "179c6985-3be3-43b9-9a67-145503736998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6b. Scale by √d_k = √4 = 2.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6b: Scale\n",
    "scale_factor = math.sqrt(d_k)\n",
    "attention_scores = attention_scores / scale_factor\n",
    "print(f\"6b. Scale by √d_k = √{d_k} = {scale_factor:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9bbe00a-f767-4a0a-a86b-1b83778b6c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 1, 1, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a38adc9-3223-406a-af36-d0a6e91082d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6c. Apply padding mask\n",
      "\n",
      "Mask shape before broadcasting: torch.Size([2, 1, 1, 4])  # (2, 1, 1, 4) ,tensor([[[[1, 1, 1, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 0]]]], dtype=torch.int32)\n",
      "Scores shape: torch.Size([2, 2, 4, 4])  # (2, 2, 4, 4), tensor([[[[ 0.0837, -0.6469,  0.5353,  0.0846],\n",
      "          [-0.3182,  0.1519,  0.0024,  0.0136],\n",
      "          [ 0.0844,  0.2953, -0.4188,  0.0746],\n",
      "          [ 0.1109, -0.1928,  0.1204, -0.0145]],\n",
      "\n",
      "         [[-0.1524, -0.4348, -0.1936,  0.0891],\n",
      "          [ 0.0098,  0.1942, -0.1310, -0.0286],\n",
      "          [ 0.4636,  0.7938, -0.1351, -0.2739],\n",
      "          [-0.1893,  0.1109, -0.5606, -0.2232]]],\n",
      "\n",
      "\n",
      "        [[[-0.5096,  0.0795, -0.0856, -0.0803],\n",
      "          [-0.1269, -0.2314, -0.1118, -0.1832],\n",
      "          [-0.1946,  0.3386,  0.4300,  0.1983],\n",
      "          [-0.2665, -0.0698,  0.1086, -0.0145]],\n",
      "\n",
      "         [[ 0.0207,  0.2308, -0.7012, -0.2112],\n",
      "          [ 0.0973, -0.0376, -0.1245, -0.1954],\n",
      "          [ 0.3621,  0.0742,  0.1147,  0.0064],\n",
      "          [ 0.1187, -0.1014, -0.3541, -0.2232]]]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Broadcasting magic:\n",
      "  Mask (2, 1, 1, 4) → broadcasts to (2, 2, 4, 4)\n",
      "  Dimension 1: 1 → 2 (applied to both heads)\n",
      "  Dimension 2: 1 → 4 (applied to all queries)\n",
      "\n",
      "Scores after masking (Sentence 0, Head 0):\n",
      "tensor([[[[ 8.3672e-02, -6.4692e-01,  5.3531e-01, -1.0000e+09],\n",
      "          [-3.1819e-01,  1.5193e-01,  2.3727e-03, -1.0000e+09],\n",
      "          [ 8.4363e-02,  2.9529e-01, -4.1882e-01, -1.0000e+09],\n",
      "          [ 1.1093e-01, -1.9285e-01,  1.2042e-01, -1.0000e+09]],\n",
      "\n",
      "         [[-1.5240e-01, -4.3485e-01, -1.9356e-01, -1.0000e+09],\n",
      "          [ 9.8047e-03,  1.9422e-01, -1.3099e-01, -1.0000e+09],\n",
      "          [ 4.6359e-01,  7.9380e-01, -1.3507e-01, -1.0000e+09],\n",
      "          [-1.8929e-01,  1.1086e-01, -5.6055e-01, -1.0000e+09]]],\n",
      "\n",
      "\n",
      "        [[[-5.0957e-01,  7.9494e-02, -8.5567e-02, -1.0000e+09],\n",
      "          [-1.2693e-01, -2.3140e-01, -1.1178e-01, -1.0000e+09],\n",
      "          [-1.9456e-01,  3.3864e-01,  4.2997e-01, -1.0000e+09],\n",
      "          [-2.6654e-01, -6.9804e-02,  1.0863e-01, -1.0000e+09]],\n",
      "\n",
      "         [[ 2.0747e-02,  2.3081e-01, -7.0121e-01, -1.0000e+09],\n",
      "          [ 9.7288e-02, -3.7556e-02, -1.2451e-01, -1.0000e+09],\n",
      "          [ 3.6206e-01,  7.4170e-02,  1.1467e-01, -1.0000e+09],\n",
      "          [ 1.1870e-01, -1.0139e-01, -3.5406e-01, -1.0000e+09]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Notice: Column 3 (padding) has very negative values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6c: Apply mask\n",
    "print(\"6c. Apply padding mask\")\n",
    "print()\n",
    "print(f\"Mask shape before broadcasting: {src_mask.shape}  # (2, 1, 1, 4) ,{src_mask}\")\n",
    "print(f\"Scores shape: {attention_scores.shape}  # (2, 2, 4, 4), {attention_scores}\")\n",
    "print()\n",
    "\n",
    "print(\"Broadcasting magic:\")\n",
    "print(\"  Mask (2, 1, 1, 4) → broadcasts to (2, 2, 4, 4)\")\n",
    "print(\"  Dimension 1: 1 → 2 (applied to both heads)\")\n",
    "print(\"  Dimension 2: 1 → 4 (applied to all queries)\")\n",
    "print()\n",
    "\n",
    "attention_scores.masked_fill_(src_mask == 0, -1e9)\n",
    "print(\"Scores after masking (Sentence 0, Head 0):\")\n",
    "print(attention_scores)\n",
    "print(\"Notice: Column 3 (padding) has very negative values\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74638410-fd9e-4327-a31d-b268ad62df26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6d. Softmax (normalize each row)\n",
      "\n",
      "Attention weights (Sentence 0, Head 0):\n",
      "tensor([[0.3276, 0.1578, 0.5146, 0.0000],\n",
      "        [0.2514, 0.4022, 0.3464, 0.0000],\n",
      "        [0.3522, 0.4349, 0.2129, 0.0000],\n",
      "        [0.3640, 0.2686, 0.3674, 0.0000]], grad_fn=<SelectBackward0>)\n",
      "Notice: Column 3 (padding) ≈ 0.000\n",
      "\n",
      "Row sums (should be ~1.0):\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6d: Softmax\n",
    "attention_weights = attention_scores.softmax(dim=-1)\n",
    "print(\"6d. Softmax (normalize each row)\")\n",
    "print()\n",
    "print(\"Attention weights (Sentence 0, Head 0):\")\n",
    "print(attention_weights[0, 0])\n",
    "print(\"Notice: Column 3 (padding) ≈ 0.000\")\n",
    "print()\n",
    "\n",
    "print(\"Row sums (should be ~1.0):\")\n",
    "print(attention_weights[0, 0].sum(dim=-1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3edd62f9-9611-4004-a5ad-efd740ce869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6e. Multiply by V: attention_weights @ V\n",
      "\n",
      "attention_weights shape: torch.Size([2, 2, 4, 4])  # (2, 2, 4, 4)\n",
      "V_heads shape: torch.Size([2, 2, 4, 4])  # (2, 2, 4, 4)\n",
      "\n",
      "Output shape: torch.Size([2, 2, 4, 4])  # (2, 2, 4, 4)\n",
      "\n",
      "Attention output for first token, Head 0:\n",
      "tensor([ 0.3990,  0.0728, -0.2735,  0.0642], grad_fn=<SelectBackward0>)\n",
      "This is a weighted mix of all value vectors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6e: Weighted sum of V\n",
    "print(\"6e. Multiply by V: attention_weights @ V\")\n",
    "print()\n",
    "print(f\"attention_weights shape: {attention_weights.shape}  # (2, 2, 4, 4)\")\n",
    "print(f\"V_heads shape: {V_heads.shape}  # (2, 2, 4, 4)\")\n",
    "print()\n",
    "\n",
    "attention_output = attention_weights @ V_heads\n",
    "print(f\"Output shape: {attention_output.shape}  # (2, 2, 4, 4)\")\n",
    "print()\n",
    "\n",
    "print(\"Attention output for first token, Head 0:\")\n",
    "print(attention_output[0, 0, 0])\n",
    "print(\"This is a weighted mix of all value vectors\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edaec75b-41cd-4404-84b7-f89aad3b19dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: Concatenate heads back together\n",
      "----------------------------------------------------------------------\n",
      "Before concat: torch.Size([2, 2, 4, 4])  # (2, 2, 4, 4)\n",
      "                (batch, heads, seq, d_k)\n",
      "\n",
      "After transpose(1,2): torch.Size([2, 4, 2, 4])  # (2, 4, 2, 4)\n",
      "                       (batch, seq, heads, d_k)\n",
      "\n",
      "WHY transpose back? To group heads per token:\n",
      "  Before: Head 0 [all tokens], Head 1 [all tokens]\n",
      "  After:  Token 0 [all heads], Token 1 [all heads], ...\n",
      "\n",
      "After .contiguous(): Same shape, but memory layout fixed\n",
      "\n",
      "After .view(2, 4, 8): torch.Size([2, 4, 8])\n",
      "                      (batch, seq, d_model)\n",
      "\n",
      "First token after concatenation:\n",
      "tensor([ 0.3990,  0.0728, -0.2735,  0.0642, -0.3110, -0.0801, -0.0454,  0.2214],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "This is [Head0_output || Head1_output] concatenated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# STEP 7: Concatenate heads\n",
    "# =============================================\n",
    "print(\"STEP 7: Concatenate heads back together\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"Before concat: {attention_output.shape}  # (2, 2, 4, 4)\")\n",
    "print(f\"                (batch, heads, seq, d_k)\")\n",
    "print()\n",
    "\n",
    "# Step 7a: Transpose back\n",
    "attention_transposed = attention_output.transpose(1, 2)\n",
    "print(f\"After transpose(1,2): {attention_transposed.shape}  # (2, 4, 2, 4)\")\n",
    "print(f\"                       (batch, seq, heads, d_k)\")\n",
    "print()\n",
    "\n",
    "print(\"WHY transpose back? To group heads per token:\")\n",
    "print(\"  Before: Head 0 [all tokens], Head 1 [all tokens]\")\n",
    "print(\"  After:  Token 0 [all heads], Token 1 [all heads], ...\")\n",
    "print()\n",
    "\n",
    "# Step 7b: Contiguous (fix memory layout)\n",
    "attention_contiguous = attention_transposed.contiguous()\n",
    "print(\"After .contiguous(): Same shape, but memory layout fixed\")\n",
    "print()\n",
    "\n",
    "# Step 7c: Flatten last 2 dims\n",
    "attention_concat = attention_contiguous.view(batch_size, seq_len, d_model)\n",
    "print(f\"After .view(2, 4, 8): {attention_concat.shape}\")\n",
    "print(f\"                      (batch, seq, d_model)\")\n",
    "print()\n",
    "\n",
    "print(\"First token after concatenation:\")\n",
    "print(attention_concat[0, 0])\n",
    "print(\"This is [Head0_output || Head1_output] concatenated\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80dec93c-0b78-4ca5-bc47-1a50934d3a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 8: Final linear projection (W_o)\n",
      "----------------------------------------------------------------------\n",
      "Final shape: torch.Size([2, 4, 8])  # (2, 4, 8)\n",
      "\n",
      "Final output for first token:\n",
      "tensor([-0.1838,  0.0888, -0.0824, -0.1091,  0.0114, -0.0534, -0.0661, -0.1518],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# STEP 8: Output projection\n",
    "# =============================================\n",
    "print(\"STEP 8: Final linear projection (W_o)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "final_output = W_o(attention_concat)\n",
    "print(f\"Final shape: {final_output.shape}  # (2, 4, 8)\")\n",
    "print()\n",
    "\n",
    "print(\"Final output for first token:\")\n",
    "print(final_output[0, 0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d41f5fd4-abb3-48e0-9f46-a85a7f954cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SHAPE EVOLUTION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Input:        (2, 4, 8)     batch, seq_len, d_model\n",
      "↓ Embedding\n",
      "Embedded:     (2, 4, 8)\n",
      "↓ Linear Q/K/V\n",
      "Q, K, V:      (2, 4, 8)\n",
      "↓ Reshape\n",
      "Q_reshaped:   (2, 4, 2, 4)  batch, seq, heads, d_k\n",
      "↓ Transpose\n",
      "Q_heads:      (2, 2, 4, 4)  batch, heads, seq, d_k\n",
      "↓ Q @ K^T\n",
      "Scores:       (2, 2, 4, 4)  batch, heads, queries, keys\n",
      "↓ Softmax\n",
      "Weights:      (2, 2, 4, 4)\n",
      "↓ @ V\n",
      "Output:       (2, 2, 4, 4)  batch, heads, seq, d_k\n",
      "↓ Transpose\n",
      "Output_T:     (2, 4, 2, 4)  batch, seq, heads, d_k\n",
      "↓ Flatten (view)\n",
      "Concat:       (2, 4, 8)     batch, seq, d_model\n",
      "↓ W_o\n",
      "Final:        (2, 4, 8)     batch, seq, d_model\n",
      "\n",
      "======================================================================\n",
      "WHY RESHAPING IS NECESSARY\n",
      "======================================================================\n",
      "\n",
      "1. PARALLEL PROCESSING:\n",
      "   Shape (2, 2, 4, 4) lets PyTorch compute both heads simultaneously\n",
      "\n",
      "2. MATRIX MULTIPLICATION:\n",
      "   Q @ K^T requires matching dimensions:\n",
      "   (batch, h, seq, d_k) @ (batch, h, d_k, seq) → (batch, h, seq, seq)\n",
      "\n",
      "3. BROADCASTING:\n",
      "   Mask (2, 1, 1, 4) → (2, 2, 4, 4) applies same mask to all heads\n",
      "\n",
      "4. CONCATENATION:\n",
      "   Transpose + view groups head outputs per token for final projection\n",
      "\n",
      "✅ DONE! Each token now has context from all other tokens via attention!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================\n",
    "# SUMMARY VISUALIZATION\n",
    "# =============================================\n",
    "print(\"=\"*70)\n",
    "print(\"SHAPE EVOLUTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Input:        (2, 4, 8)     batch, seq_len, d_model\")\n",
    "print(\"↓ Embedding\")\n",
    "print(\"Embedded:     (2, 4, 8)\")\n",
    "print(\"↓ Linear Q/K/V\")\n",
    "print(\"Q, K, V:      (2, 4, 8)\")\n",
    "print(\"↓ Reshape\")\n",
    "print(\"Q_reshaped:   (2, 4, 2, 4)  batch, seq, heads, d_k\")\n",
    "print(\"↓ Transpose\")\n",
    "print(\"Q_heads:      (2, 2, 4, 4)  batch, heads, seq, d_k\")\n",
    "print(\"↓ Q @ K^T\")\n",
    "print(\"Scores:       (2, 2, 4, 4)  batch, heads, queries, keys\")\n",
    "print(\"↓ Softmax\")\n",
    "print(\"Weights:      (2, 2, 4, 4)\")\n",
    "print(\"↓ @ V\")\n",
    "print(\"Output:       (2, 2, 4, 4)  batch, heads, seq, d_k\")\n",
    "print(\"↓ Transpose\")\n",
    "print(\"Output_T:     (2, 4, 2, 4)  batch, seq, heads, d_k\")\n",
    "print(\"↓ Flatten (view)\")\n",
    "print(\"Concat:       (2, 4, 8)     batch, seq, d_model\")\n",
    "print(\"↓ W_o\")\n",
    "print(\"Final:        (2, 4, 8)     batch, seq, d_model\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"WHY RESHAPING IS NECESSARY\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"1. PARALLEL PROCESSING:\")\n",
    "print(\"   Shape (2, 2, 4, 4) lets PyTorch compute both heads simultaneously\")\n",
    "print()\n",
    "print(\"2. MATRIX MULTIPLICATION:\")\n",
    "print(\"   Q @ K^T requires matching dimensions:\")\n",
    "print(\"   (batch, h, seq, d_k) @ (batch, h, d_k, seq) → (batch, h, seq, seq)\")\n",
    "print()\n",
    "print(\"3. BROADCASTING:\")\n",
    "print(\"   Mask (2, 1, 1, 4) → (2, 2, 4, 4) applies same mask to all heads\")\n",
    "print()\n",
    "print(\"4. CONCATENATION:\")\n",
    "print(\"   Transpose + view groups head outputs per token for final projection\")\n",
    "print()\n",
    "\n",
    "print(\"✅ DONE! Each token now has context from all other tokens via attention!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5355cd-01a6-48e8-afc0-8a85ae8a6197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

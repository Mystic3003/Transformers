{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "455f559b-7f86-4559-a120-ea2129b05280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# =============================================\n",
    "# LayerNormalization (from earlier)\n",
    "# =============================================\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))   # Scale\n",
    "        self.bias = nn.Parameter(torch.zeros(features))   # Shift\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "# =============================================\n",
    "# ResidualConnection\n",
    "# =============================================\n",
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements: x + Dropout(Sublayer(LayerNorm(x))) , original in the paper -LayerNormalization(x+Sublayer(x))\n",
    "    \n",
    "    This is the \"Pre-LN\" variant where LayerNorm comes BEFORE the sublayer.\n",
    "    Original paper used \"Post-LN\": LayerNorm(x + Sublayer(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, features)\n",
    "            sublayer: A function/module (e.g., attention or feedforward)\n",
    "        \n",
    "        Returns:\n",
    "            x + dropout(sublayer(norm(x)))\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "# =============================================\n",
    "# Dummy Sublayer (FeedForward example)\n",
    "# =============================================\n",
    "class SimpleFeedForward(nn.Module):\n",
    "    \"\"\"Simple 2-layer feedforward for demo\"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(torch.relu(self.linear1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42881e89-8412-4767-9b0e-d250c709515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESIDUAL CONNECTION DEMO ===\n",
      "\n",
      "Step 1 - Input x:\n",
      "Shape: torch.Size([2, 3, 4])\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# DEMO: Step-by-step execution\n",
    "# =============================================\n",
    "print(\"=== RESIDUAL CONNECTION DEMO ===\\n\")\n",
    "\n",
    "# Setup\n",
    "batch_size = 2 # number of sentences\n",
    "seq_len = 3 # number of tokens in each sentence \n",
    "d_model = 4 #feature dimension \n",
    "d_ff = 8 ##hidden layer dimension\n",
    "dropout = 0.1 \n",
    "\n",
    "# Input: (batch, seq, features)\n",
    "x = torch.tensor([[[1.0, 2.0, 3.0, 4.0],\n",
    "                   [5.0, 6.0, 7.0, 8.0],\n",
    "                   [9.0, 10.0, 11.0, 12.0]],\n",
    "\n",
    "                  [[13.0, 14.0, 15.0, 16.0],\n",
    "                   [17.0, 18.0, 19.0, 20.0],\n",
    "                   [21.0, 22.0, 23.0, 24.0]]])\n",
    "\n",
    "print(\"Step 1 - Input x:\")\n",
    "print(\"Shape:\", x.shape)  # (2, 3, 4)\n",
    "print(x[0])  # Show first batch\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0e87f4-fd70-4e4d-9cb0-734df8947482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleFeedForward(\n",
       "  (linear1): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (linear2): Linear(in_features=8, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sublayer (feedforward)\n",
    "sublayer = SimpleFeedForward(d_model, d_ff)\n",
    "sublayer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6327aec1-b55f-483b-9774-02070ffa0fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualConnection(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (norm): LayerNormalization()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create residual connection\n",
    "residual = ResidualConnection(features=d_model, dropout=dropout)\n",
    "residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8054f9d6-259b-4793-88ed-dd7c9d414531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 - Apply LayerNorm(x):\n",
      "Shape: torch.Size([2, 3, 4])\n",
      "non-normalized and normalized input : tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.]],\n",
      "\n",
      "        [[13., 14., 15., 16.],\n",
      "         [17., 18., 19., 20.],\n",
      "         [21., 22., 23., 24.]]]) tensor([[[-1.1619, -0.3873,  0.3873,  1.1619],\n",
      "         [-1.1619, -0.3873,  0.3873,  1.1619],\n",
      "         [-1.1619, -0.3873,  0.3873,  1.1619]],\n",
      "\n",
      "        [[-1.1619, -0.3873,  0.3873,  1.1619],\n",
      "         [-1.1619, -0.3873,  0.3873,  1.1619],\n",
      "         [-1.1619, -0.3873,  0.3873,  1.1619]]], grad_fn=<AddBackward0>)\n",
      "normalized[0]: tensor([[-1.1619, -0.3873,  0.3873,  1.1619],\n",
      "        [-1.1619, -0.3873,  0.3873,  1.1619],\n",
      "        [-1.1619, -0.3873,  0.3873,  1.1619]], grad_fn=<SelectBackward0>)\n",
      "Check: mean≈0, std≈1?\n",
      "  Mean: tensor([2.9802e-08, 2.9802e-08, 2.9802e-08], grad_fn=<MeanBackward1>)\n",
      "  Std: tensor([1.0000, 1.0000, 1.0000], grad_fn=<StdBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MANUAL STEP-BY-STEP (what happens inside) --  x + Dropout(Sublayer(LayerNorm(x)))\n",
    "# ============================================\n",
    "print(\"Step 2 - Apply LayerNorm(x):\")\n",
    "normalized = residual.norm(x)\n",
    "print(\"Shape:\", normalized.shape)  # (2, 3, 4)\n",
    "print('non-normalized and normalized input :',x,normalized)\n",
    "print(\"normalized[0]:\", normalized[0])\n",
    "print(\"Check: mean≈0, std≈1?\")\n",
    "print(\"  Mean:\", normalized[0].mean(dim=-1))\n",
    "print(\"  Std:\", normalized[0].std(dim=-1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7233cde9-26a1-48ba-a449-14c531ff680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 - Apply Sublayer (FeedForward):\n",
      "Shape: torch.Size([2, 3, 4])\n",
      "sublayer_out: tensor([[[ 0.0744, -0.3077, -0.0528, -0.0717],\n",
      "         [ 0.0744, -0.3077, -0.0528, -0.0717],\n",
      "         [ 0.0744, -0.3077, -0.0528, -0.0717]],\n",
      "\n",
      "        [[ 0.0744, -0.3077, -0.0528, -0.0717],\n",
      "         [ 0.0744, -0.3077, -0.0528, -0.0717],\n",
      "         [ 0.0744, -0.3077, -0.0528, -0.0717]]], grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3 - Apply Sublayer (FeedForward):\")\n",
    "sublayer_out = sublayer(normalized)\n",
    "print(\"Shape:\", sublayer_out.shape)  # (2, 3, 4)\n",
    "print(\"sublayer_out:\",sublayer_out)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "767a5443-4f6f-4546-a407-3c3e32aab7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 - Apply Dropout:\n",
      "Shape: torch.Size([2, 3, 4])\n",
      "dropped: tensor([[[ 0.0827, -0.3419, -0.0587, -0.0797],\n",
      "         [ 0.0827, -0.3419, -0.0587, -0.0797],\n",
      "         [ 0.0827, -0.3419, -0.0587, -0.0797]],\n",
      "\n",
      "        [[ 0.0000, -0.3419, -0.0587, -0.0797],\n",
      "         [ 0.0827, -0.3419, -0.0587, -0.0797],\n",
      "         [ 0.0827, -0.3419, -0.0587, -0.0797]]], grad_fn=<MulBackward0>)\n",
      "Notice: ~10% elements zeroed/scaled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4 - Apply Dropout:\")\n",
    "residual.dropout.train()  # Enable dropout\n",
    "dropped = residual.dropout(sublayer_out)\n",
    "print(\"Shape:\", dropped.shape)  # (2, 3, 4)\n",
    "print(\"dropped:\", dropped)\n",
    "print(\"Notice: ~10% elements zeroed/scaled\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f67d38bd-82e4-45b1-a556-5cff20b112ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 - ADD residual (x + dropped):\n",
      "Shape: torch.Size([2, 3, 4])\n",
      "output: tensor([[[ 1.0827,  1.6581,  2.9413,  3.9203],\n",
      "         [ 5.0827,  5.6581,  6.9413,  7.9203],\n",
      "         [ 9.0827,  9.6581, 10.9413, 11.9203]],\n",
      "\n",
      "        [[13.0000, 13.6581, 14.9413, 15.9203],\n",
      "         [17.0827, 17.6581, 18.9413, 19.9203],\n",
      "         [21.0827, 21.6581, 22.9413, 23.9203]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5 - ADD residual (x + dropped):\")\n",
    "output = x + dropped\n",
    "print(\"Shape:\", output.shape)  # (2, 3, 4)\n",
    "print(\"output:\", output)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22b121ef-80c7-4f75-b7bd-61c31563c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare magnitudes:\n",
      "  Input x[0,0]: tensor([1., 2., 3., 4.])\n",
      "  Sublayer only: tensor([ 0.0744, -0.3077, -0.0528, -0.0717], grad_fn=<SelectBackward0>)\n",
      "  With residual: tensor([1.0827, 1.6581, 2.9413, 3.9203], grad_fn=<SelectBackward0>)\n",
      "  → Residual keeps original signal!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Compare magnitudes:\")\n",
    "print(\"  Input x[0,0]:\", x[0, 0])\n",
    "print(\"  Sublayer only:\", sublayer_out[0, 0])\n",
    "print(\"  With residual:\", output[0, 0])\n",
    "print(\"  → Residual keeps original signal!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1e8c476-44ad-46e1-802c-af86bb7fe106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 - END-TO-END: residual(x, sublayer)\n",
      "Shape: torch.Size([2, 3, 4])\n",
      "direct_output: tensor([[[ 1.0827,  1.6581,  2.9413,  3.9203],\n",
      "         [ 5.0827,  5.6581,  6.9413,  7.9203],\n",
      "         [ 9.0000,  9.6581, 11.0000, 11.9203]],\n",
      "\n",
      "        [[13.0827, 13.6581, 14.9413, 15.9203],\n",
      "         [17.0000, 18.0000, 18.9413, 19.9203],\n",
      "         [21.0827, 21.6581, 22.9413, 23.9203]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# END-TO-END using forward()\n",
    "# ============================================\n",
    "print(\"Step 6 - END-TO-END: residual(x, sublayer)\")\n",
    "direct_output = residual(x, sublayer)\n",
    "print(\"Shape:\", direct_output.shape)\n",
    "print(\"direct_output:\", direct_output)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e628fe9-7dd5-46cf-95b7-af464fb7421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 - GRADIENT FLOW (why residuals matter)\n",
      "Gradient of input: tensor([0.8409, 0.8771, 0.7823, 1.4997])\n",
      "Notice: gradient is non-zero even if sublayer gradient vanishes!\n",
      "This is because: ∂(x + f(x))/∂x = 1 + ∂f/∂x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# Gradient flow demonstration\n",
    "# ============================================\n",
    "print(\"Step 7 - GRADIENT FLOW (why residuals matter)\")\n",
    "x_grad = torch.randn(2, 3, 4, requires_grad=True)\n",
    "sublayer_grad = SimpleFeedForward(d_model, d_ff)\n",
    "residual_grad = ResidualConnection(d_model, dropout=0.0)  # No dropout for clarity\n",
    "\n",
    "# Forward\n",
    "out = residual_grad(x_grad, sublayer_grad)\n",
    "loss = out.sum()\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradient of input:\", x_grad.grad[0, 0])\n",
    "print(\"Notice: gradient is non-zero even if sublayer gradient vanishes!\")\n",
    "print(\"This is because: ∂(x + f(x))/∂x = 1 + ∂f/∂x\")\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "835a6512-02af-4bc1-b6e0-d49cd8ca8342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 - RESIDUAL vs NO RESIDUAL comparison\n",
      "============================================================\n",
      "WITHOUT residual (just sublayer output):\n",
      "  Shape: torch.Size([2, 3, 4])\n",
      "  Range: -0.3077440857887268 to 0.07440011203289032\n",
      "\n",
      "WITH residual (x + sublayer):\n",
      "  Shape: torch.Size([2, 3, 4])\n",
      "  Range: 1.0 to 23.92030906677246\n",
      "\n",
      "✅ Residual connection preserves input information while adding new features!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Visualize residual vs no residual\n",
    "# ============================================\n",
    "print(\"Step 8 - RESIDUAL vs NO RESIDUAL comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Without residual\n",
    "no_residual_out = sublayer(residual.norm(x))\n",
    "print(\"WITHOUT residual (just sublayer output):\")\n",
    "print(\"  Shape:\", no_residual_out.shape)\n",
    "print(\"  Range:\", no_residual_out.min().item(), \"to\", no_residual_out.max().item())\n",
    "print()\n",
    "\n",
    "# With residual\n",
    "with_residual_out = x + residual.dropout(sublayer(residual.norm(x)))\n",
    "print(\"WITH residual (x + sublayer):\")\n",
    "print(\"  Shape:\", with_residual_out.shape)\n",
    "print(\"  Range:\", with_residual_out.min().item(), \"to\", with_residual_out.max().item())\n",
    "print()\n",
    "\n",
    "print(\"✅ Residual connection preserves input information while adding new features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f03ed6-3acb-4ea5-a84c-4509b7d8a727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406f974-f84d-4b49-9141-343d925601ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3689f5-4fb0-46b1-b908-d8f28fc17ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
